[{"authors":["admin"],"categories":null,"content":"My research centers on political-economic issues such as corruption, economic development, and business-state relations in developing countries, and in particular the Middle East and North Africa. I am also involved in the development of Bayesian statistical models with Stan for hard-to-study subjects like corruption, polarization, and other latent social constructs.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"http://www.robertkubinec.com/author/robert-kubinec/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/robert-kubinec/","section":"authors","summary":"My research centers on political-economic issues such as corruption, economic development, and business-state relations in developing countries, and in particular the Middle East and North Africa. I am also involved in the development of Bayesian statistical models with Stan for hard-to-study subjects like corruption, polarization, and other latent social constructs.","tags":null,"title":"Robert Kubinec","type":"authors"},{"authors":null,"categories":null,"content":"   While perusing the news, I read eagerly about the CDC’s recent study examining associations between county-level mask mandates and COVID-19 growth rates. This study has already been the subject of angry retorts from the restaurant industry due to the CDC’s claim that restaurant closures reduced COVID-19 spread. Looking at the underlying details in the study, though, revealed some concerns about the particular model they used, known as a two-way fixed effects model, and also an important factor they left out of the model entirely: political partisanship.\nTo examine these deficiencies, I replicated their model from scratch and also employed different panel data models to provide clearer answers. In brief, the negative association between mask mandates and the spread of COVID-19 does hold up, but it appears to be highly conditional on partisanship. In addition, there are a lot of complexities when it comes to comparing counties, and I do not believe the CDC or anyone else has yet resolved all of them. So while I applaud the CDC for doing a lot of hard work on data collection, we still have a ways to go before we can fully understand how effective mask mandates were against COVID-19.\nIn this post I first discuss my challenges replicating their analysis, and then I critique the choices they made in their paper. Later I put forward my own analysis to probe in more detail the role of mask mandates. I include the code I use to estimate models in this blog post to be transparent about how I did the re-analysis. If you are not familiar with R code, feel free to skip those sections.\nIf you would like access to the code and data I used to write this blog post, please use this Github repository.\nCollecting the CDC Paper’s Data The first and most necessary step to replicate the CDC’s findings is to obtain data to fit their model. This exercise is also important because it shows if 1) their article is clear enough to successfully re-trace their steps and if 2) all the data is available to do so. The authors of the study have posted their mask and other county policy data online, which was easy to download and analyze. Unfortunately, some of their data, particularly testing and case data collected by Health and Human Services (HHS), is not available to the wider public. Given the new administration’s commitment to transparency, it really should be available publicly. Because I am writing a blog post and wanted data I could share, I chose instead to use open source repositories of case and test data, specifically the New York Times for county-level cases and the COVID-19 Tracking Project for test data by state. Because I could not find testing at the county level, I approximated tests by county by multiplying that county’s share of state COVID-19 tests with that state’s daily testing total.\nThere were some ambiguities in interpreting the article’s instructions about the CDC’s county policy data. The policy restrictions, including stay-at-home orders, mask mandates, and restaurant and bar closures, have multiple categories, such as partial enforcement, and it was not clear if I should use all the categories or collapse them to binary indicators. This became an issue later in the model because some of the categories were collinear and the model dropped them, particularly bar closures. I am not sure for that reason if my use of the CDC’s data exactly matches theirs.\n Replicating the CDC’s Results Given these caveats about data collection, the model estimates I obtained using the same specification–two-way fixed effects for county and day–are broadly similar to their estimates, although the associations are weaker on average. I also used the same comparison periods they describe in the paper. For each county, I selected the first twenty days preceding the mask mandate implementation, and then fit separate models to five different post-mandate periods: 1-20 days following the mandate, 21-40 days, 41-60 days, 61-80 days, and 81-100 days. The ambiguity that surfaced while doing this occurred in counties with multiple periods of mask mandates: what if there weren’t twenty days prior to the second mask mandate? I included as many as were available, but its still important to note this ambiguity as\nThe plot below compares my estimates of the effect of mask mandates to theirs by the five different post-mandate time periods. The weaker associations in my estimates could simply be due to measurement error from using unofficial sources, so the smaller effects are not necessarily a huge concern (though it would be better if the CDC released all of the data they used). On the whole, I think this replication was at least partially successfulas the confidence intervals of the estimates overlap.\nThe following code replicating the CDC’s paper is quite long but is included for transparency’s sake. I used the fixest package with R to fit linear models due to the scale of the data and the number of fixed effects (one for each county plus one for each day).\n# estimate CDC model using lm # baseline = 1-20 days before implementation # models differentiated by number of days included post-implementation # note: no mention of whether this makes the panel imbalanced # a bit odd too if there are multiple times that county switches in and out of mask mandates d1 \u0026lt;- filter(combined_data, (n_day_before \u0026gt; 0 \u0026amp; n_day_before \u0026lt; 21) | (n_day_after \u0026gt; 0 \u0026amp; n_day_after \u0026lt; 21)) %\u0026gt;% fixest::feols( cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather | fips + date, data = ., cluster = \u0026quot;FIPS_State\u0026quot;, weights = ~ pop ) d2 \u0026lt;- filter(combined_data, (n_day_before \u0026gt; 0 \u0026amp; n_day_before \u0026lt; 21) | (n_day_after \u0026gt; 20 \u0026amp; n_day_after \u0026lt; 41)) %\u0026gt;% fixest::feols( cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather | fips + date, data = ., cluster = \u0026quot;FIPS_State\u0026quot;, weights = ~ pop ) d3 \u0026lt;- filter(combined_data, (n_day_before \u0026gt; 0 \u0026amp; n_day_before \u0026lt; 21) | (n_day_after \u0026gt; 40 \u0026amp; n_day_after \u0026lt; 61)) %\u0026gt;% fixest::feols( cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather | fips + date, data = ., cluster = \u0026quot;FIPS_State\u0026quot;, weights = ~ pop ) d4 \u0026lt;- filter(combined_data, (n_day_before \u0026gt; 0 \u0026amp; n_day_before \u0026lt; 21) | (n_day_after \u0026gt; 60 \u0026amp; n_day_after \u0026lt; 81)) %\u0026gt;% fixest::feols( cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather | fips + date, data = ., cluster = \u0026quot;FIPS_State\u0026quot;, weights = ~ pop ) d5 \u0026lt;- filter( combined_data, (n_day_before \u0026gt; 0 \u0026amp; n_day_before \u0026lt; 21) | (n_day_after \u0026gt; 80 \u0026amp; n_day_after \u0026lt; 101) ) %\u0026gt;% fixest::feols( cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather | fips + date, data = ., cluster = \u0026quot;FIPS_State\u0026quot;, weights = ~ pop ) # combine data from all models together into a tibble all_coefs \u0026lt;- lapply(list(d1,d2,d3,d4,d5),function(d) slice(summary(d)$coeftable,1)) %\u0026gt;% bind_rows %\u0026gt;% mutate(Model=c(\u0026quot;1-20 Days\u0026quot;, \u0026quot;21-40 Days\u0026quot;, \u0026quot;41-60 Days\u0026quot;, \u0026quot;61-80 Days\u0026quot;, \u0026quot;81-100 Days\u0026quot;), upb=Estimate + 1.96*`Std. Error`, lb=Estimate - 1.96*`Std. Error`, Type=\u0026quot;Replication\u0026quot;) # combine with original estimates all_coefs \u0026lt;- bind_rows(all_coefs, tibble(Estimate=c(-0.5,-1.1,-1.5,-1.7,-1.8), lb=c(-0.1,-0.6,-0.8,-0.9,-0.7), upb=c(-0.8,-1.6,-2.1,-2.6,-2.8), Model=c(\u0026quot;1-20 Days\u0026quot;, \u0026quot;21-40 Days\u0026quot;, \u0026quot;41-60 Days\u0026quot;, \u0026quot;61-80 Days\u0026quot;, \u0026quot;81-100 Days\u0026quot;),Type=\u0026quot;CDC\u0026quot;)) all_coefs %\u0026gt;% ggplot(aes(x=Estimate,y=Model)) + geom_errorbarh(aes(xmin=lb,xmax=upb,colour=Type),position=position_dodge(width=0.5)) + geom_text(aes(label=round(Estimate,digits=2),group=Type), position=position_dodge(width=0.5), size=3,vjust=-.75) + theme_tufte() + geom_vline(xintercept=0,linetype=2) + scale_colour_viridis_d() + scale_x_continuous(labels=scales::percent_format(scale=1)) + labs(y=\u0026quot;Comparison Period\u0026quot;,x=\u0026quot;Percentage Change in Daily COVID-19 Cases\u0026quot;) + ggtitle(\u0026quot;Replication of CDC Mask Results with Open Source Data\u0026quot;)  Issues with the CDC’s Analysis Given the reasonably successful replication, I now turn to some of the issues with the analysis. One peculiar choice they made was to subdivide their data into the different days following a mask mandate as the plot above shows. The reason for this choice, though they did not discuss it in their paper, is probably due to the difficulty in comparing COVID-19 case rates across counties. It is quite likely that counties will wait to adopt mask mandates until COVID-19 rates are very high, so the unconditional association between mask mandates and COVID-19 could end up being strongly positive. This issue can be seen in the plot below, where I show each county’s COVID-19 growth rate by day, and colour the lines by whether a mask mandate was enforced. As can be seen, the imposition of mask mandates begins later in the course of the pandemic around March and April as case rates pick up and the CDC issues mask-wearing guidance.\n# plot the orders as line plots to show when the tend to be implemented combined_data %\u0026gt;% ggplot(aes(y=cases/pop,x=date)) + geom_line(aes(colour=mask_status,group=fips),alpha=0.5) + theme_tufte() + scale_color_viridis_d() + labs(x=\u0026quot;\u0026quot;,y=\u0026quot;Percent of County Infected with COVID-19\u0026quot;, caption=\u0026quot;COVID-10 cases data from the New York Times and mask mandate data from the CDC.\u0026quot;) + scale_y_continuous(labels=scales::percent) + ggtitle(\u0026quot;County COVID-19 Infection Rates Over Time\u0026quot;) The CDC tried to resolve this by restricting their sample to the twenty days preceding a mask mandate and one of the five categories mentioned before for twenty-day periods following the mask mandate. This is one approach, but not a particularly satisfying one. The day cutoffs are very arbitrary (why twenty-day intervals? Why only the previous twenty days before a mask mandate?). I think a much better approach, and one I show later, is to try to control for some of the differences that may lead to mask adoption, and to use all the sample data unless there is a very clear reason not to.\nHowever, there is an even more significant concern, and that is the modeling approach itself. The CDC used a linear model with fixed effects for both days and counties, or what is commonly known as a “two-way” fixed effects model. This would be the equivalent of having a dummy/binary variable in the model for each county and day separately, or 3477 additional variables. This stunning number should give us pause: what is the point of adding thousands of variables to the model? The resulting linear model is so big that R’s default linear model function lm could not estimate it, requiring me to use the fixest package which is more memory efficient. What is happening to our interpretation of mask mandates by including so many extra variables?\nI’ve written a previous blog post about the two-way fixed effect model and panel data models in general, following on a paper I wrote with Jon Kropko on the topic, which I refer to the reader to a fuller answer to these questions. Our research and an increasing number of papers question whether the two-way fixed effects model does what people say it does, which is control for a wide array of possible omitted variables.\nThe problem is that fixed effects are not, on their own, some magic formula to get rid of inferential problems. Including fixed effects can help isolate one of two dimensions in the data: 1) variation in COVID-19 growth rates within counties over time (what the plot above shows) or 2) a cross-section of county growth rates for a given day. The first corresponds to including a dummy variable for each county, and the second including a dummy variable for each day.\nIn general, both of these comparisons are potentially interesting. Looking at growth rates within counties implies that we think COVID-19 growth rates following the introduction of mask mandates will fall. However, this comparison can be difficult because we know there are a lot of other factors that change over time within counties, such as people’s increasing awareness of the effectiveness of masks. Similarly, we would think that if mask mandates are effective, counties with mandates on a given day should have lower growth rates than counties without mandates (the cross-section). Of course, differences across counties can affect these comparisons, especially if there are reasons why some counties may want to adopt and enforce mask mandates more than others.\nThe CDC, though, included both kind of fixed effects, which makes it very hard to know what comparison they are trying to draw. In the blog post mentioned above, I show why this particular estimate is nearly impossible to understand. It would seem the CDC is to trying to address all possible inferential issues by including a cornucopia of fixed effects, but fixed effects only enable the researcher to draw comparisons, not remove all sources of bias. To deal with the possible factors that might affect our estimates, we need to think about including data that measures potential confounding variables, not just including as many fixed effects as possible and hoping for the best.\nTo illustrate this, I replicate the CDC’s analysis except that I compare a model with fixed effects for counties (within-country variation), a model with fixed effects for days (cross-sectional variation), and plot them next to the two-way fixed effects results:\nbind_rows(all_coefs_within, all_coefs_between,filter(all_coefs,Type==\u0026quot;Replication\u0026quot;)) %\u0026gt;% ggplot(aes(x=Estimate,y=Model)) + geom_errorbarh(aes(xmin=lb,xmax=upb,colour=Type),position=position_dodge(width=.8)) + geom_text(aes(label=round(Estimate,digits=2),group=Type), position=position_dodge(width=.8), size=2.5,vjust=-.75,fontface=\u0026quot;bold\u0026quot;) + theme_tufte() + geom_vline(xintercept=0,linetype=2) + scale_colour_viridis_d() + scale_x_continuous(labels=scales::percent_format(scale=1)) + labs(y=\u0026quot;Comparison Period\u0026quot;,x=\u0026quot;Percentage Change in Daily COVID-19 Cases\u0026quot;) + ggtitle(stringr::str_wrap(\u0026quot;Comparison of Mask Mandates Effects both Within-County Over-time and Daily Cross-sections\u0026quot;,width = 60)) As can be seen, the models with either fixed effects for days or counties tend to have larger and more negative estimates of the effect of mask mandates than do the two-way fixed effects models. This is of course puzzling, as if the two-way fixed effects models controls accounts for both sets of fixed effects, shouldn’t the estimate be somewhere between the two one-way models? That intuition is misleading and deeply flawed. The two-way fixed effects model combines variation in both dimensions in a way that is complex and highly non-linear, which means it could produce an estimate completely independent of both dimensions in the data. That is the reason why that estimate is hard to interpret, and frankly not the best choice for the CDC. I have no idea what the two-way estimates substantively mean–whether and to what extent they refer to variation within counties or variation across counties–and as a result I can’t also explain why they are lower than either considered separately.\n How to Do It Better To summarize the blog post so far, the CDC is trying to take seriously the fact that mask mandates were not randomly assigned to counties, and that the course of the pandemic is affecting counties in a variety of ways over time. However, their approach tries to use brute force modeling and arbitrary data subsetting. There must be a better way, you ask. Well, there is, if we are comfortable with trying first to draw relevant comparisons while acknowledging that in this situation, there probably is no way to easily remove all concerns about other factors that could affect or explain the results. However, we can still do better by addressing these inferential threats head on rather than try to sneak around them with hordes of dummy variables.\nAs mentioned earlier, there are two basic and much clearer options for panel data models: within-case county models and cross-section by day models. (Later on I will show some new work that offers other options, but we’ll stick with these for now). I chose to go with the cross-sectional models, which may be a bit surprising, but the reason has to do with available data. For the within-county over-time model, we should be concerned about rising awareness of masks and beliefs in their effectiveness. Unfortunately, there is very little over-time data on counties we can use. Most polling is only available at the state level at best. We have Google mobility data by day, but that is about it. If we are concerned that people become more compliant with mask mandates over time because they are becoming better informed about how masks help, there is no way to include data about those changing perceptions in the model.\nBy contrast, in the cross-sectional model we would need to be worried about a different issues: are there different reasons why some counties would adopt mask mandates in the first place? That is, when we compare counties with and without mask mandates, are there reasons why they adopt mask mandates and see fewer infections? The answer is, of course there are. However, we can measure these factors because they are constant over time. For my purposes, I collected three important control variables: county median income, the proportion foreign-born from the Census, and crucially, 2016 presidential vote totals for Donald J. Trump.\nIn my own work on the pandemic, which you can read more here, I show that Trump partisanship has a very strong relationshp to spread of the pandemic. In that paper I examine states, so I can look at within-state variation because I can measure changes in Trump approval ratings over time. Lacking that data for counties, though, we can still examine whether partisanship affects mask mandates–which we should expect. Counties that are more conservative are probably less likely to adopt mask mandates, and if they do adopt them, to comply with them, while the opposite probably holds true for more liberal areas. Adding in county-level income helps account for the resources a county might have to address the pandemic and also how many workers are exposed to the virus through their daily jobs as we know that higher-paid workers tended to work at home. The proportion of foreign-born is a proxy for travel patterns as foreign residents tend to live closer to areas with access to international travel, which was a cause of the early spread of COVID-19. By accounting for these factors, we can make more compelling comparisons between counties by controlling for the factors that might prompt some counties to be more likely to adopt mask mandates and to see greater levels of compliance.\nIn the code below, I include the CDC’s data with the additional control variables. I also interact mask mandate status with vote share to see if mask mandates do seem to be more effective in more liberal areas. While it would be nice to include a two-way fixed effects for comparison’s sake, the model could not estimate due to some collinearities between varibles (not clear why). Oddly enough, I could estimate such a model when using the smaller subsets of the data that the CDC did. It is really an odd duck.\nFollowing is the code I used to estimate a model with additional controls and fixed effects for days:\nd1_part_between \u0026lt;- fixest::feols( cases_diff_log ~ mask_status*vote_share + scale(test_prop) + vote_share +prop_foreign + scale(median_income) + stay_status + rest_action + bar_order + ban_gather | date, data = combined_data, cluster = \u0026quot;FIPS_State\u0026quot;, weights = ~ pop ) In the plot below I show all of the results of the model ordered by how negative or positive their association is with COVID-19 spread. What is immediately apparent is a strong and very positive association between Trump vote share and COVID-19: more conservative areas tend to see more infections after controlling for other factors. Furthermore, removing restrictions on bars is strongly associated with more infections, though we don’t see similar associations for restaurants, which may be a comfort to non-bar restaurant owners and the precautions they’ve taken. Importantly, the interaction effect on Trump vote share and mask mandates is strongly positive, suggesting that mask mandates are less effective in conservative areas.\nd1_part_between$coeftable %\u0026gt;% mutate(Variables=row.names(d1_part_between$coeftable), Variables=forcats::fct_recode(Variables, \u0026quot;Ban on \u0026gt;10 Gatherings\u0026quot; = \u0026quot;ban_gather\u0026quot;, \u0026quot;Bars Curbside Only\u0026quot; = \u0026quot;bar_orderCurbside/delivery only\u0026quot;, \u0026quot;No Bar Restrictions\u0026quot; = \u0026quot;bar_orderNo restriction found\u0026quot;, \u0026quot;Bars Open with Limitations\u0026quot; = \u0026quot;bar_orderOpen with limitations\u0026quot;, \u0026quot;Mask Mandate\u0026quot; = \u0026quot;mask_statusPublic Mask Mandate\u0026quot;, \u0026quot;Mask Mandate X Trump Vote\u0026quot; = \u0026quot;mask_statusPublic Mask Mandate:vote_share\u0026quot;, \u0026quot;Median Income\u0026quot; = \u0026quot;scale(median_income)\u0026quot;, \u0026quot;% Foreign\u0026quot; = \u0026quot;prop_foreign\u0026quot;, \u0026quot;Restaurant Curbside Only\u0026quot; = \u0026quot;rest_actionCurbside/carryout/delivery only\u0026quot;, \u0026quot;Restaurant Open with Precautions\u0026quot; = \u0026quot;rest_actionOpen with social distancing/reduced seating/enhanced sanitation\u0026quot;, \u0026quot;Stay at Home Advisory\u0026quot; = \u0026quot;stay_statusAdvisory/Recommendation\u0026quot;, \u0026quot;Stay at Home Mandatory\u0026quot; = \u0026quot;stay_statusMandatory - all people\u0026quot;, \u0026quot;Stay at Home Certain Areas\u0026quot; = \u0026quot;stay_statusMandatory - all people in certain areas of state\u0026quot;, \u0026quot;Stay at Home At-risk Certain Areas\u0026quot; = \u0026quot;stay_statusMandatory - at-risk in certain areas of state\u0026quot;, \u0026quot;Stay at Home At Risk Only\u0026quot; = \u0026quot;stay_statusMandatory - at-risk people only\u0026quot;, \u0026quot;Tests\u0026quot; = \u0026quot;scale(test_prop)\u0026quot;, \u0026quot;Trump Vote\u0026quot; = \u0026quot;vote_share\u0026quot; )) %\u0026gt;% ggplot(aes(x=Estimate,y=reorder(stringr::str_wrap(Variables,width = 25),Estimate))) + geom_errorbarh(aes(xmin=Estimate - 1.96*`Std. Error`, xmax=Estimate + 1.96*`Std. Error`)) + geom_text(aes(label=round(Estimate,digits=2)), size=2.5,vjust=-.75) + theme_tufte() + geom_vline(xintercept=0,linetype=2) + scale_colour_viridis_d() + scale_x_continuous(labels=scales::percent_format(scale=1)) + labs(y=\u0026quot;Comparison Period\u0026quot;,x=\u0026quot;Percentage Change in Daily COVID-19 Cases\u0026quot;) + ggtitle(stringr::str_wrap(\u0026quot;Trump Vote Share Interaction with Mask Mandates\u0026quot;,width = 60)) To better understand the relationship between vote share and mask mandates, I plot the sample-average marginal effects for mask mandates below. These estimates show what effect the mask mandate seemed to have across counties when we vary vote share for Donald Trump from the lowest observed value of 4.1% voting for Trump to the maximum value of 96% voting for Trump:\n# use Leeper\u0026#39;s method for calculating marginal effects # ignore uncertainty as that would require bootstrapping/jackknifing eps \u0026lt;- 1e-7 setstep \u0026lt;- function(x) { x + (max(abs(x), 1, na.rm = TRUE) * sqrt(eps)) - x } # loop over full range of vote shares combined_filter \u0026lt;- ungroup(combined_data) %\u0026gt;% select( date, mask_status, test_prop, vote_share, prop_foreign, median_income, stay_status, rest_action, bar_order, ban_gather ) %\u0026gt;% mutate(test_prop = scale(test_prop), median_income = scale(median_income)) %\u0026gt;% filter(complete.cases(.)) over_vote \u0026lt;- parallel::mclapply(seq( min(combined_filter$vote_share), max(combined_filter$vote_share), length.out = 100 ), function(v) { this_data \u0026lt;- rbind( mutate( combined_filter, vote_share = v, mask_status = \u0026quot;No Public Mask Mandate\u0026quot; ), mutate( combined_filter, vote_share = v, mask_status = \u0026quot;Public Mask Mandate\u0026quot; ) ) # get predictions and take average difference y_hat \u0026lt;- predict(d1_part_between, newdata = this_data) ld \u0026lt;- nrow(combined_filter) # marginal effect = E[y_hat under treatment - y_hat under # control] conditional on vote_share = v marg_eff \u0026lt;- mean(y_hat[(ld + 1):(2 * ld)] - y_hat[1:ld]) tibble(`Marginal Effect` = marg_eff, `Vote Share` = v) }, mc.cores = parallel::detectCores()) %\u0026gt;% bind_rows over_vote %\u0026gt;% ggplot(aes(y=`Marginal Effect`, x=`Vote Share`)) + geom_line(linetype=2) + scale_y_continuous(labels=scales::percent_format(scale=1)) + scale_x_continuous(labels=scales::percent) + theme_tufte() + labs(y=\u0026quot;Change in Growth Rates\u0026quot;, x=\u0026quot;Trump 2016 Vote Share\u0026quot;) + ggtitle(\u0026quot;Marginal Effect of Mask Mandates on COVID-19 Growth Rates\u0026quot;, subtitle = \u0026quot;Conditional on 2016 County-level Trump Vote Share\u0026quot;) Unfortunately, I don’t have uncertainty for the plot as that would require a lot more work given the fixed effects. However, the relationship is quite strong and clear: mask mandates are most effective in counties that did not vote for Trump and least effective in counties that voted the most for Trump. In those counties, mask mandates are almost ineffective at preventing COVID-19. As a result, it’s clear that the CDC should have included partisanship in their model in some way.\nAgain, this model is imperfect: there are certainly other factors we might want to include in the model, and there is always the chance that there is some other part of the picture we are missing. Such, however, is life with observational data. We can still learn from these comparisons provided we are appropriately skeptical.\nIn addition, we could consider using some newer, more nuanced methods. I turn to these next.\n Another Way: Multiple Period Diff-in-Diff Recent research on panel data models has been promoting the use of what is called multi-period difference-in-difference. Difference-in-difference modeling has a big fan base because it seems to draw a compelling and possibly causally identified comparison between units observed at two time periods, pre and post-treatment. Curiously, though few notice it, the comparisons are actually of two cross-sections over time, and so is substantively similar to the cross-sectional model I ran earlier.\nHowever, when there are more than two time periods, or when units are treated at different times, then the simple difference-in-difference model doesn’t apply. There issues are present in spades in the CDC’s analysis: many counties adopted mask mandates at different times and for different lengths of time. The CDC tried to restrict their sample to specific days, to avoid this, but because counties adopted mask mandates on different dates, they inevitably end up comparing different periods as well.\nThere is increasing research on how best to make a difference-in-difference like comparison in more complicated situations. In this section I use the work of Callaway and Sant’Anna, which has a compelling take on this question. Using their approach, we first assign counties to the same group for comparison if they adopted a mask mandate on the same day. In that way, we might think that some of the over-time issues I mentioned earlier will not matter as much. Second, we can be more up front about what we use as the reference group, either counties that on that day did not have a mask mandate or counties which never imposed a mask mandate. For our purposes, I’ll adopt the former approach given that many counties had a mask mandate at some point. As if that weren’t enough, we can even include what they call anticipation effects, which would occur if people started to change their behavior before the mask mandate was implemented.\nIn the code below I fit their model to the CDC data with an allowance for anticipation of up to 3 days. The resulting model ends up being gigantic as the 3,000 plus counties are assigned to one of a possible 200+ groups given the number of days. The model also complained that some of our groups were consequently too small, i.e., only a few counties adopted a mask mandate on that particular day. Furthermore, I only include the additional control variables like vote share, not the CDC’s policy data as it is likely to be problematic to include it when the groups are so small. As is always the case, this model is not a panacea for our concerns due to these issues, but we can still learn quite a bit from it.\n# takes about 30 minutes to estimate if(run_did) { did_est \u0026lt;- combined_data %\u0026gt;% ungroup %\u0026gt;% select(cases_diff_log,first_treat,fips_num,date_num, vote_share,test_prop,prop_foreign,median_income, stay_status,rest_action,bar_order,ban_gather) %\u0026gt;% filter(complete.cases(.)) %\u0026gt;% att_gt(yname=\u0026quot;cases_diff_log\u0026quot;, gname=\u0026quot;first_treat\u0026quot;, idname=\u0026quot;fips_num\u0026quot;, tname=\u0026quot;date_num\u0026quot;, xformla=~vote_share + test_prop + prop_foreign + median_income, # xformla=~vote_share + test_prop +prop_foreign + median_income + stay_status + rest_action + bar_order + ban_gather, control_group=\u0026quot;notyettreated\u0026quot;, anticipation=3,panel=TRUE,allow_unbalanced_panel = F, cores=4,weightsname = NULL, est_method=\u0026quot;reg\u0026quot;, data=.) saveRDS(did_est,\u0026quot;data/did_est.rds\u0026quot;) } else { # File is too big to store on github, to download use this link: # https://drive.google.com/file/d/18GhUFbcLbx4a27Y5LJ2Ov_Iexn0ws6mb/view?usp=sharing did_est \u0026lt;- readRDS(\u0026quot;data/did_est.rds\u0026quot;) } We then end up with separate estimates for mask mandate for each of the 3,000+ groups. This is cool, though not particularly useful for us. What I like about Callaway and Sant’Anna’s work though is that they are more transparent about the different dimensions of panel data (even if they tend not use that terminology). Given the group-level estimates, they have a way of aggregating these to an effect of the mask mandate for each day of the sample. I show these in the plot below.\n# we need to aggregate and plot these # some bug in the package requires this to be loaded library(DRDID) library(matrixStats) library(MatrixModels) ag_did \u0026lt;- aggte(did_est,type=\u0026quot;dynamic\u0026quot;) ag_overall \u0026lt;- aggte(did_est,type=\u0026quot;group\u0026quot;) ggdid(ag_did) + theme_tufte() + theme(axis.ticks.x = element_blank()) + scale_x_continuous(breaks=c(-200,-100,-50,-25,0,25,50,100,200)) + geom_hline(yintercept=0,linetype=2) + xlab(\u0026quot;Days Before/After Mask Mandate\u0026quot;) This plot encapsulates a much more nuanced set of comparisons than those considered previously. At the same time, the uncertainty in the estimates means there is also only so much we can learn. It would appear that at earlier and later time periods, mask mandates were associated with more COVID-19, but its impossible to say for sure. One approach to deal with this could be to collapse the data to weeks and then estimate a model, which might pool the data somewhat to get more precise answers.\nWhat we can also do with this model is combine the estimates into a single, overall effect. I show that in the table below.\ntibble(`Average Treatment Effect for Treated`=ag_overall$overall.att, `5% CI Lower`=ag_overall$overall.att - 1.96*ag_overall$overall.se, `95% CI Upper`=ag_overall$overall.att + 1.96*ag_overall$overall.se) %\u0026gt;% knitr::kable(digits=2) %\u0026gt;% kableExtra::kable_styling()   Average Treatment Effect for Treated  5% CI Lower  95% CI Upper      -0.33  -0.94  0.28     This table shows that it is most likely there is a negative association for mask mandates with COVID-19: hurray! Unfortunately, the estimate is fairly imprecise and it could also be associated with increased disease spread. As is so often the case, data analysis does not always produce definitive answers. However, I believe it is most important to be open about what we know and what we don’t know, which is why I spent the time to write this post. On the whole, the evidence suggests a negative association between mask mandates and COVID-19 spread, but we need to do a better job collecting additional data to allow us to do informed comparisons at the county level.\n ","date":1615561200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615561200,"objectID":"265e7414a228b2467b564c7e52624ee7","permalink":"http://www.robertkubinec.com/post/cdc_fe/","publishdate":"2021-03-12T15:00:00Z","relpermalink":"/post/cdc_fe/","section":"post","summary":"While perusing the news, I read eagerly about the CDC’s recent study examining associations between county-level mask mandates and COVID-19 growth rates. This study has already been the subject of angry retorts from the restaurant industry due to the CDC’s claim that restaurant closures reduced COVID-19 spread.","tags":null,"title":"The Good, the Bad and the Ugly in the CDC's Face Mask Study","type":"post"},{"authors":null,"categories":null,"content":"   You can access RBloggers, an R consortium of bloggers, at this link.\n","date":1606489200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606489200,"objectID":"4a101d2920e978840bd9613a8b6dcc16","permalink":"http://www.robertkubinec.com/post/rbloggers/","publishdate":"2020-11-27T15:00:00Z","relpermalink":"/post/rbloggers/","section":"post","summary":"You can access RBloggers, an R consortium of bloggers, at this link.","tags":null,"title":"Linking to RBloggers","type":"post"},{"authors":null,"categories":null,"content":"   In this blog post, I use Gelman and Loken’s garden of forking paths analysis to construct a simulation showing why skepticism of AstraZeneca’s vaccine results is warranted at this early stage. This simulation uses the numbers from their press release to illustrate how noise in treatment regimes can undermine serendipitous results. As Gelman and Loken describe, researchers can stumble on conclusions which they are then very able to provide post hoc justifications for. Unfortunately, when incentives to get results are strong, human beings are also very likely to focus on the positive at the expense of obtaining the full picture.\nSimilar to my last post on Pfizer’s vaccine, I start with Eric Novik’s excellent blog post on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:\n\\[ VE = 1 - \\frac{p_t}{p_c} \\]\nWhere \\(p_t\\) is the proportion of cases in the treatment (vaccinated) group with COVID-19 and \\(p_c\\) is the proportion of cases in the control (un-vaccinated) group). In essence, if we assume the vaccinated group will have no more cases than the control group, this statistic will converge to 1 as \\(p_t\\) goes to zero, so VE of 100% would be a case in which there are no cases in the treatment group.\nThey don’t give us all the information to figure out how many people were infected with COVID-19 in treatment versus control, but we can infer \\(p_t\\) and \\(p_c\\) by solving two equations given the fact that we know that the proportion infected in the whole trial was equal to \\(\\frac{131}{11636}\\) and the overall VE was 0.7:\n\\[\\begin{align} 1 - \\frac{p_t}{p_c} \u0026amp;= 0.7\\\\ p_t + p_c \u0026amp;= \\frac{131}{11636} \\end{align}\\]\nThankfully, this system has a single solution where \\(p_t\\) is equal to (~0.0026) and \\(p_c\\) is equal to (~0.0087). In other words, the proportion infected in the treatment group was about four times less than the control group. We’ll assume that these distributions are the correct ones, and then sample subgroup-varying \\(p_{ti}\\) and \\(p_{ci}\\) from Beta distributions that are fairly tight around these values. This will allow us to model treatment heterogeneity within the sample. The distribution of possible VEs given subgroup heterogeneity can be seen in the plot below:\n# use the mean/variance parameterization of the beta distribution pt \u0026lt;- 393/151268 pc \u0026lt;- 655/75634 # Generate values for VE given beta distributions for pt/pc VE \u0026lt;- 1 - rbeta(10000,pt*5000,(1-pt)*5000)/rbeta(10000,pc*5000,(1-pc)*5000) hist(VE) This plot shows that the true VE in the population is on average 0.7 but could vary substantially. It could be below 0.5 for a small subset of the population and above 0.9 for a small subset of the population. To simulate our data, we will draw VEs from this beta distribution separately for each of four possible subgroups \\(i \\in \\{1, 2, 3, 4\\}\\) in a hypothetical study:\n\\[ VE_i \\sim 1 - \\frac{Beta(5000p_{ti},5000(1-p_{ti}))}{Beta(5000p_{ci},5000(1-p_{ci}))} \\]\nFour subgroups are chosen to be roughly equal to the size of the half-dose group in the AstraZeneca study given a sample size of 11,636. I add a further step in that I assume that the probability of VE being reported for a subgroup \\(i\\), which I term \\(Pr(r=1)\\), is increasing in the rate of VE. That is, as VE rises, the subgroup analysis is more likely to be reported. For these reasons, we can distinguish between the full range of VE estimates for a subgroup \\(i\\), \\(VE_i\\), and the particular subgroup \\(i\\) that is reported, \\(VE_{ir=1}\\):\n\\[ Pr(VE_{ir}) = Pr(r|VE_i)VE_{ir=1} + (1 - Pr(r|VE_i))VE_{ir=0} \\]\nThis equation shows that the extent of the bias in using only reported results is equal to \\((1 - Pr(r|VE_i))VE_{ir=0}\\), or the probability that a result won’t be reported times the level of VE in the non-reported subgroups.\nThe R code to generate this model in terms of treatment/control COVID cases and whether or not subgroup analyses are reported is as follows:\n# number of samples N \u0026lt;- 1000 sim_data \u0026lt;- lapply(1:N,function(n) { # generate subgroup treatment/control COVID proportions sub_pt \u0026lt;- rbeta(4,pt*5000,(1-pt)*5000) sub_pc \u0026lt;- rbeta(4,pc*5000,(1-pc)*5000) # generate subgroup VEs VE \u0026lt;- 1 - sub_pt/sub_pc # generate COVID case data with binomial distribution covid_t \u0026lt;- sapply(sub_pt, function(pt) rbinom(n=1,size = floor(11636/4),prob=pt)) covid_c \u0026lt;- sapply(sub_pc, function(pc) rbinom(n=1,size = floor(11636/4),prob=pc)) # output data tibble(draw=n, groups=1:4, sub_pt=sub_pt, sub_pc=sub_pc, VE=VE, VE_r=as.numeric(runif(n=4)\u0026lt;plogis(-250 + 300*VE)), covid_t = covid_t, covid_c=covid_c) }) %\u0026gt;% bind_rows Given this simulation, only a minority (4%) of subgroup analyses are reported. The average VE for the reported subgroups is 77% as opposed to a VE of 69% across all simulations. As such, the simulation assumes that it is unlikely that subgroups with low vaccine efficacy will end up being reported, while subgroups with stronger efficacy are more likely to be reported.\nFor each random draw from the simulation, I can then fit a model that analyses only those analyses that are reported:\n# modified code from Eric Novik vac_model \u0026lt;- \u0026quot;data { int g; // number of sub-groups int\u0026lt;lower=0\u0026gt; r_c[g]; // num events, control int\u0026lt;lower=0\u0026gt; r_t[g]; // num events, treatment int\u0026lt;lower=1\u0026gt; n_c[g]; // num cases, control int\u0026lt;lower=1\u0026gt; n_t[g]; // num cases, treatment } parameters { vector\u0026lt;lower=0, upper=1\u0026gt;[g] p_c; // binomial p for control vector\u0026lt;lower=0, upper=1\u0026gt;[g] p_t; // binomial p for treatment } transformed parameters { real VE = mean(1 - p_t ./ p_c); // average vaccine effectiveness across groups } model { p_t ~ beta(2,2); // weakly informative, p_c ~ beta(2,2); // centered around no effect r_c ~ binomial(n_c, p_c); // likelihood for control r_t ~ binomial(n_t, p_t); // likelihood for treatment } generated quantities { vector[g] effect = p_t - p_c; // treatment effect vector[g] log_odds = log(p_t ./ (1 - p_t)) - log(p_c ./ (1 - p_c)); }\u0026quot; to_stan \u0026lt;- cmdstan_model(write_stan_file(vac_model)) est_bias \u0026lt;- lapply(unique(sim_data$draw), function(i) { sink(\u0026quot;output.txt\u0026quot;) this_data \u0026lt;- filter(sim_data,draw==i,VE_r==1) stan_data \u0026lt;- list(g=length(unique(this_data$groups)), r_c=as.array(this_data$covid_c), r_t=as.array(this_data$covid_t), n_c=as.array(round(rep(floor(11636/4), length(unique(this_data$groups)))/2)), n_t=as.array(round(rep(floor(11636/4), length(unique(this_data$groups)))/2))) if(stan_data$g\u0026lt;1) { tibble(draw=i, VE=NA) } else { est_mod \u0026lt;- to_stan$sample(data=stan_data, seed=624018, chains=1,iter_warmup=500,iter_sampling=1000,refresh=500) draws \u0026lt;- est_mod$draws() %\u0026gt;% as_draws_df tibble(draw=i, VE=draws$VE) } sink() }) %\u0026gt;% bind_rows I can then plot the density of the estimated reported VE from this simulation along with a line indicating the true average VE in the population:\nest_bias %\u0026gt;% ggplot(aes(x=VE)) + geom_density(fill=\u0026quot;blue\u0026quot;,alpha=0.5) + geom_vline(xintercept=mean(sim_data$VE),linetype=2) + theme_tufte() + xlim(c(0,1)) + xlab(\u0026quot;Reported VE\u0026quot;) + ylab(\u0026quot;\u0026quot;) + annotate(\u0026quot;text\u0026quot;,x=mean(sim_data$VE),y=3,label=paste0(\u0026quot;True VE = \u0026quot;,round(mean(sim_data$VE)*100,1),\u0026quot;%\u0026quot;)) As can be seen, it is much more likely that VEs higher than the true average VE will be reported. The extent of the bias is driven by the simulation and how much weight the simulation puts on discovering high VE values. Given the profit that AstraZeneca stands to gain, and the fame and prestige for the involved academics, it would seem logical that reported analyses from subgroups will tend to be subgroups that out-perform the average.\nNote that this simulation shows how AstraZeneca could be reporting valid statistical results, yet these results can still be a biased estimate of what we want to know, which is how the vaccine works for the population as a whole. The possibility of random noise in subgroups and the fact that subgroups are likely to respond differently means that we should be skeptical of analyses that only report certain subgroups instead of all subgroups. Only when we understand the variability in subgroups can we say whether the reported finding in AstraZeneca’s press release represents a real break-through or simply luck. Of course, they can test it directly by doing another trial, which it seems to be is their intention. Serendipity, though, isn’t enough of a reason to trust these results unless we can examine all of their data.\n","date":1606489200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606489200,"objectID":"9d961e4fed989cf5fdaf483f85e08b4a","permalink":"http://www.robertkubinec.com/post/astrazeneca/","publishdate":"2020-11-27T15:00:00Z","relpermalink":"/post/astrazeneca/","section":"post","summary":"In this blog post, I use Gelman and Loken’s garden of forking paths analysis to construct a simulation showing why skepticism of AstraZeneca’s vaccine results is warranted at this early stage.","tags":null,"title":"Why People Are Doubting the AstraZeneca Vaccine Report","type":"post"},{"authors":null,"categories":null,"content":"   I start with Eric Novik’s excellent blog post on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:\n\\[ VE = 1 - \\frac{p_t}{p_c} \\]\nWhere \\(p_t\\) is the proportion of cases in the treatment (vaccinated) group with COVID-19 and \\(p_c\\) is the proportion of cases in the control (un-vaccinated) group). This is kind of an odd statistic as it would seem to make more sense to simply report the ratio rather than one minus the ratio, although subtracting one means a null ratio (difference of 1) equals zero. I am not sure a general audience is very aware of the distinction and what VE in fact means. In essence, if we assume the vaccinated group will have no more cases than the control group, this statistic will converge to 1 as \\(p_t\\) goes to zero, so VE of 100% would be a case in which there are no cases in the treatment group.\nAs Eric Novik shows, the VE is actually a calculated quantity and isn’t modeled directly. The clinical pre-registration says they will use a beta-binomial model. The put a prior on a transformation of the VE (vaccine efficacy) of \\(Beta(0.700102, 1)\\). We can get an idea of what that looks like by examining the distribution of these values:\nprior_VE \u0026lt;- rbeta(10000,.700102,1) round(quantile((2*prior_VE - 1)/(prior_VE - 1),probs=seq(0,1,by=.05)),5) ## 0% 5% 10% 15% 20% 25% ## -4994.45563 -12.93514 -5.62962 -3.11045 -1.86012 -1.15668 ## 30% 35% 40% 45% 50% 55% ## -0.63120 -0.28035 -0.00591 0.19966 0.36281 0.50447 ## 60% 65% 70% 75% 80% 85% ## 0.61128 0.69638 0.76767 0.83150 0.88354 0.92454 ## 90% 95% 100% ## 0.95870 0.98584 1.00000 This prior puts significant mass at negative VEs which would suggest that the vaccine actually makes things worse for the treatment group than for the control group. While this would seem unrealistic, it also suggests that very high VEs, such as above 90%, are relatively unlikely. We can see that the prior suggests it is about as likely that the VE is negative (the vaccine causes the virus) as it is likely that the VE is greater than 80%.\nGiven this conservative but weakly informative prior, we can then calculate a p-value for what they pre-register in their study, which is the probability that the VE (vaccine efficacy) is greater than 30%. We can then fit a Bayesian beta-binomial model with these priors by modifying Eric Novik’s original code:\n# from Eric Novik, changed the prior per pre-reg vac_model \u0026lt;- \u0026quot;data { int\u0026lt;lower=1\u0026gt; r_c; // num events, control int\u0026lt;lower=1\u0026gt; r_t; // num events, treatment int\u0026lt;lower=1\u0026gt; n_c; // num cases, control int\u0026lt;lower=1\u0026gt; n_t; // num cases, treatment real a[2]; // prior values for treatment effect } parameters { real\u0026lt;lower=0, upper=1\u0026gt; p_c; // binomial p for control real\u0026lt;lower=0, upper=1\u0026gt; p_t; // binomial p for treatment } transformed parameters { real VE = 1 - p_t / p_c; // vaccine effectiveness } model { (VE - 1)/(VE - 2) ~ beta(a[1], a[2]); // prior for treatment effect r_c ~ binomial(n_c, p_c); // likelihood for control r_t ~ binomial(n_t, p_t); // likelihood for treatment } generated quantities { real effect = p_t - p_c; // treatment effect real log_odds = log(p_t / (1 - p_t)) - log(p_c / (1 - p_c)); }\u0026quot; to_stan \u0026lt;- cmdstan_model(write_stan_file(vac_model)) n \u0026lt;- 4.4e4 # number of volunteers r_c \u0026lt;- 162 # number of events in control r_t \u0026lt;- 8 # number of events in vaccine group stan_data \u0026lt;- list(n=n,r_c=r_c,r_t=r_t, n_c=n/2,n_t=n/2, a=c(.700102,1)) Then we can sample and plot the results. We’ll draw a lot of samples so we can get good estimates in the tails (i.e. very low p-values).\npfizer_est \u0026lt;- to_stan$sample(data=stan_data,chains=1,iter_warmup=500,iter_sampling=100000,refresh=50000,show_messages=F) ## Running MCMC with 1 chain... ## ## Chain 1 Iteration: 1 / 100500 [ 0%] (Warmup) ## Chain 1 Iteration: 501 / 100500 [ 0%] (Sampling) ## Chain 1 Iteration: 50500 / 100500 [ 50%] (Sampling) ## Chain 1 Iteration: 100500 / 100500 [100%] (Sampling) ## Chain 1 finished in 2.1 seconds. draws \u0026lt;- pfizer_est$draws() %\u0026gt;% as_draws_df Here’s a plot of the VE as reported in the press release with the dotted line as the average estimate:\nmean_ve \u0026lt;- mean(draws$VE) draws %\u0026gt;% ggplot(aes(x=VE)) + geom_density(fill=\u0026quot;blue\u0026quot;,alpha=0.5)+ theme_tufte() + ylab(\u0026quot;\u0026quot;) + scale_x_continuous(labels=scales::percent_format(accuracy = 1)) + geom_vline(aes(xintercept=mean(VE)),linetype=2) + annotate(\u0026quot;text\u0026quot;,x=mean_ve,y=12,label=paste0(\u0026quot; VE = \u0026quot;,round(mean_ve*100,1),\u0026quot;%\u0026quot;)) This estimate matches the press release. So let’s calculate the p-value:\nmean(1 - as.numeric(draws$VE\u0026gt;.3)) ## [1] 0 Virtually nil.\nHowever, we can also test some other thresholds, such as the FDA value of 50% VE:\nmean(1 - as.numeric(draws$VE\u0026gt;.5)) ## [1] 0 Still vanishingly small. We can try something a bit closer, such as whether VE exceeds 90%:\nmean(1 - as.numeric(draws$VE\u0026gt;.9)) ## [1] 0.01913 This p-value is at least reportable at 0.02. As this is a Bayesian model, this can be interpreted directly that the probability that the vaccine efficacy is less than 90% is quite small, less than a 2% chance.\n","date":1605798000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605798000,"objectID":"198c5b50c82f3bb41ef51549ca147718","permalink":"http://www.robertkubinec.com/post/vaccinepval/","publishdate":"2020-11-19T15:00:00Z","relpermalink":"/post/vaccinepval/","section":"post","summary":"I start with Eric Novik’s excellent blog post on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:\n\\[ VE = 1 - \\frac{p_t}{p_c} \\]","tags":null,"title":"A More Realistic P-Value for the Pfizer Vaccine Report","type":"post"},{"authors":null,"categories":null,"content":"   My wife and I have been faced with a decision in our pregnancies that has always caused me some consternation: should we take the MaterniT 21 test to see if our baby might have Down’s syndrome (trisomy 21) or other genetic abnormalities? This test, marketed by the company LabCorp, offers a way to analyze the blood of a pregnant woman for DNA markers that could indicate genetic problems. While non-invasive, it isn’t cheap (at least in the United States), and if it returns a positive result, it could be used for more invasive procedures like amniocentesis that could be harmful to the fetus.\nSo while it might seem like a good idea (let’s learn as much as we can), the test can only guide decisions if it’s reasonably accurate. However, due to corporate greed and manipulated statistics, people put way too much trust in the test when they probably shouldn’t. As a result, women and their partners may be led to make unwise decisions based on misplaced confidence in the test.\nIn this post I explain why you shouldn’t buy into the company’s marketing about the test’s accuracy–in fact, LabCorp is patently misleading consumers about how useful its test is for diagnosing Down’s syndrome in utero. It will take a bit of time to read through this post, so I’ll give the highlights first:\n For women under 30, even if they get a positive result from the MaterniT 21 test, it’s still more likely that the fetus does not have Down’s syndrome. Only for women over 40 does the test provide reasonably conclusive results. The misleading statistics put out by LabCorp might convince some women to do dangerous, invasive follow-on testing (or even an abortion) based on misplaced confidence in the test.  Investigating LabCorp LabCorp (under its subsidiary Integrated Genetics) offers a brochure for the test on its website. This brochure mentions how accurate the test is in only one place:\nWhile they don’t define what the technical terms are in the parentheses, 97.9% does sound pretty good. In another part of the brochure, the company says that “the test delivers clear positive or negative results.” I take this as enough evidence to say that LabCorp wants women to believe that the test will determine whether the fetus has Down’s syndrome with reasonable confidence, such as a greater than 90% probability.\nNow for the fun part–I’m going to break down what is meant by the number cited in the brochure above and trace it back to the original research to see what it actually means. You may have noticed the footnote in the image above–that is a reference to an article published in 2014 that evaluated tests like MaterniT 21, known as cell-free DNA tests. A scan of the article’s results shows that LabCorp did indeed get the number right–97.9% for trisomy 21. They did not, of course, mention that the article found much lower rates of detection for other trisomies and genetic conditions, but for now we’ll ignore that oversight. The test is marketed as a way to detect Down’s syndrome, so let’s focus on just that as the test’s selling point.\nAs I was doing research, I found a more recent paper that did a meta-analysis, which is a compilation of the results of all previous research, in this case on tests like MaterniT 21. This paper is quite useful for evaluating the test as it aggregates information from many different researchers, lowering the concern that any one study was a fluke. It turns out that this meta-analysis has a number very close to what the brochure article cited – 99.2%, i.e., even higher.\nAt this point you may think, “your concerns about this test seem unfounded.” Ah, but we have not yet discussed what these numbers actually mean, which if you notice, the brochure did not provide any more information about. All it said was 97.9% positive-predictive value for a “high-risk cohort.”\nMy intuition is that relatively few readers of the brochure would have any idea what that refers to. Most people just want to know how accurate the test is, and 97.9% sounds really high, so it must be good. In reality, it is much harder to know how well a test works, because the usefulness of a test depends on how hard it is to find what the test detects.\nExplaining this point is difficult, so I’ll use as an example that is easier to relate to. I’m sure you’ve come across the hand-held metal detectors of the kind that people like to use for treasure hunting on beaches (see picture below). Suppose you are a person who has taken up an interest in treasure hunting and you want to know how useful these metal detector will be in helping you find Blackbeard’s long-lost hoard.\nWhat you really want to know is, if the thing beeps, does it mean you’ve found treasure? The answer to this question just so happens to be the same calculation as the “positive-predictive value” that the test company put in their brochure.\nThe detector can easily go wrong if it beeps when it comes across a worthless piece of scrap metal. On the other hand, the detector could also fail if it didn’t beep because a treasure hoard was buried too deep. The first case is called a false positive: the machine beeped but you didn’t find any treasure. The second case is called a false negative: you were standing on treasure and the machine didn’t help you out one bit.\nSo let’s get back to the brochure. The positive predictive value is equal to the chance that your metal detector’s beep means you hit pay dirt. But if you ask a metal detector salesperson what the chance is that you find treasure if the machine beeps, you might get some evasive answers/sarcastic smirks. He or she certainly won’t be able to tell you how likely it is that you find treasure, unless… the salesperson actually knows how much treasure there is in the sand (in which case you might wonder why they are still a salesperson). The point is, you can only calculate a positive predictive value for your detector if you know how likely it is you’ll be standing on treasure at any point along the beach. If treasure is impossibly rare–which it probably is, I hate to break it to you–you’ll be digging up a lot of empty sand or rusty beer cans even if the machine’s sensors are working fine.\nThe point of this whole analogy is that knowing what you should do if the machine beeps depends on how much treasure is in the sand. If you knew that you were standing right next to a sunken pirate ship, you would probably want to dig up every beep you hear. If you’re on the Jersey shore and the sand is littered with old Dr. Pepper cans and fish hooks, you might not want to dig up anything unless you need the exercise.\nNow I can finally explain the test company’s deception. They provided a positive predictive value, which is what you want to know: what’s the chance the fetus has Down’s syndrome if the test comes back positive? But LabCorp pulled this number from a high-risk cohort. Doing so is no different than selling you a metal detector and telling you it’ll detect treasure 97% percent of the time so long as you’re standing on Blackbeard’s grave.\nSo where did this high-risk cohort come from? What is very important to know (and completely ignored by the test brochure) is the fact that the risk of Down’s syndrome increases dramatically with age. The chart below is from the CDC, and you can see that the rate of a live birth with Down’s syndrome increases very rapidly once a woman reaches 40 years of age.\nNow let’s go back to the papers they cited as proof of the positive predictive value. I’ll only focus on the meta-analysis as it represents a lot of combined research. Unfortunately, and to me somewhat surprisingly, the paper does not report the median age of women in the study. However, because we know the actual number of women who had children with Down’s syndrome from the paper, we can figure out how old the women were by calculating the percentage who ended up having babies with Down’s. I calculate this in the code below based on a table of data in the article. The code is shown for transparency’s sake, and I include the full table at the end of this blog post as a reference.\njournal_data \u0026lt;- readxl::read_xlsx(\u0026quot;downs_table.xlsx\u0026quot;) print(paste0(\u0026quot;Percentage of live births with Down\u0026#39;s Syndrome in the meta-analysis study is: \u0026quot;,round(sum(journal_data$positives)/(sum(journal_data$positives) + sum(journal_data$negatives)),3)*100,\u0026quot;%\u0026quot;)) ## [1] \u0026quot;Percentage of live births with Down\u0026#39;s Syndrome in the meta-analysis study is: 4.6%\u0026quot; A rate of 4.6% of babies with Down’s syndrome means that the women in the study were probably 46 years old on average, according to data from the National Down Syndrome Society. By comparison, the CDC says that the rate of Down’s syndrome among newborns for the population as a whole is only 0.001%. The fetuses in the studies in this meta-analysis had a rate of Down’s syndrome 50 times higher than average. In other words, the testing company used the equivalent of a beach full of buried treasure to evaluate the usefulness of its metal detector.\nAt this point, you might start to feel suspicious about the articles I cited. Were the scientists running these studies in cahoots with the testing company? Probably not. Doing these kinds of studies is expensive–they have to enroll women, then test them, then evaluate their babies after delivery–and if they had an average set of women, they wouldn’t have very many fetuses that would ultimately turn out to have Down’s, requiring them to enroll many more subjects in the study. In a similar sense, if you were designing a metal detector, you might go out and test it by burying some gold coins in the beach and see if it works rather than stroll aimlessly around until you found treasure. So we shouldn’t necessarily blame the scientists, though we certainly should question the LabCorp’s intentionally deceptive use of the study’s statistics in selling the test.\nWe can get more useful positive predictive values by re-calculating the results from the article above and adjusting the Down’s syndrome prevalence to match younger women’s ages. I calculate this in the code block below and plot the positive predictive value by age:\njournal_data \u0026lt;- mutate(journal_data,detection=as.numeric(stringr::str_extract(detection,\u0026quot;[0-9]+\u0026quot;)), fpr=as.numeric(stringr::str_extract(fpr,\u0026quot;[0-9]+\u0026quot;))) sensitivity \u0026lt;- sum(journal_data$positives)/(sum(journal_data$positives) + (sum(journal_data$positives) - sum(journal_data$detection))) specificity \u0026lt;- sum(journal_data$negatives)/(sum(journal_data$negatives) + sum(journal_data$fpr)) # need Down\u0026#39;s syndrome prevalence data by age # Data taken from: https://journals.sagepub.com/doi/abs/10.1136/jms.9.1.2 # Using predicted values from model vs. observed values # Morris, Dutton and Alberman (2002) age_trends \u0026lt;- readxl::read_xlsx(\u0026quot;downs_estimates.xlsx\u0026quot;) %\u0026gt;% filter(age\u0026gt;13, age\u0026lt;53) %\u0026gt;% mutate(pred_ods=as.numeric(stringr::str_remove(pred_ods,\u0026quot;1:\u0026quot;)), # convert odds to percentages pred_ods=1/pred_ods, # false positive rate fp=(1 - pred_ods)*(1 - specificity), ppv=pred_ods / (pred_ods + fp)) age_trends %\u0026gt;% ggplot(aes(y=ppv,x=age)) + geom_line() + ggtitle(\u0026quot;Chance of Fetus with Downs if MaterniT 21 Comes Back Positive\u0026quot;) + scale_y_continuous(labels=scales::percent) + geom_hline(yintercept=0.5,linetype=2) + geom_hline(yintercept=0.979,linetype=3,colour=\u0026quot;red\u0026quot;) + theme(panel.background = element_blank()) + ylab(\u0026quot;Chance of Fetus with Down\u0026#39;s Syndrome\u0026quot;) + xlab(\u0026quot;Age of Mother\u0026quot;) + annotate(\u0026quot;text\u0026quot;,x=c(20,42),y=c(.52,.99),label=c(\u0026quot;50%\u0026quot;,\u0026quot;LabCorp Brochure: 97.9%\u0026quot;)) As can be seen, the chance that a fetus has Down’s Syndrome if the test comes back positive–the positive predictive value–does not start to rise sharply until a woman is in her late 30s and 40s. Even in her early 30s, a positive test result only means there is a 50 percent chance that the fetus has Down’s Syndrome. I calculated these numbers from the meta-analysis that showed the test was pretty accurate, so the numbers could even be lower with the results of a different, less optimistic study.\nYou can see now that the test company’s positive predictive value of 97.9% only applies to women at the far end of the curve–around age 42 or 43 and higher. For the vast majority of women in child-bearing years, this number is simply inaccurate. Technically, the test company can claim they included the proviso “in a high-risk cohort,” but it seems very unlikely anyone would go as far as I did to know what that means. And it turns out, it means a lot. For women under 30, it’s still more likely than not that their fetus is free of Down’s even if the test comes back positive. If women and their partners were told, “even if the test comes back positive, there’s still only a 40 percent chance the fetus has Down’s Syndrome,” I do wonder how many people would still choose to take it.\nThe real punch, though, is that this non-invasive test can be used as a reason to do a more dangerous, invasive test. One potential test a doctor might recommend next, amniocentesis, has a risk of miscarriage as high as 1 percent. Let’s suppose that a million women under 30 take this test. According to my analysis above, there would probably be 832 false positive results (test results that came back positive but the fetus does not have Down’s Syndrome). If everyone went on to do amniocentesis to confirm the diagnosis, that would result in 8 miscarriages of otherwise healthy pregnancies. Considering that there are about 4 million live births per year in the United States, and the average age of a woman giving birth is 27, these disturbing numbers are quite plausible.\nWhile it would seem that most people (and their doctors) would choose more invasive testing to confirm Down’s before reaching conclusions, it still strikes me as entirely plausible that someone would consider aborting the fetus based on the first, non-invasive positive result. That particular possibility frightens me, that someone might take the first relatively inaccurate test as a reason to terminate a pregnancy.\nFor all these reasons, do share this research with others. It’s important to know what tests really do, and also to pressure Congress and regulators to force test providers to release clear and non-misleading statistics. It should be as easy for them to compile a chart like the one I did above. In fact, these kinds of plots should be included in all scholarly research on tests to avoid people mis-characterizing results.\n Down’s Syndrome Data Table This table derived from Table 2 in Gil, Quezada, Revello, Akolekar, and Nicolaides (2015).\n  Study Method GA True Positives Detected True Negatives False Positives    Chiu (2011)41 MPSS 13 (—) 86 86 146 3  Ehrich (2011)42 MPSS 16 (8–36) 39 39 410 1  Palomaki (2011)43 MPSS 15 (8–21) 212 209 1471 3  Sehnert (2011)44 MPSS 15 (10–28) 13 13 34 0  Ashoor (2012)45 CSS 12 (11–13) 50 50 347 0  Bianchi (2012)46 MPSS 15 (10–23) 89 89 404 0  Jiang (2012)48 MPSS — (10–34) 16 16 887 0  Lau (2012)49 MPSS 12 (11–28) 11 11 97 0  Nicolaides (2012)50 CSS 12 (11–13) 8 8 1941 0  Norton (2012)51 CSS 16 (10–38) 81 81 2888 1  Sparks (2012)53 CSS 18 (11–36) 36 36 131 0  Guex (2013)55 MPSS 12 (11–13) 30 30 146 0  Liang (2013)57 MPSS 21 (11–39) 39 39 367 0  Nicolaides (2013)59 SNP 13 (11–13) 25 25 204 0  Song (2013)61 MPSS 16 (11–21) 8 8 1733 0  Verweij (2013)62 CSS 14 (10–28) 18 17 486 0  Bianchi (2014)63 MPSS 17 (8–39) 5 5 1947 6  Comas (2014)64 CSS/SNP 14 (9–23) 4 4 311 0  Pergament (2014)71 SNP 14 (7–40) 58 58 905 0  Porreco (2014)72 MPSS 17 (9–37) 137 137 3185 3  Shaw (2014)73 MPSS \u0026gt; 12 11 11 184 0  Stumm (2014)74 MPSS 15 (11–32) 41 40 430 0  Quezada (2015)75 CSS 10 (10–11) 32 32 2753 1  Song (2015)76 MPSS 9 (8–12) 2 2 201 0     ","date":1600009200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1600009200,"objectID":"a60b9fc7e75faf5ae823b86eb7234141","permalink":"http://www.robertkubinec.com/post/maternity21/","publishdate":"2020-09-13T15:00:00Z","relpermalink":"/post/maternity21/","section":"post","summary":"My wife and I have been faced with a decision in our pregnancies that has always caused me some consternation: should we take the MaterniT 21 test to see if our baby might have Down’s syndrome (trisomy 21) or other genetic abnormalities?","tags":null,"title":"Why You Should Be Careful About the MaterniT 21 Test","type":"post"},{"authors":null,"categories":null,"content":"   We’ve all been in that seminar where the author puts up a slide containing regression coefficients, and buried in the bottom line of the table we can see little Ys and Ns indicating the kind of panel data model employed. Quite often, these brief descriptions of models are taken as a mere statistical ritual, believed to be efficacious at appeasing the mercurial statistical deities, but rarely if ever investigated more thoroughly. Furthermore, no one wants to be in the difficult position of discussing panel data models, which inevitably boils down to a conversation laced with econometric terms generating more heat than light.\nSo what if I told you … panel data, or data with two dimensions, such as repeated observations on multiple cases over time, is really not that complicated. In fact, there are only two basic ways to analyze panel data, which I will explain briefly in this piece, just as every panel dataset has two basic dimensions (cases and time). However, when we confuse these dimensions, bad things can happen. In fact, one of the most popular panel data models, the two-way fixed effects model–widely used in the social sciences–is in fact statistical nonsense because it does not clearly distinguish between these two dimensions. This statement should sound implausible to you–really?, but it’s quite easy to demonstrate, as I’ll show you in this post.\nThis blog post is based on a recent publication with Jonathan Kropko which you can access here. In this post I provide a more reader-friendly overview of the article, dropping the academic-ese and focusing on substance as I think there are important issues for many people doing research.\nIn short: there are an untold number of analyses of panel data affected by an issue that is almost impossible to identify because R and Stata obscure the problem. Thanks to multi-collinearity checks that automatically drop predictors in regression models, a two-way fixed effects model can produce sensible-looking results that are not just irrelevant to the question at hand, but practically nonsense. Instead, we would all be better served by using simpler 1-way fixed effects models (intercepts on time points or cases/subjects, but not both).\nHow We Think About Panel Data Panel data is one of the richest resources we have for observational inference in the social and even natural sciences. By comparing cases to each other as they change over time, such as countries and gross domestic product (GDP), we can learn much more than we can by only examining isolated cases. However, researchers for decades have been told by econometricians and applied statisticians that they must use a certain kind of panel data model or risk committing grave inferential errors. As a result, probably the most common panel data model is to include case fixed effects, or a unique intercept (i.e. a dummy variable) for every case in the panel data model. The second most likely is to include a fixed effect or intercept for every time point and case in the data. The belief is that including all these intercepts will control for omitted variables because there are factors unique to cases or time points that will be “controlled for” by these intercepts.\nThe result of this emphasis on the lurking dangers of panel data inference results in what I have decided to call the Cheeseburger Syndrome. This phenomenon was first studied by Saturday Night Live in a classic 1980s skit:\n Applied researchers are in the position of customers in the video, continually asking for models that will analyze the variation in their panel data which actually exists. Applied statisticians are often the cooks, informing customers that regardless of what particular dataset they have, they will only ever be able to get a “cheeseburger.” As a result, there is remarkable uniformity of application of panel data models in the social sciences, even if these models don’t always fit the data very well.\nWhat we put forward in our piece linked above is that any statistical model should have, as its first requirement, that it match the researcher’s question. Problems of omitted variables are important, but necessarily secondary. It does not matter how good the cheeseburger is if the researcher really wants eggs easy over.\nIn addition, fixed effects models do not control for omitted variables. What fixed effect models do is isolate one dimension of variance in the model. As a result, any variables that don’t vary on that dimension are by definition removed from the model. This side-effect is trumpeted as the great inferential benefit of fixed effect models, but it has nothing to do with inference. Fixed effects (or their cousin, random effects/hierarchical models) are simply about selecting which part of the panel dataset is most germane to the analysis.\nThe rule of thumb that we put forward in our paper is that fixed effects/dummy variables/intercepts on cases correspond to the following research question:\n How much does a case change in relation to itself over time?\n While fixed effects/dummy variables/intercepts on time points correspond to the following research question:\n How much does a case change relative to other cases?\n Some questions are fruitfully asked when comparing cases to themselves over time. For example, if a case is a human being, we might want to know whether obtaining more education leads to higher earnings. Conversely, if a case is a country, we might want to know if wealthier countries tend to be more democratic than poorer countries. Some questions primarily employ within-case variation, while others look at cross-sectional variation. Both are present in any panel dataset, and both are potentially interesting.\nIf fixed effects enable comparisons, then what happens if we have dummy variables/intercepts for every case and time point (the so-called two-way fixed effects model)? What comparison are we then making? As it turns out, the answer to this question is not very clear at all. I refer you to the paper above for a full exposition, but in essence, because cases and time points are nested, we end up making comparisons across both dimensions simultaneously, and this is just as obtuse as it sounds. There is no clear research question that matches this model.\nFurthermore, if the original aim was to remove omitted variables, these omitted variables inevitably end up in the estimate again because a two-way estimate necessarily relates to both dimensions of variance simultaneously. As a result, it is not very clear what the point is. The one known use of the model is for difference-in-difference estimation, but only with two time points (see our paper for more detail).\n Exposition with Data This all may sound to you like a nice story. But how can you really know we are telling the truth? There are plenty of equations in our paper, but there is nothing like being able to see the data. One of the contributions of our paper is to create a simulation for panel data where we can control the within-case and cross-sectional variation separately for the same panel data set. This allows us to compare all of the possible panel data models with the same dataset while including or excluding omitted variables and other issues.\nTo show you how this works, I will generate a dataset with a single covariate in which the effect of that covariate is +1 for within-case comparisons and -3 for cross-sectional comparisons. There is noise in the data, but the effect is the same for all cases and for all cross-sections. The following code simulates fifty cases and fifty time points using our panelsim package (currently available only via Github):\n# to install this package, use # remotes::install_github(\u0026quot;saudiwin/panelsim\u0026quot;) require(panelsim) # generate dataset with fixed coefficients within cases and cross-section # no variation in effects across cases or time gen_data \u0026lt;- tw_data(N=50, T=50, case.eff.mean = 1, cross.eff.mean = -3, cross.eff.sd = 0, case.eff.sd = 0, noise.sd=.25) Because there is only one covariate, we can pretty easily visualize the relationships in the cross sectional and within-case variation. First, the value of the outcome/response \\(y\\) is shown for five cases, where one dot represents the value of \\(x\\) for each time point for that case:\ngen_data$data %\u0026gt;% filter(case\u0026lt;5) %\u0026gt;% ggplot(aes(x=x,y=y)) + geom_point() + stat_smooth(method=\u0026quot;lm\u0026quot;) + facet_wrap(~case,scales=\u0026quot;free\u0026quot;) + theme_minimal() As can be seen, there is a consistent positive relationship of approximately +1 between \\(x\\) and \\(y\\) within cases. We can also examine the relationship for the cross-section by subsetting the data to each time point and plotting the cases for five of the time points:\ngen_data$data %\u0026gt;% filter(time\u0026lt;5) %\u0026gt;% ggplot(aes(x=x,y=y)) + geom_point() + stat_smooth(method=\u0026quot;lm\u0026quot;) + facet_wrap(~time,scales=\u0026quot;free\u0026quot;) + theme_minimal() There is a separate and quite distinct relationship in the cross section between \\(x\\) and \\(y\\). Both components are present in the outcome. To find the actual coefficient, we can simply fit linear regression models on the generated data, first with intercepts/dummies for cases:\nsummary(lm(y ~ x + factor(case),data=gen_data$data)) ## ## Call: ## lm(formula = y ~ x + factor(case), data = gen_data$data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8932 -0.1599 -0.0015 0.1682 0.8304 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.773045 0.035151 21.992 \u0026lt; 2e-16 *** ## x 1.002403 0.018501 54.182 \u0026lt; 2e-16 *** ## factor(case)2 -0.184927 0.049529 -3.734 0.000193 *** ## factor(case)3 1.466627 0.050043 29.307 \u0026lt; 2e-16 *** ## factor(case)4 -1.363988 0.049898 -27.335 \u0026lt; 2e-16 *** ## factor(case)5 -0.884801 0.049660 -17.817 \u0026lt; 2e-16 *** ## factor(case)6 1.097485 0.049812 22.032 \u0026lt; 2e-16 *** ## factor(case)7 -0.992114 0.049705 -19.960 \u0026lt; 2e-16 *** ## factor(case)8 -1.597938 0.050057 -31.922 \u0026lt; 2e-16 *** ## factor(case)9 -2.126106 0.050445 -42.147 \u0026lt; 2e-16 *** ## factor(case)10 -0.028781 0.049525 -0.581 0.561205 ## factor(case)11 -1.171168 0.049831 -23.503 \u0026lt; 2e-16 *** ## factor(case)12 -0.132703 0.049526 -2.679 0.007423 ** ## factor(case)13 -0.612141 0.049588 -12.344 \u0026lt; 2e-16 *** ## factor(case)14 0.866259 0.049705 17.428 \u0026lt; 2e-16 *** ## factor(case)15 -0.867871 0.049670 -17.473 \u0026lt; 2e-16 *** ## factor(case)16 0.514860 0.049600 10.380 \u0026lt; 2e-16 *** ## factor(case)17 -0.633858 0.049621 -12.774 \u0026lt; 2e-16 *** ## factor(case)18 -0.696507 0.049617 -14.038 \u0026lt; 2e-16 *** ## factor(case)19 0.806729 0.049680 16.239 \u0026lt; 2e-16 *** ## factor(case)20 -0.764169 0.049643 -15.393 \u0026lt; 2e-16 *** ## factor(case)21 -0.597111 0.049591 -12.041 \u0026lt; 2e-16 *** ## factor(case)22 -1.405450 0.049928 -28.150 \u0026lt; 2e-16 *** ## factor(case)23 0.261895 0.049546 5.286 1.36e-07 *** ## factor(case)24 0.973181 0.049738 19.566 \u0026lt; 2e-16 *** ## factor(case)25 -0.710288 0.049618 -14.315 \u0026lt; 2e-16 *** ## factor(case)26 -0.718412 0.049631 -14.475 \u0026lt; 2e-16 *** ## factor(case)27 -0.946765 0.049698 -19.050 \u0026lt; 2e-16 *** ## factor(case)28 -3.194416 0.051604 -61.902 \u0026lt; 2e-16 *** ## factor(case)29 0.120342 0.049531 2.430 0.015184 * ## factor(case)30 -0.965050 0.049712 -19.413 \u0026lt; 2e-16 *** ## factor(case)31 -1.108207 0.049794 -22.256 \u0026lt; 2e-16 *** ## factor(case)32 -0.888764 0.049668 -17.894 \u0026lt; 2e-16 *** ## factor(case)33 -0.920184 0.049700 -18.515 \u0026lt; 2e-16 *** ## factor(case)34 0.588541 0.049613 11.863 \u0026lt; 2e-16 *** ## factor(case)35 -0.008302 0.049525 -0.168 0.866881 ## factor(case)36 -1.068951 0.049757 -21.484 \u0026lt; 2e-16 *** ## factor(case)37 -2.534691 0.050831 -49.865 \u0026lt; 2e-16 *** ## factor(case)38 -0.699826 0.049616 -14.105 \u0026lt; 2e-16 *** ## factor(case)39 0.389961 0.049578 7.866 5.46e-15 *** ## factor(case)40 -1.291089 0.049878 -25.885 \u0026lt; 2e-16 *** ## factor(case)41 -2.527182 0.050822 -49.726 \u0026lt; 2e-16 *** ## factor(case)42 -1.718180 0.050056 -34.325 \u0026lt; 2e-16 *** ## factor(case)43 0.539154 0.049589 10.872 \u0026lt; 2e-16 *** ## factor(case)44 0.946498 0.049741 19.029 \u0026lt; 2e-16 *** ## factor(case)45 -1.672768 0.050093 -33.393 \u0026lt; 2e-16 *** ## factor(case)46 -1.924235 0.050325 -38.236 \u0026lt; 2e-16 *** ## factor(case)47 0.297708 0.049551 6.008 2.16e-09 *** ## factor(case)48 0.045939 0.049527 0.928 0.353731 ## factor(case)49 1.051133 0.049777 21.117 \u0026lt; 2e-16 *** ## factor(case)50 -1.102510 0.049764 -22.155 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.2476 on 2449 degrees of freedom ## Multiple R-squared: 0.9173, Adjusted R-squared: 0.9156 ## F-statistic: 543.2 on 50 and 2449 DF, p-value: \u0026lt; 2.2e-16 We can see in the regression coefficient above that the coefficient for \\(x\\) is almost exactly +1.\nNext we can fit a model with cases/intercepts for time points (cross-sectional variation):\nsummary(lm(y ~ x + factor(time),data=gen_data$data)) ## ## Call: ## lm(formula = y ~ x + factor(time), data = gen_data$data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9114 -0.1674 -0.0046 0.1699 0.7753 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.68479 0.03530 -19.400 \u0026lt; 2e-16 *** ## x -2.99280 0.01924 -155.534 \u0026lt; 2e-16 *** ## factor(time)2 1.51868 0.05005 30.345 \u0026lt; 2e-16 *** ## factor(time)3 2.26458 0.05073 44.637 \u0026lt; 2e-16 *** ## factor(time)4 -1.04731 0.04975 -21.050 \u0026lt; 2e-16 *** ## factor(time)5 1.73263 0.05025 34.482 \u0026lt; 2e-16 *** ## factor(time)6 1.26293 0.04988 25.319 \u0026lt; 2e-16 *** ## factor(time)7 0.06635 0.04952 1.340 0.18044 ## factor(time)8 1.67967 0.05018 33.472 \u0026lt; 2e-16 *** ## factor(time)9 -0.75507 0.04964 -15.211 \u0026lt; 2e-16 *** ## factor(time)10 -0.55308 0.04957 -11.157 \u0026lt; 2e-16 *** ## factor(time)11 1.34417 0.04996 26.905 \u0026lt; 2e-16 *** ## factor(time)12 2.20623 0.05064 43.565 \u0026lt; 2e-16 *** ## factor(time)13 0.02975 0.04952 0.601 0.54803 ## factor(time)14 0.10429 0.04952 2.106 0.03530 * ## factor(time)15 0.27429 0.04955 5.536 3.43e-08 *** ## factor(time)16 1.05558 0.04981 21.193 \u0026lt; 2e-16 *** ## factor(time)17 -0.40503 0.04954 -8.175 4.68e-16 *** ## factor(time)18 0.86367 0.04971 17.375 \u0026lt; 2e-16 *** ## factor(time)19 1.57862 0.05013 31.492 \u0026lt; 2e-16 *** ## factor(time)20 -0.01728 0.04952 -0.349 0.72717 ## factor(time)21 0.62057 0.04961 12.508 \u0026lt; 2e-16 *** ## factor(time)22 -1.38478 0.04995 -27.721 \u0026lt; 2e-16 *** ## factor(time)23 0.31574 0.04954 6.374 2.20e-10 *** ## factor(time)24 2.96579 0.05157 57.515 \u0026lt; 2e-16 *** ## factor(time)25 0.24178 0.04953 4.881 1.12e-06 *** ## factor(time)26 2.83592 0.05144 55.133 \u0026lt; 2e-16 *** ## factor(time)27 -0.86284 0.04968 -17.369 \u0026lt; 2e-16 *** ## factor(time)28 0.92505 0.04975 18.594 \u0026lt; 2e-16 *** ## factor(time)29 1.05202 0.04980 21.125 \u0026lt; 2e-16 *** ## factor(time)30 0.15122 0.04953 3.053 0.00229 ** ## factor(time)31 1.81587 0.05033 36.082 \u0026lt; 2e-16 *** ## factor(time)32 1.38787 0.04999 27.763 \u0026lt; 2e-16 *** ## factor(time)33 1.87706 0.05038 37.257 \u0026lt; 2e-16 *** ## factor(time)34 1.76956 0.05029 35.184 \u0026lt; 2e-16 *** ## factor(time)35 1.55739 0.05016 31.048 \u0026lt; 2e-16 *** ## factor(time)36 0.57794 0.04960 11.653 \u0026lt; 2e-16 *** ## factor(time)37 0.10647 0.04952 2.150 0.03166 * ## factor(time)38 -0.72418 0.04962 -14.594 \u0026lt; 2e-16 *** ## factor(time)39 1.80861 0.05033 35.938 \u0026lt; 2e-16 *** ## factor(time)40 0.24163 0.04953 4.879 1.14e-06 *** ## factor(time)41 1.27448 0.04994 25.521 \u0026lt; 2e-16 *** ## factor(time)42 1.82843 0.05029 36.361 \u0026lt; 2e-16 *** ## factor(time)43 1.57770 0.05011 31.487 \u0026lt; 2e-16 *** ## factor(time)44 1.50850 0.05006 30.135 \u0026lt; 2e-16 *** ## factor(time)45 0.23318 0.04953 4.708 2.64e-06 *** ## factor(time)46 -0.92491 0.04969 -18.613 \u0026lt; 2e-16 *** ## factor(time)47 1.97346 0.05044 39.125 \u0026lt; 2e-16 *** ## factor(time)48 0.60207 0.04961 12.135 \u0026lt; 2e-16 *** ## factor(time)49 0.75674 0.04964 15.244 \u0026lt; 2e-16 *** ## factor(time)50 -1.38077 0.04996 -27.637 \u0026lt; 2e-16 *** ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.2476 on 2449 degrees of freedom ## Multiple R-squared: 0.9173, Adjusted R-squared: 0.9156 ## F-statistic: 543.4 on 50 and 2449 DF, p-value: \u0026lt; 2.2e-16 Again, the estimated coefficient is almost exactly what we generated. Success!\nHowever, this brings us back to one of the questions we started with. We know the within-case relationship and the cross-sectional relationship, so what happens if we put dummies/intercepts on both cases and time? Well let’s find out:\nsummary(lm(y~x + factor(case) + factor(time),data=gen_data$data)) ## ## Call: ## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.87158 -0.16705 -0.00396 0.16311 0.78026 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.806372 0.087811 9.183 \u0026lt; 2e-16 *** ## x 1.000661 0.143392 6.978 3.84e-12 *** ## factor(case)2 -0.184865 0.049834 -3.710 0.000212 *** ## factor(case)3 1.465951 0.074525 19.671 \u0026lt; 2e-16 *** ## factor(case)4 -1.363415 0.068454 -19.917 \u0026lt; 2e-16 *** ## factor(case)5 -0.884457 0.057091 -15.492 \u0026lt; 2e-16 *** ## factor(case)6 1.096983 0.064585 16.985 \u0026lt; 2e-16 *** ## factor(case)7 -0.991716 0.059427 -16.688 \u0026lt; 2e-16 *** ## factor(case)8 -1.597253 0.075111 -21.265 \u0026lt; 2e-16 *** ## factor(case)9 -2.125203 0.089352 -23.785 \u0026lt; 2e-16 *** ## factor(case)10 -0.028795 0.049592 -0.581 0.561545 ## factor(case)11 -1.170649 0.065430 -17.892 \u0026lt; 2e-16 *** ## factor(case)12 -0.132674 0.049635 -2.673 0.007568 ** ## factor(case)13 -0.611905 0.053240 -11.493 \u0026lt; 2e-16 *** ## factor(case)14 0.865860 0.059440 14.567 \u0026lt; 2e-16 *** ## factor(case)15 -0.867514 0.057639 -15.051 \u0026lt; 2e-16 *** ## factor(case)16 0.514605 0.053863 9.554 \u0026lt; 2e-16 *** ## factor(case)17 -0.633567 0.055045 -11.510 \u0026lt; 2e-16 *** ## factor(case)18 -0.696223 0.054818 -12.701 \u0026lt; 2e-16 *** ## factor(case)19 0.806361 0.058129 13.872 \u0026lt; 2e-16 *** ## factor(case)20 -0.763847 0.056206 -13.590 \u0026lt; 2e-16 *** ## factor(case)21 -0.596870 0.053399 -11.178 \u0026lt; 2e-16 *** ## factor(case)22 -1.404855 0.069741 -20.144 \u0026lt; 2e-16 *** ## factor(case)23 0.261759 0.050838 5.149 2.83e-07 *** ## factor(case)24 0.972749 0.061053 15.933 \u0026lt; 2e-16 *** ## factor(case)25 -0.710003 0.054856 -12.943 \u0026lt; 2e-16 *** ## factor(case)26 -0.718107 0.055565 -12.924 \u0026lt; 2e-16 *** ## factor(case)27 -0.946375 0.059073 -16.020 \u0026lt; 2e-16 *** ## factor(case)28 -3.193052 0.122837 -25.994 \u0026lt; 2e-16 *** ## factor(case)29 0.120274 0.049902 2.410 0.016019 * ## factor(case)30 -0.964644 0.059789 -16.134 \u0026lt; 2e-16 *** ## factor(case)31 -1.107721 0.063746 -17.377 \u0026lt; 2e-16 *** ## factor(case)32 -0.888409 0.057538 -15.440 \u0026lt; 2e-16 *** ## factor(case)33 -0.919792 0.059160 -15.548 \u0026lt; 2e-16 *** ## factor(case)34 0.588263 0.054612 10.772 \u0026lt; 2e-16 *** ## factor(case)35 -0.008290 0.049589 -0.167 0.867240 ## factor(case)36 -1.068500 0.061960 -17.245 \u0026lt; 2e-16 *** ## factor(case)37 -2.533613 0.101653 -24.924 \u0026lt; 2e-16 *** ## factor(case)38 -0.699544 0.054758 -12.775 \u0026lt; 2e-16 *** ## factor(case)39 0.389746 0.052653 7.402 1.84e-13 *** ## factor(case)40 -1.290532 0.067565 -19.101 \u0026lt; 2e-16 *** ## factor(case)41 -2.526108 0.101383 -24.916 \u0026lt; 2e-16 *** ## factor(case)42 -1.717495 0.075071 -22.878 \u0026lt; 2e-16 *** ## factor(case)43 0.538917 0.053290 10.113 \u0026lt; 2e-16 *** ## factor(case)44 0.946063 0.061202 15.458 \u0026lt; 2e-16 *** ## factor(case)45 -1.672061 0.076523 -21.850 \u0026lt; 2e-16 *** ## factor(case)46 -1.923394 0.085197 -22.576 \u0026lt; 2e-16 *** ## factor(case)47 0.297558 0.051103 5.823 6.56e-09 *** ## factor(case)48 0.045901 0.049675 0.924 0.355567 ## factor(case)49 1.050663 0.062931 16.696 \u0026lt; 2e-16 *** ## factor(case)50 -1.102052 0.062325 -17.682 \u0026lt; 2e-16 *** ## factor(time)2 0.009865 0.089885 0.110 0.912618 ## factor(time)3 -0.027398 0.115374 -0.237 0.812314 ## factor(time)4 -0.039227 0.044426 -0.883 0.377342 ## factor(time)5 -0.038629 0.098266 -0.393 0.694277 ## factor(time)6 0.015206 0.081771 0.186 0.852496 ## factor(time)7 -0.052421 0.051843 -1.011 0.312051 ## factor(time)8 -0.009808 0.095634 -0.103 0.918319 ## factor(time)9 -0.029774 0.042955 -0.693 0.488287 ## factor(time)10 -0.073201 0.043597 -1.679 0.093275 . ## factor(time)11 -0.034386 0.085805 -0.401 0.688639 ## factor(time)12 0.003032 0.112421 0.027 0.978486 ## factor(time)13 -0.092023 0.051904 -1.773 0.076362 . ## factor(time)14 0.017896 0.051201 0.350 0.726720 ## factor(time)15 -0.099884 0.057487 -1.738 0.082424 . ## factor(time)16 -0.059046 0.077744 -0.759 0.447632 ## factor(time)17 -0.066464 0.044756 -1.485 0.137669 ## factor(time)18 -0.039199 0.071529 -0.548 0.583733 ## factor(time)19 -0.040240 0.093375 -0.431 0.666546 ## factor(time)20 -0.019978 0.049628 -0.403 0.687311 ## factor(time)21 -0.021785 0.064293 -0.339 0.734760 ## factor(time)22 -0.016429 0.049358 -0.333 0.739273 ## factor(time)23 0.016278 0.055739 0.292 0.770279 ## factor(time)24 -0.021054 0.138843 -0.152 0.879486 ## factor(time)25 -0.016068 0.054798 -0.293 0.769382 ## factor(time)26 -0.054303 0.135548 -0.401 0.688739 ## factor(time)27 -0.034151 0.043223 -0.790 0.429545 ## factor(time)28 -0.075377 0.074360 -1.014 0.310839 ## factor(time)29 -0.046340 0.077258 -0.600 0.548686 ## factor(time)30 -0.062577 0.053830 -1.162 0.245150 ## factor(time)31 -0.049767 0.101325 -0.491 0.623362 ## factor(time)32 -0.037785 0.087273 -0.433 0.665091 ## factor(time)33 -0.052008 0.103393 -0.503 0.614999 ## factor(time)34 -0.059567 0.100140 -0.595 0.552009 ## factor(time)35 -0.104873 0.094761 -1.107 0.268534 ## factor(time)36 -0.010170 0.062857 -0.162 0.871484 ## factor(time)37 -0.082164 0.053289 -1.542 0.123243 ## factor(time)38 -0.057402 0.042945 -1.337 0.181464 ## factor(time)39 -0.056943 0.101323 -0.562 0.574169 ## factor(time)40 0.007376 0.054276 0.136 0.891908 ## factor(time)41 -0.070988 0.084778 -0.837 0.402484 ## factor(time)42 0.010645 0.099772 0.107 0.915043 ## factor(time)43 -0.013565 0.092496 -0.147 0.883418 ## factor(time)44 -0.015229 0.090355 -0.169 0.866166 ## factor(time)45 0.010920 0.054013 0.202 0.839797 ## factor(time)46 -0.060932 0.043387 -1.404 0.160328 ## factor(time)47 -0.020632 0.105521 -0.196 0.844996 ## factor(time)48 -0.038896 0.064256 -0.605 0.545015 ## factor(time)49 0.028808 0.066612 0.432 0.665437 ## factor(time)50 NA NA NA NA ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.2479 on 2401 degrees of freedom ## Multiple R-squared: 0.9187, Adjusted R-squared: 0.9154 ## F-statistic: 277 on 98 and 2401 DF, p-value: \u0026lt; 2.2e-16 You might expect that perhaps the two-way coefficient would be somewhere between the cross-sectional and within-case coefficients. Nope. It’s equal in this simulation to roughly 1, but that depends on how much variance there is in the cross-section and the within-case components of the dataset as they are now being combined in a non-linear fashion (if you can’t wrap your mind around this, don’t worry, it’s nearly impossible to do and unfruitful if you do accomplish it). However, the coefficient is still equal to a value that appears reasonable, although it is much less precise, and we might be willing to traipse along with it. However, you should pay attention to what happened in the model summary above–one of the time dummies (factor(time)50) came back with a missing (NA) coefficient! What is that about?\nI’m so glad you asked. As we mentioned earlier, R has the ability to check for multi-collinearity in the predictor variables to avoid getting errors when attempting to estimate linear regression. Multi-collinearity can arise for reasons that have nothing to do with fixed effects, such as accidentally including variables that are sums of each other, etc. It often happens in fixed effects regression models when people try to include variables that only vary in the cross-section when using case fixed effects (a symptom of the Cheeseburger Syndrome mentioned previously). In this instance, though, there is no reason to think that there is multi-collinearity as we generated the data with two continuous variables (\\(x\\) and \\(y\\)). The data are perfect with no missingness. So how could this happen?\nThis discovery is one of the central contributions of our paper. Whenever panel data has a single effect for the cross-section and for within-case variation–as is the dominant way of thinking about panel data–the two-way fixed effects coefficient is statistically unidentified. That is, trying to estimate it is equivalent to saying 2 + 2 = 5 or the earth is as round as a square. As we show in the paper, it algebraically reduces to dividing by zero. R magically saved the day by dropping a variable, but we can show what would happen if R had not intervened.\nIn the code below I manually estimate a regression using the matrix inversion formula, \\((X^TX)^{-1}X^Ty\\), where \\(X\\) in this case is all of our predictor variables, including the case/time dummies:\nX \u0026lt;- model.matrix(y~x + factor(case) + factor(time),data=gen_data$data) y \u0026lt;- gen_data$data$y try(solve(t(X)%*%X)%*%t(X)%*%y) ## Error in solve.default(t(X) %*% X) : ## system is computationally singular: reciprocal condition number = 5.63975e-19 You can see the error message: the system (matrix computation of OLS regression) is computationally singular. You’ll need to see the paper for how this all breaks down, but essentially the question you are putting to R is nonsensical. You simply cannot estimate a single joint effect while including all the variables. Instead, you need to drop one, which essentially means the effect is relative to the whichever intercept happened to be dropped (in this case time point 50). The coefficient could change if we simply re-arrange the labeling of the time fixed effects, as in the following:\ngen_data$data$time \u0026lt;- as.numeric(factor(gen_data$data$time,levels=sample(unique(gen_data$data$time)))) summary(lm(y~x + factor(case) + factor(time),data=gen_data$data)) ## ## Call: ## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.87158 -0.16705 -0.00396 0.16311 0.78026 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) 0.827062 0.058816 14.062 \u0026lt; 2e-16 *** ## x 0.892381 0.169360 5.269 1.49e-07 *** ## factor(case)2 -0.181065 0.049934 -3.626 0.000294 *** ## factor(case)3 1.423934 0.082322 17.297 \u0026lt; 2e-16 *** ## factor(case)4 -1.327772 0.074606 -17.797 \u0026lt; 2e-16 *** ## factor(case)5 -0.863081 0.059798 -14.433 \u0026lt; 2e-16 *** ## factor(case)6 1.065727 0.069627 15.306 \u0026lt; 2e-16 *** ## factor(case)7 -0.966975 0.062893 -15.375 \u0026lt; 2e-16 *** ## factor(case)8 -1.554645 0.083062 -18.717 \u0026lt; 2e-16 *** ## factor(case)9 -2.069070 0.100829 -20.521 \u0026lt; 2e-16 *** ## factor(case)10 -0.029652 0.049597 -0.598 0.549997 ## factor(case)11 -1.138408 0.070718 -16.098 \u0026lt; 2e-16 *** ## factor(case)12 -0.130897 0.049657 -2.636 0.008442 ** ## factor(case)13 -0.597254 0.054619 -10.935 \u0026lt; 2e-16 *** ## factor(case)14 0.841102 0.062910 13.370 \u0026lt; 2e-16 *** ## factor(case)15 -0.845316 0.060527 -13.966 \u0026lt; 2e-16 *** ## factor(case)16 0.498709 0.055464 8.992 \u0026lt; 2e-16 *** ## factor(case)17 -0.615509 0.057060 -10.787 \u0026lt; 2e-16 *** ## factor(case)18 -0.678563 0.056754 -11.956 \u0026lt; 2e-16 *** ## factor(case)19 0.783446 0.061177 12.806 \u0026lt; 2e-16 *** ## factor(case)20 -0.743853 0.058618 -12.690 \u0026lt; 2e-16 *** ## factor(case)21 -0.581894 0.054834 -10.612 \u0026lt; 2e-16 *** ## factor(case)22 -1.367817 0.076249 -17.939 \u0026lt; 2e-16 *** ## factor(case)23 0.253267 0.051327 4.934 8.59e-07 *** ## factor(case)24 0.945844 0.065030 14.545 \u0026lt; 2e-16 *** ## factor(case)25 -0.692276 0.056805 -12.187 \u0026lt; 2e-16 *** ## factor(case)26 -0.699163 0.057758 -12.105 \u0026lt; 2e-16 *** ## factor(case)27 -0.922121 0.062427 -14.771 \u0026lt; 2e-16 *** ## factor(case)28 -3.108184 0.141697 -21.936 \u0026lt; 2e-16 *** ## factor(case)29 0.115992 0.050029 2.318 0.020507 * ## factor(case)30 -0.939410 0.063371 -14.824 \u0026lt; 2e-16 *** ## factor(case)31 -1.077463 0.068540 -15.720 \u0026lt; 2e-16 *** ## factor(case)32 -0.866360 0.060394 -14.345 \u0026lt; 2e-16 *** ## factor(case)33 -0.895419 0.062541 -14.317 \u0026lt; 2e-16 *** ## factor(case)34 0.570972 0.056476 10.110 \u0026lt; 2e-16 *** ## factor(case)35 -0.007546 0.049593 -0.152 0.879081 ## factor(case)36 -1.040438 0.066215 -15.713 \u0026lt; 2e-16 *** ## factor(case)37 -2.466600 0.115948 -21.273 \u0026lt; 2e-16 *** ## factor(case)38 -0.681990 0.056674 -12.034 \u0026lt; 2e-16 *** ## factor(case)39 0.376359 0.053819 6.993 3.47e-12 *** ## factor(case)40 -1.255870 0.073466 -17.095 \u0026lt; 2e-16 *** ## factor(case)41 -2.459329 0.115618 -21.271 \u0026lt; 2e-16 *** ## factor(case)42 -1.674928 0.083011 -20.177 \u0026lt; 2e-16 *** ## factor(case)43 0.524163 0.054686 9.585 \u0026lt; 2e-16 *** ## factor(case)44 0.918966 0.065225 14.089 \u0026lt; 2e-16 *** ## factor(case)45 -1.628044 0.084840 -19.190 \u0026lt; 2e-16 *** ## factor(case)46 -1.871074 0.095680 -19.556 \u0026lt; 2e-16 *** ## factor(case)47 0.288205 0.051692 5.575 2.75e-08 *** ## factor(case)48 0.043567 0.049713 0.876 0.380918 ## factor(case)49 1.021395 0.067481 15.136 \u0026lt; 2e-16 *** ## factor(case)50 -1.073533 0.066692 -16.097 \u0026lt; 2e-16 *** ## factor(time)2 -0.069009 0.100571 -0.686 0.492671 ## factor(time)3 -0.108184 0.073336 -1.475 0.140295 ## factor(time)4 0.002837 0.043759 0.065 0.948307 ## factor(time)5 -0.011388 0.082981 -0.137 0.890857 ## factor(time)6 -0.079838 0.110453 -0.723 0.469861 ## factor(time)7 -0.066293 0.048598 -1.364 0.172660 ## factor(time)8 -0.135578 0.123915 -1.094 0.274015 ## factor(time)9 -0.149671 0.096133 -1.557 0.119623 ## factor(time)10 -0.135415 0.137057 -0.988 0.323244 ## factor(time)11 -0.046205 0.101477 -0.455 0.648917 ## factor(time)12 -0.094439 0.062068 -1.522 0.128252 ## factor(time)13 -0.056940 0.061062 -0.933 0.351172 ## factor(time)14 -0.074651 0.076729 -0.973 0.330690 ## factor(time)15 -0.064301 0.086107 -0.747 0.455286 ## factor(time)16 -0.148654 0.105819 -1.405 0.160210 ## factor(time)17 -0.109373 0.139416 -0.785 0.432820 ## factor(time)18 -0.023932 0.052512 -0.456 0.648610 ## factor(time)19 -0.054156 0.088109 -0.615 0.538842 ## factor(time)20 -0.035535 0.098979 -0.359 0.719611 ## factor(time)21 -0.116553 0.143594 -0.812 0.417053 ## factor(time)22 -0.010896 0.065124 -0.167 0.867139 ## factor(time)23 -0.030352 0.055015 -0.552 0.581207 ## factor(time)24 -0.097372 0.166084 -0.586 0.557742 ## factor(time)25 -0.033847 0.056850 -0.595 0.551644 ## factor(time)26 -0.144292 0.145023 -0.995 0.319857 ## factor(time)27 -0.076492 0.070012 -1.093 0.274696 ## factor(time)28 -0.081450 0.086159 -0.945 0.344576 ## factor(time)29 -0.059062 0.059659 -0.990 0.322275 ## factor(time)30 -0.069904 0.049340 -1.417 0.156679 ## factor(time)31 -0.116713 0.102264 -1.141 0.253863 ## factor(time)32 -0.050535 0.050595 -0.999 0.317977 ## factor(time)33 -0.025185 0.043191 -0.583 0.559876 ## factor(time)34 -0.026497 0.046296 -0.572 0.567147 ## factor(time)35 -0.059114 0.048596 -1.216 0.223940 ## factor(time)36 -0.109133 0.105935 -1.030 0.303025 ## factor(time)37 -0.119734 0.053185 -2.251 0.024457 * ## factor(time)38 -0.042987 0.101938 -0.422 0.673288 ## factor(time)39 -0.035869 0.047662 -0.753 0.451788 ## factor(time)40 -0.113464 0.165575 -0.685 0.493239 ## factor(time)41 -0.059933 0.110559 -0.542 0.587805 ## factor(time)42 -0.009158 0.057266 -0.160 0.872963 ## factor(time)43 -0.126494 0.150871 -0.838 0.401878 ## factor(time)44 -0.059636 0.047396 -1.258 0.208424 ## factor(time)45 -0.039694 0.107192 -0.370 0.711183 ## factor(time)46 -0.056278 0.054291 -1.037 0.300026 ## factor(time)47 -0.146145 0.129554 -1.128 0.259404 ## factor(time)48 -0.088757 0.069468 -1.278 0.201491 ## factor(time)49 -0.136982 0.103233 -1.327 0.184661 ## factor(time)50 NA NA NA NA ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 0.2479 on 2401 degrees of freedom ## Multiple R-squared: 0.9187, Adjusted R-squared: 0.9154 ## F-statistic: 277 on 98 and 2401 DF, p-value: \u0026lt; 2.2e-16 As you can see, the coefficient changed because we arbitrarily swapped the order of the time fixed effects, and as R will drop the last one in the case of multi-collinearity, the estimate changed.\nAt this point, you should be concerned. One of our frustrations with this piece is that although the working paper has been in circulation for years, no one seems to have cared about this apparently quite important problem.\nYou might wonder though that you’ve seen two-way fixed effects models where this didn’t happen. As we show in the paper, the two-way FE model is identified if we generate the data differently. What we have to do is use a different effect of \\(x\\) on \\(y\\) for each time point/case, or what can be called a varying slopes model (slope for regression coefficient). We are not varying the fixed effects/intercepts themselves, rather we are varying the relationship between \\(x\\) and \\(y\\) across time points and cases. We must do this to prevent the two-way FE model from being unidentified.\nTo demonstrate this, we will generate new data except the coefficients will vary across case and time points randomly:\n# make the slopes vary with the .sd parameters gen_data \u0026lt;- tw_data(N=20, T=50, case.eff.mean = 1, cross.eff.mean = -1, cross.eff.sd =.25, case.eff.sd =1, noise.sd=1) summary(lm(y~x + factor(case) + factor(time),data=gen_data$data)) ## ## Call: ## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5458 -0.7073 0.0026 0.6446 3.1709 ## ## Coefficients: ## Estimate Std. Error t value Pr(\u0026gt;|t|) ## (Intercept) -0.673576 0.264996 -2.542 0.011188 * ## x -0.812224 0.005523 -147.067 \u0026lt; 2e-16 *** ## factor(case)2 0.431969 0.201817 2.140 0.032582 * ## factor(case)3 0.169660 0.201920 0.840 0.400994 ## factor(case)4 0.216513 0.201806 1.073 0.283604 ## factor(case)5 0.377437 0.201835 1.870 0.061794 . ## factor(case)6 0.233856 0.201762 1.159 0.246726 ## factor(case)7 0.291743 0.202146 1.443 0.149292 ## factor(case)8 0.040054 0.201779 0.199 0.842696 ## factor(case)9 0.182815 0.201763 0.906 0.365123 ## factor(case)10 0.197116 0.201782 0.977 0.328886 ## factor(case)11 0.264956 0.201782 1.313 0.189480 ## factor(case)12 0.075685 0.201799 0.375 0.707708 ## factor(case)13 0.249261 0.201795 1.235 0.217062 ## factor(case)14 0.118726 0.201765 0.588 0.556382 ## factor(case)15 0.494571 0.201828 2.450 0.014451 * ## factor(case)16 -0.085249 0.203634 -0.419 0.675579 ## factor(case)17 0.383878 0.201766 1.903 0.057403 . ## factor(case)18 0.290350 0.201837 1.439 0.150619 ## factor(case)19 0.483656 0.201790 2.397 0.016734 * ## factor(case)20 0.287368 0.201792 1.424 0.154759 ## factor(time)2 0.596679 0.319058 1.870 0.061780 . ## factor(time)3 1.407443 0.319158 4.410 1.16e-05 *** ## factor(time)4 -0.328379 0.320361 -1.025 0.305616 ## factor(time)5 -0.119690 0.319047 -0.375 0.707634 ## factor(time)6 1.539948 0.319015 4.827 1.62e-06 *** ## factor(time)7 0.338443 0.319042 1.061 0.289052 ## factor(time)8 0.528072 0.319048 1.655 0.098232 . ## factor(time)9 -0.032228 0.319030 -0.101 0.919556 ## factor(time)10 0.765270 0.319056 2.399 0.016657 * ## factor(time)11 0.789097 0.319058 2.473 0.013568 * ## factor(time)12 2.149822 0.319199 6.735 2.87e-11 *** ## factor(time)13 0.118301 0.319032 0.371 0.710862 ## factor(time)14 -0.734958 0.319015 -2.304 0.021452 * ## factor(time)15 0.888259 0.319109 2.784 0.005486 ** ## factor(time)16 0.553245 0.319027 1.734 0.083221 . ## factor(time)17 0.652004 0.319030 2.044 0.041264 * ## factor(time)18 1.351653 0.319104 4.236 2.50e-05 *** ## factor(time)19 0.260828 0.319061 0.817 0.413860 ## factor(time)20 -0.662932 0.319060 -2.078 0.038005 * ## factor(time)21 1.509116 0.319095 4.729 2.60e-06 *** ## factor(time)22 -0.014879 0.319029 -0.047 0.962810 ## factor(time)23 -0.538377 0.319023 -1.688 0.091827 . ## factor(time)24 0.493220 0.319046 1.546 0.122464 ## factor(time)25 -1.820174 0.319100 -5.704 1.57e-08 *** ## factor(time)26 -0.571426 0.319015 -1.791 0.073583 . ## factor(time)27 0.093017 0.319026 0.292 0.770683 ## factor(time)28 -1.328269 0.319035 -4.163 3.43e-05 *** ## factor(time)29 0.342019 0.319038 1.072 0.283983 ## factor(time)30 -0.774338 0.319014 -2.427 0.015401 * ## factor(time)31 0.461852 0.319026 1.448 0.148040 ## factor(time)32 0.308478 0.319046 0.967 0.333858 ## factor(time)33 1.343319 0.319094 4.210 2.80e-05 *** ## factor(time)34 1.989206 0.319258 6.231 7.03e-10 *** ## factor(time)35 -0.422407 0.319019 -1.324 0.185801 ## factor(time)36 0.614837 0.319085 1.927 0.054299 . ## factor(time)37 -0.416240 0.319061 -1.305 0.192359 ## factor(time)38 0.779448 0.319089 2.443 0.014762 * ## factor(time)39 -0.847622 0.319015 -2.657 0.008019 ** ## factor(time)40 3.245566 0.319204 10.168 \u0026lt; 2e-16 *** ## factor(time)41 -0.589393 0.319031 -1.847 0.064999 . ## factor(time)42 0.509922 0.319178 1.598 0.110469 ## factor(time)43 -0.815928 0.319029 -2.558 0.010699 * ## factor(time)44 1.200743 0.320664 3.745 0.000192 *** ## factor(time)45 -0.677540 0.319015 -2.124 0.033946 * ## factor(time)46 0.329598 0.319049 1.033 0.301842 ## factor(time)47 0.254068 0.319037 0.796 0.426027 ## factor(time)48 0.728453 0.319070 2.283 0.022652 * ## factor(time)49 2.376336 0.319302 7.442 2.25e-13 *** ## factor(time)50 0.525069 0.319064 1.646 0.100172 ## --- ## Signif. codes: 0 \u0026#39;***\u0026#39; 0.001 \u0026#39;**\u0026#39; 0.01 \u0026#39;*\u0026#39; 0.05 \u0026#39;.\u0026#39; 0.1 \u0026#39; \u0026#39; 1 ## ## Residual standard error: 1.009 on 930 degrees of freedom ## Multiple R-squared: 0.9625, Adjusted R-squared: 0.9598 ## F-statistic: 346.3 on 69 and 930 DF, p-value: \u0026lt; 2.2e-16 Bada bing bada boom. Now there are no missing coefficients. However, the coefficient on \\(x\\) isn’t any easier to interpret, and in fact it is in even less transparent as it involves taking averages of averages of coefficients in the cross-section and within-case variation (say that five times fast). It is also peculiar that the model can only be fit with this kind of variation in the slopes, as opposed to the intercepts, as might appear more logical. The models with fixed effects only for cases or time points do not have this problem at all.\nWhat makes this lack of identification particularly malicious is that it is virtually impossible to identify unless someone takes the trouble as we did to generate data from scratch. Visually, the dataset with single coefficients for cross-sectional/within-case variation appears pristine. It is only deep under the hood that the problem appears. The fact that R (and Stata has similar behavior) can magically make the problem go away by changing the data only makes it less likely that the problem will ever be identified. We simply do not know how many papers in the literature have been affected by this problem (or by similar situations where the regression slopes are almost fixed across cases or time points).\nThere is one important caveat to this post. The two-way fixed effects model can be a difference-in -differences model, but only if there are exactly two (2) time points. It is not possible to run a two-way FE model with many time points and call it difference-in-difference as there are the same difficulties in interpretation. We discuss this more in the paper.\n Summary In summary, fixed effects models are very useful for panel data as they help isolate dimensions of variation that matter for research questions. They are not magical tools for causal inference, but rather frameworks for understanding data. It is important to think about which dimension (cross-section or within-case) is more relevant, and then go with that dimension. All other concerns, including modeling spatial or time autocorrelation, omitted variables and endogeneity are all secondary to this first and most important point, which is what comparisons can be drawn from the data?\nOne important area that this can be applied to is allowing people to fit more models with fixed effects on time points rather than cases. There are many questions that can be best understood by comparing cases to each other rather than cases to themselves over time. Studying long-run institutions like electoral systems, for example, can only be understood in terms of cross-sectional variation. There is nothing unique or special about the within-case model.\nSome claim that within-case variation is better because cases have less heterogeneity than the cross-section. However, there is no way this can be true a priori. If minimizing heterogeneity is the aim, then it is important to consider the time frame and how units change over time. For example, if we have a very long time series, say 200 years, and we are comparing the U.S. and Canada, then we might believe that the comparison of the U.S. of 1800 to the Canada of 1800 (the cross-section) has less noise than a comparison of the U.S. of 1800 to the U.S. of 2020 (within-case variation). We need to think more about our data and what it represents rather than taking the short-cut of employing the same model everyone else uses.\nWhile we did not discuss random effects/hierarchical models in this post, the same principles apply even if “partial pooling” is used rather than “no pooling.” Intercepts are designed to draw comparisons, and however the intercepts are modeled, it is important to think about what they are saying about the data, and whether that statement makes any sense.\nSo if you don’t want to eat that cheeseburger… we release you. Go enjoy your tofu-based meat alternative with relish.\n ","date":1587567600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587567600,"objectID":"07209bad5748f1054cb311b0ad5f5670","permalink":"http://www.robertkubinec.com/post/fixed_effects/","publishdate":"2020-04-22T15:00:00Z","relpermalink":"/post/fixed_effects/","section":"post","summary":"We’ve all been in that seminar where the author puts up a slide containing regression coefficients, and buried in the bottom line of the table we can see little Ys and Ns indicating the kind of panel data model employed.","tags":null,"title":"What Panel Data Is Really All About","type":"post"},{"authors":null,"categories":null,"content":"   For an up to date version of this model, please see our paper at https://osf.io/preprints/socarxiv/jp4wk/.\n","date":1585407600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585407600,"objectID":"e7c2b77371b47150ee4d08085682310b","permalink":"http://www.robertkubinec.com/post/kubinec_model_draft/","publishdate":"2020-03-28T15:00:00Z","relpermalink":"/post/kubinec_model_draft/","section":"post","summary":"For an up to date version of this model, please see our paper at https://osf.io/preprints/socarxiv/jp4wk/.","tags":null,"title":"A Proposed Model for Partial Identification of SARS-CoV2 Infection Rates Given Observed Tests and Cases","type":"post"},{"authors":null,"categories":null,"content":"    Background Conjoint survey experiments have become more popular in political science since the publication of Hainmueller, Hopkins and Yamamoto (2014). However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see SocArchiv Draft). I employ both traditional power measures and newer statistics from Gelman and Carlin (2014) reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).\nThe original Rmarkdown and saved simulation files can be downloaded from the site’s Github.\nThe packages required to run this simulation are listed in the code block below:\n#Required packages require(ggplot2) require(dplyr) require(tidyr) require(multiwayvcov) require(lmtest) require(stringr) require(kableExtra) # package MASS also used but not loaded # != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows. # to install parallelsugar: # install.packages(\u0026#39;devtools\u0026#39;) # library(devtools) # install_github(\u0026#39;nathanvan/parallelsugar\u0026#39;) # If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used if(.Platform$OS.type==\u0026#39;windows\u0026#39;) { parallelfunc \u0026lt;- parallelsugar::mclapply_socket } else { parallelfunc \u0026lt;- parallel::mclapply }  Simulation Set-up The following parameters control the range of coefficients tested and the number of simulations. The survey experiment design employs vignettes in which appeals and the actors making appeals are allowed to vary between respondents. Any one vignette has one actor and one appeal. The probability of assignment is assumed to be a simple random fraction of the number of appeal-actor combinations (14). If run_sim is set to TRUE, the simulation is run, otherwise the simulation results are loaded from an RDS file and plotted. Running the simulation will take approximately 6 to 12 hours depending on the number of cores and speed of the CPU.\n#Actually run the simulation or just load the data and look at it? run_sim \u0026lt;- FALSE # Max number of respondents fixed at 2700 num_resp \u0026lt;- 2700 # Number of iterations (breaks in sample size) num_breaks \u0026lt;- 300 # Number of simulations to run per iteration n_sims \u0026lt;- 1000 I then create a grid of all possible actor-appeal combinations as I am using simple randomization of profiles before presenting them to respondents. There are two vectors of treatments (actors and appeals) that each have 7 separate treatments for a total of 14 separate possible treatments.\n# Two treatment variables producing a cross-product of 7x7 treatments1 \u0026lt;- c(\u0026#39;military\u0026#39;,\u0026#39;MOI\u0026#39;,\u0026#39;president\u0026#39;,\u0026#39;MOJ\u0026#39;,\u0026#39;parliament\u0026#39;,\u0026#39;municipality\u0026#39;,\u0026#39;government\u0026#39;) treatments2 \u0026lt;- c(\u0026#39;exprop.firm\u0026#39;,\u0026#39;exprop.income\u0026#39;,\u0026#39;permit.reg\u0026#39;,\u0026#39;contracts.supply\u0026#39;,\u0026#39;permit.export\u0026#39;,\u0026#39;permit.import\u0026#39;,\u0026#39;reforms\u0026#39;) total_treat \u0026lt;- length(c(treatments1,treatments2)) grid_pair \u0026lt;- as.matrix(expand.grid(treatments1,treatments2)) print(head(grid_pair)) ## Var1 Var2 ## [1,] \u0026quot;military\u0026quot; \u0026quot;exprop.firm\u0026quot; ## [2,] \u0026quot;MOI\u0026quot; \u0026quot;exprop.firm\u0026quot; ## [3,] \u0026quot;president\u0026quot; \u0026quot;exprop.firm\u0026quot; ## [4,] \u0026quot;MOJ\u0026quot; \u0026quot;exprop.firm\u0026quot; ## [5,] \u0026quot;parliament\u0026quot; \u0026quot;exprop.firm\u0026quot; ## [6,] \u0026quot;municipality\u0026quot; \u0026quot;exprop.firm\u0026quot;  Simulation To simulate the data, I first sample 14 coefficients \\(\\beta_j\\) (one for each treatment \\(J\\)) from a normal distribution with mean zero and standard deviation one. I then randomly sample from two profile combinations for each of the \\(I\\) respondents in accordance with simple random sampling. Two profile combinations, for a total of four tasks \\(T\\), are selected to reflect the fact that paired vignettes will be shown to each respondent as in the study design. I also sample a pre-treatment covariate \\(Z_I\\) that is a random binomial vector with probability of 0.2 (thus 20% of respondents will fall into this cell). A treatment interaction effect \\(\\beta_z\\) is sampled from a normal distribution with mean 0.5 and standard deviation of 0.3 to provide a sampling distribution for the true effect, instead of assuming that the true effect is a fixed population value. Adding a distribution for \\(\\beta_j\\) reflects additional uncertainty beyond standard sampling distribution uncertainty. In this case, it represents additional measurement error between the true concept and the indicators used in the survey design.\nI also post-stratify some estimates with a pre-treatment covariate \\(Q_I\\) from a binomial distribution of probability .5 that has a constant effect on \\(Y_{it}\\) of \\(+1\\) (representing a fixed effect).\nI then randomly sample a pair of outcomes, for a total of four tasks \\(T\\), for \\(Y_{it}\\) in the range of \\([1,10]\\) by drawing a number from a multivariate normal distribution. The mean \\(\\mu_{it}\\) of this normal distribution is equal to a linear model with an intercept of 5, the 14 dummy variables for treatment indicators \\(X_j\\) with associated coefficients \\(\\beta_j\\), the interaction \\(\\beta_z\\) between the pre-treatment covariate \\(Z_i\\) and \\(X_{ij}\\), and a post-stratification covariate \\(Q_i\\). To simplify matters, \\(Z_i\\) is not given its own constituent term as I am not interested in the unconditional effect of \\(Z_i\\) on \\(Y_{it}\\), only the effect of \\(X_{ijt}\\) on \\(Y_{it}\\) given \\(Z_{i}\\). Finally, I draw correlated errors from a multivariate normal distribution with mean of zero and length of 4 (equal to the number of tasks per respondent) to produce a \\(4 \\times 4\\) variance matrix \\(\\varSigma_i\\) with a diagonal of 4 and intra-respondent covariation of 1 (correlation of 0.5).\n\\[ \\begin{aligned} X_{ITJ} \u0026amp;\\sim \\mathrm{B} \\Big( \\frac{1}{J \\times 2} \\Big)\\\\ B_{J} \u0026amp;\\sim \\mathrm{N}(0,2)\\\\ \\beta_z \u0026amp;\\sim \\mathrm{N}(0.5,0.3)\\\\ Z_I \u0026amp;\\sim \\mathrm{B}(0.2)\\\\ Q_I \u0026amp;\\sim \\mathrm{B}(0.5)\\\\ \\mu_{it} \u0026amp;= 5 + \\sum_{j=1}^{J} \\sum_{t=1}^{T} \\beta_j * X_{itj} + \\beta_z * X_{it1} *Z_i + Q_i\\\\ Y_{it} \u0026amp;\\sim \\mathrm{N}(\\mu_{it},\\varSigma_i) \\end{aligned} \\]\nThis process will produce some numbers outside the \\([1,10]\\) range; however, it is better to leave these values in as explicit truncation will violate the assumptions of the underlying causal model.\nI run 1000 simulations for each of 300 sequential sample sizes ranging from 100 to 2700. I then take the mean significant effect and report that as the likely significant effect size for that sample size. I also record the ratio of draws for which the effect is significant (the power). However, given that the true effect is not fixed, I interpret power as the ability detect a true effect greater than zero. I record both unadjusted p-values and p-values adjusted using the cluster.vcov function from the multiwayvcov package by clustering around respondent ID to reflect the pairing of vignettes. I also use separate results when post-stratifying on a pre-treatment covariate \\(Q_I\\).\nIn addition, I included M-errors (error of absolute magnitude of significant coefficients) and S-errors (incorrect sign of significant coefficients). M-errors provide an estimate of publication bias given that the \\(p=0.05\\) threshold is a hard boundary and will necessarily result in smaller effects being reported as statistically insignificant when in fact they are greater than zero. S-errors help determine the probability that an estimated effect is the correct sign even if it is significant. S-errors are particularly problematic in small samples when sampling error can produce large negative deviations that may be statistically significant.\nif(run_sim==TRUE) { file.create(\u0026#39;output_log.txt\u0026#39;,showWarnings = FALSE) # Need to randomize over the simulations so that parallelization works correctly on windows sampled_seq \u0026lt;- sample(seq(100,num_resp,length.out = num_breaks)) all_sims \u0026lt;- parallelfunc(sampled_seq,function(x) { out_probs \u0026lt;- 1:n_sims cat(paste0(\u0026quot;Now running simulation on data sample size \u0026quot;,x),file=\u0026#39;output_log.txt\u0026#39;,sep=\u0026#39;\\n\u0026#39;,append=TRUE) out_data \u0026lt;- lapply(1:n_sims, function(j) { total_probs \u0026lt;- sapply(1:x,function(x) { treat_rows \u0026lt;- sample(1:nrow(grid_pair),4) treatments_indiv \u0026lt;- c(grid_pair[treat_rows,]) return(treatments_indiv) }) by_resp \u0026lt;- t(total_probs) by_resp \u0026lt;- as_data_frame(by_resp) names(by_resp) \u0026lt;- c(paste0(\u0026#39;actor.\u0026#39;,1:4,\u0026quot;_cluster\u0026quot;,c(1,1,2,2)),paste0(\u0026#39;gift.\u0026#39;,1:4,\u0026quot;_cluster\u0026quot;,c(1,1,2,2))) by_resp$respondent \u0026lt;- paste0(\u0026#39;Respondent_\u0026#39;,1:nrow(by_resp)) by_resp \u0026lt;- gather(by_resp,attribute,indicator,-respondent) %\u0026gt;% separate(attribute,into=c(\u0026#39;attribute\u0026#39;,\u0026#39;cluster\u0026#39;),sep=\u0026#39;_\u0026#39;) %\u0026gt;% separate(attribute,into=c(\u0026#39;attribute\u0026#39;,\u0026#39;task\u0026#39;)) %\u0026gt;% spread(attribute,indicator) # Assign true coefficients for treatments #Beta_js coefs \u0026lt;- data_frame(coef_val=rnorm(n=length(c(treatments1,treatments2)),mean=0,sd=1), treat_label=c(treatments1,treatments2)) # Create cluster covariance in the errors sigma_matrix \u0026lt;- matrix(2,nrow=4,ncol=4) diag(sigma_matrix) \u0026lt;- 4 # Add on the outcome as a normal draw, treatment coefficients, interaction coefficient, group errors/interaction by respondent by_resp \u0026lt;- gather(by_resp,treatment,appeal_type,actor,gift) %\u0026gt;% left_join(coefs,by=c(\u0026#39;appeal_type\u0026#39;=\u0026#39;treat_label\u0026#39;)) # Record interaction coefficient (true estimate of interest) true_effect \u0026lt;- rnorm(n=1,mean=0.5,sd=0.3) by_resp \u0026lt;- select(by_resp,-treatment) %\u0026gt;% spread(appeal_type,coef_val) %\u0026gt;% group_by(respondent) %\u0026gt;% mutate(error=MASS::mvrnorm(1,mu=rep(0,4),Sigma=sigma_matrix)) %\u0026gt;% ungroup # interaction coefficient only in function if military==TRUE by_resp \u0026lt;- mutate(by_resp,int_coef=true_effect*rbinom(n = n(),prob = 0.2,size=1), int_coef=if_else(military!=0,int_coef,0)) by_resp \u0026lt;- lapply(by_resp, function(x) { if(is.double(x)) { x[is.na(x)] \u0026lt;- 0 } return(x) }) %\u0026gt;% as_data_frame # To make the outcome, need to turn the dataset long # However, we now need to drop the reference categories # Drop one dummy from actor/gift to prevent multicollinearity = reforms + government combination out_var \u0026lt;- gather(by_resp,var_name,var_value,-respondent,-task,-cluster) %\u0026gt;% filter(!(var_name %in% c(\u0026#39;reforms\u0026#39;,\u0026#39;government\u0026#39;))) %\u0026gt;% group_by(respondent,task) %\u0026gt;% summarize(outcome=sum(var_value)+5) combined_data \u0026lt;- left_join(out_var,by_resp,by=c(\u0026#39;respondent\u0026#39;,\u0026#39;task\u0026#39;)) # Re-estimate with a blocking variable combined_data$Q \u0026lt;- c(rep(1,floor(nrow(combined_data)/2)), rep(0,ceiling(nrow(combined_data)/2))) combined_data$outcome \u0026lt;- if_else(combined_data$Q==1,combined_data$outcome+1, combined_data$outcome) # # Create data predictor matrix and estimate coefficients from the simulated dataset # to_lm \u0026lt;- ungroup(combined_data) %\u0026gt;% select(contracts.supply:reforms,int_coef,Q) to_lm \u0026lt;- mutate_all(to_lm,funs(if_else(.!=0,1,.))) %\u0026gt;% mutate(outcome=combined_data$outcome) #No post-stratification # I don\u0026#39;t estimate a constituent term for int_coef because it is assumed to be zero results \u0026lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality + parliament + permit.export + permit.import + permit.reg + president + int_coef:military,data=to_lm) results_clust \u0026lt;- cluster.vcov(results,cluster = combined_data$respondent) pvals_adj \u0026lt;- coeftest(results,vcov.=results_clust)[-1,4]\u0026lt;0.05 pvals_orig \u0026lt;- coeftest(results)[-1,4]\u0026lt;0.05 total_sig_orig \u0026lt;- mean(pvals_orig) total_sig_adj \u0026lt;- mean(pvals_adj) int_sig_orig \u0026lt;- pvals_orig[\u0026#39;military:int_coef\u0026#39;] int_sig_adj \u0026lt;- pvals_adj[\u0026#39;military:int_coef\u0026#39;] # Now run the poststratification model results_ps \u0026lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality + parliament + permit.export + permit.import + permit.reg + president + int_coef:military + Q,data=to_lm) results_clust \u0026lt;- cluster.vcov(results,cluster = combined_data$respondent) pvals_adj \u0026lt;- coeftest(results_ps,vcov.=results_clust)[-1,4]\u0026lt;0.05 pvals_orig \u0026lt;- coeftest(results_ps)[-1,4]\u0026lt;0.05 total_sig_orig_blocker \u0026lt;- mean(pvals_orig) total_sig_adj_blocker \u0026lt;- mean(pvals_adj) int_sig_orig_blocker \u0026lt;- pvals_orig[\u0026#39;military:int_coef\u0026#39;] int_sig_adj_blocker \u0026lt;- pvals_adj[\u0026#39;military:int_coef\u0026#39;] out_results \u0026lt;- data_frame(int_sig_adj,int_sig_orig,int_sig_adj_blocker,int_sig_orig_blocker, total_sig_adj,total_sig_orig,total_sig_adj_blocker, total_sig_orig_blocker,abs_true_effect=abs(true_effect), true_effect=true_effect, est_effect=coef(results)[\u0026#39;military:int_coef\u0026#39;], est_effect_ps=coef(results)[\u0026#39;military:int_coef\u0026#39;]) }) out_data \u0026lt;- bind_rows(out_data) return(out_data) },mc.cores=parallel::detectCores(),mc.preschedule=FALSE) #save the data for inspection all_sims_data \u0026lt;- bind_rows(all_sims) %\u0026gt;% mutate(sample_size=rep(sampled_seq,each=n_sims), iter=rep(1:n_sims,times=num_breaks)) } if(run_sim==TRUE) { saveRDS(object = all_sims_data,file=\u0026#39;all_sims_data.rds\u0026#39;) } else { all_sims_data \u0026lt;- readRDS(\u0026#39;all_sims_data.rds\u0026#39;) } This simulation yields a row with the significant effect of the interaction term for that simulation for a total of n_sims draws. From this raw data I am able to calculate all of the necessary statistics mentioned above.\n# add in different calculations all_sims_data \u0026lt;- group_by(all_sims_data,sample_size) %\u0026gt;% mutate(sigeffVorig=ifelse(int_sig_orig, est_effect, NA), sigeffVadj=ifelse(int_sig_adj,est_effect,NA), sigeffVps_orig=ifelse(int_sig_orig_blocker,est_effect_ps,NA), sigeffVps_adj=ifelse(int_sig_adj_blocker,est_effect_ps,NA), powerVorig=int_sig_orig \u0026amp; (true_effect\u0026gt;0), powerVadj=int_sig_adj \u0026amp; (true_effect\u0026gt;0), powerVps_orig=int_sig_orig_blocker \u0026amp; (true_effect \u0026gt; 0), powerVps_adj=int_sig_adj_blocker \u0026amp; (true_effect \u0026gt; 0), SerrVorig=ifelse(int_sig_orig,1-(sign(est_effect)==sign(true_effect)),NA), SerrVadj=ifelse(int_sig_adj,1-(sign(est_effect)==sign(true_effect)),NA), SerrVps_orig=ifelse(int_sig_orig_blocker, 1-(sign(est_effect_ps)==sign(true_effect)),NA), SerrVps_adj=ifelse(int_sig_adj_blocker, 1-(sign(est_effect_ps)==sign(true_effect)),NA), MerrVorig=ifelse(int_sig_orig,abs(est_effect)/abs_true_effect,NA), MerrVadj=ifelse(int_sig_adj,abs(est_effect)/abs_true_effect,NA), MerrVps_orig=ifelse(int_sig_orig_blocker,abs(est_effect_ps)/abs_true_effect,NA), MerrVps_adj=ifelse(int_sig_adj_blocker,abs(est_effect_ps)/abs_true_effect,NA)) long_data \u0026lt;- select(all_sims_data,matches(\u0026#39;V|sample|iter\u0026#39;)) %\u0026gt;% gather(effect_type,result,-sample_size,-iter) %\u0026gt;% separate(effect_type,into=c(\u0026#39;estimate\u0026#39;,\u0026#39;estimation\u0026#39;),sep=\u0026#39;V\u0026#39;) %\u0026gt;% mutate(estimate=factor(estimate,levels=c(\u0026#39;sigeff\u0026#39;,\u0026#39;power\u0026#39;,\u0026#39;Serr\u0026#39;,\u0026#39;Merr\u0026#39;), labels=c(\u0026#39;Mean\\nSignificant\\nEffect\u0026#39;, \u0026#39;Mean\\nPower\u0026#39;, \u0026#39;S-Error\\nRate\u0026#39;, \u0026#39;M-Error\\nRate\u0026#39;)), estimation=factor(estimation,levels=c(\u0026#39;adj\u0026#39;,\u0026#39;orig\u0026#39;,\u0026#39;ps_adj\u0026#39;,\u0026#39;ps_orig\u0026#39;), labels=c(\u0026#39;No Post-Stratification\\nClustered Errors\\n\u0026#39;, \u0026#39;No Post-Stratification\\nUn-clustered Errors\\n\u0026#39;, \u0026#39;Post-Stratification\\nClustered Errors\\n\u0026#39;, \u0026#39;Post-Stratification\\nUn-clustered Errors\\n\u0026#39;))) long_data_treatment \u0026lt;- select(all_sims_data,matches(\u0026#39;total|iter|sample\u0026#39;)) %\u0026gt;% gather(effect_type,result,-sample_size,-iter) %\u0026gt;% mutate(effect_type=factor(effect_type,levels=c(\u0026#39;total_sig_adj\u0026#39;, \u0026#39;total_sig_orig\u0026#39;, \u0026#39;total_sig_adj_blocker\u0026#39;, \u0026#39;total_sig_orig_blocker\u0026#39;), labels=c(\u0026#39;No Post-Stratification\\nClustered Errors\\n\u0026#39;, \u0026#39;No Post-Stratification\\nUn-clustered Errors\\n\u0026#39;, \u0026#39;Post-Stratification\\nClustered Errors\\n\u0026#39;, \u0026#39;Post-Stratification\\nUn-clustered Errors\\n\u0026#39;))) # Plot a sample of the data (too big to display all of it) long_data %\u0026gt;% ungroup %\u0026gt;% slice(1:10) %\u0026gt;% select(-estimation) %\u0026gt;% mutate(estimate=str_replace(estimate,\u0026quot;\\\\n\u0026quot;,\u0026quot; \u0026quot;)) %\u0026gt;% knitr::kable(.) %\u0026gt;% kable_styling(font_size = 8)   sample_size  iter  estimate  result      1334.783  1  Mean Significant Effect  0.6298429    1334.783  2  Mean Significant Effect  0.3874088    1334.783  3  Mean Significant Effect  NA    1334.783  4  Mean Significant Effect  1.1438379    1334.783  5  Mean Significant Effect  0.5653086    1334.783  6  Mean Significant Effect  1.2689594    1334.783  7  Mean Significant Effect  NA    1334.783  8  Mean Significant Effect  NA    1334.783  9  Mean Significant Effect  NA    1334.783  10  Mean Significant Effect  NA      Plotting I use the gam function in the ggplot2 package to plot a smoothed regression line of the simulation draws for each sample size.\nFirst we can look at the difference that clustered errors makes across the different statistics. The only noticeable differences are at sample sizes smaller than 500. Clustering on respondents tends to result in smaller average significant effects, but it also results in increases in sign errors. This finding differs from the literature that considers clustering important to control for intra-respondent correlation, which in this simulation was fixed at 0.5. At sample sizes larger than 500, there does not appear to be any difference between clustered and un-clustered estimates.\ng_title \u0026lt;- guide_legend(title=\u0026#39;\u0026#39;) filter(long_data,grepl(\u0026#39;No Post\u0026#39;,estimation)) %\u0026gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) + theme_minimal() + stat_smooth(colour=\u0026#39;red\u0026#39;) + xlab(\u0026#39;Sample Size\u0026#39;) + ylab(\u0026quot;\u0026quot;) + facet_wrap(~estimate,scales=\u0026#39;free\u0026#39;) + theme(panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank()) + scale_color_brewer(palette=\u0026#39;Accent\u0026#39;) + guides(colour=g_title,linetype=g_title) + theme(legend.position = \u0026#39;bottom\u0026#39;) ggsave(\u0026#39;clust_err.png\u0026#39;,units=\u0026#39;in\u0026#39;,width=6) Next I look at post-stratification as an option to improve the precision of estimates. For unclustered errors reported below, post-stratified estimates do have higher power and slightly lower average significant effects, and importantly, the post-stratified estimates worsen neither type S nor type M errors.\ng_title \u0026lt;- guide_legend(title=\u0026#39;\u0026#39;) filter(long_data,grepl(\u0026#39;Un-clustered\u0026#39;,estimation)) %\u0026gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) + theme_minimal() + stat_smooth(colour=\u0026#39;red\u0026#39;) + xlab(\u0026#39;Sample Size\u0026#39;) + ylab(\u0026quot;\u0026quot;) + facet_wrap(~estimate,scales=\u0026#39;free\u0026#39;) + theme(panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank()) + scale_color_brewer(palette=\u0026#39;Accent\u0026#39;) + guides(colour=g_title,linetype=g_title) + theme(legend.position = \u0026#39;bottom\u0026#39;) ggsave(\u0026#39;post_unclust_err.png\u0026#39;,units=\u0026#39;in\u0026#39;,width=6) Post-stratification appears to have a similar effect on clustered error estimations, although the differences are smaller. In smaller samples, post-stratified estimates do have smaller M-errors.\ng_title \u0026lt;- guide_legend(title=\u0026#39;\u0026#39;) filter(long_data,!grepl(\u0026#39;Un-clustered\u0026#39;,estimation)) %\u0026gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) + theme_minimal() + stat_smooth(colour=\u0026#39;red\u0026#39;) + xlab(\u0026#39;Sample Size\u0026#39;) + ylab(\u0026quot;\u0026quot;) + facet_wrap(~estimate,scales=\u0026#39;free\u0026#39;) + theme(panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank()) + scale_color_brewer(palette=\u0026#39;Accent\u0026#39;) + guides(colour=g_title,linetype=g_title) + theme(legend.position = \u0026#39;bottom\u0026#39;) ggsave(\u0026#39;post_clust_err.png\u0026#39;,units=\u0026#39;in\u0026#39;,width=6) Finally, I also report average numbers of significant coefficients for the 14 treatments. Given that the 14 treatments were sampled from a normal distribution with prior density in the positive values with a meaan of 0.5, in expectation 95% of estimates should be statisticall significant. While that upper limit is reached only in high sample numbers, it looks like the ratio for treatment effects reaches an acceptable level of 70 percent at about 500 sample respondents. Also, post-stratifying un-clustered models results in effects that are reported as significant at much higher rates, as would follow from the previous results about post-stratification.\ng_title \u0026lt;- guide_legend(title=\u0026#39;\u0026#39;) ggplot(long_data_treatment,aes(y=result,x=sample_size,linetype=effect_type,colour=effect_type)) + theme_minimal() + stat_smooth() + xlab(\u0026#39;Sample Size\u0026#39;) + ylab(\u0026quot;\u0026quot;) + theme(panel.grid.minor.y = element_blank(), panel.grid.major.y = element_blank()) + scale_color_brewer(palette=\u0026#39;Dark2\u0026#39;) + guides(linetype=g_title,colour=g_title) + theme(legend.position = \u0026#39;bottom\u0026#39;) ggsave(\u0026#39;all_treat_rate.png\u0026#39;,units=\u0026#39;in\u0026#39;,width=6)  Conclusion This simulation study shows that a sample size of approximately 1,000 respondents is enough to obtain high power while also lowering both the S and M-error rates for treatment interaction effects in this conjoint experiment. The treatment effects themselves are generally of high quality once the sample size reaches 500 because the total number of respondents in each treatment cell is considerably higher than in an interaction. Post-stratification appears to be a useful strategy to increase precision without inducing S or M errors; at the very least, post-stratification does not appear to have any adverse effects on the estimation.\nOn the other hand, it appears that clustering errors increases the S-error rate at small sample sizes, a surprising finding considering that clustering methods are designed to inflate, not deflate, standard errors. Given that the S-error rate reveals the likelihood of making an error about the sign of the treatment effect, this is a potentially serious problem. For that reason I intend to report both clustered and un-clustered estimates in my analysis.\n ","date":1549843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549843200,"objectID":"087e1b2baa6bb4590bd91cdb5d6d4b04","permalink":"http://www.robertkubinec.com/post/conjoint_power_simulation/","publishdate":"2019-02-11T00:00:00Z","relpermalink":"/post/conjoint_power_simulation/","section":"post","summary":"Background Conjoint survey experiments have become more popular in political science since the publication of Hainmueller, Hopkins and Yamamoto (2014). However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes.","tags":null,"title":"Simulating Conjoint Survey Experiments","type":"post"},{"authors":null,"categories":null,"content":"","date":1535731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535731200,"objectID":"d1c28b7c6dddd4b01e6ca102a8afd428","permalink":"http://www.robertkubinec.com/talk/apsa_2018_tunisia/","publishdate":"2018-08-31T16:00:00Z","relpermalink":"/talk/apsa_2018_tunisia/","section":"talk","summary":"The risk of Tunisia’s nascent democracy falling to an incumbent takeover has declined considerably with the fragmentation of its autocratic successor party, Nidaa Tounes. What explains the many defections from Nidaa Tounes since 2014? While the popular wisdom would highlight President Essebsi’s attempts to elevate his son in the party leadership, we explore whether ideological and policy differences played a key role as well. We present an original dataset of the complete roll-call voting history from Tunisia’s first and second parliaments since the Arab Spring. Using this data and dynamic item-response theory, we examine whether MPs that were soon to break away from Nidaa Tounes had significantly different voting patterns than MPs who remained with Nidaa Tounes. Our research explores the internal mechanisms of autocratic successor party fragmentation in emerging democracies and also sheds light on why national unity governments so often fall apart.","tags":null,"title":"Fragmentation of Autocratic Successor Parties: The Case of Tunisia","type":"talk"},{"authors":null,"categories":null,"content":"","date":1535544000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535544000,"objectID":"c756fc13b47abef5e8bfa47cc2ac5634","permalink":"http://www.robertkubinec.com/talk/preconf_2018/","publishdate":"2018-08-29T12:00:00Z","relpermalink":"/talk/preconf_2018/","section":"talk","summary":"Scholars continue to disagree as to what extent international social connections act as a conduit to influence contentious politics within states. To answer this question, we provide the first rigorous and real-time measure of transnational ideological diffusion across sectarian groups by employing a novel statistical method and new data to capture the transnational dynamics of polarization after the Arab Uprisings of 2011. As authoritarian governments fell, populations in several states polarized between secularists and Islamists over what kind of regime was to replace the ousted one. To examine these endogenous processes, we collected a comprehensive dataset on elite and citizen Twitter accounts in Cairo and Alexandria (Egypt) and Tunis (Tunisia) for a ten-month period during the critical year of 2013. Given the difficulty in directly measuring polarization, we also developed a new model, item response theory-vector autoregression (IRT-VAR), that allows us to incorporate measurement uncertainty while providing over-time estimates of transnational polarization. We show through our model that following catalytic events like regime ousters (such as the military coup against the Muslim Brotherhood in Egypt), we can separate the direct effects of these events on group polarization within a country from indirect transnational feedback happening through the channel of social media.","tags":null,"title":"When Groups Fall Apart: Measuring Transnational Polarization during the Arab Uprisings","type":"talk"},{"authors":null,"categories":null,"content":"     This is a paper that was presented at the StanCon2018 conference on Bayesian inference with the Stan Hamiltonian Markov Chain Monte Carlo (MCMC) method. Video of my talk is available here.\nIntroduction This notebook introduces idealstan, a new R package front-end to Stan that allows for flexible modeling of a class of latent variable models known as ideal point models. Ideal point modeling is a form of dimension reduction, and shares similarities with multi-dimensional scaling, factor analysis and item-response theory. While the parameterization employed by idealstan is a derivation based on item-response theory, it is important to note that several other parameterizations are possible (Carroll, Lewis, Lo, Poole, and Rosenthal, 2013; Armstrong, Bakker, Carroll, Hare, Poole, and Rosenthal, 2014).\nWhat distinguishes ideal point modeling from other latent space models is that the latent factor is bi-directional: in other words, the unobserved latent construct that underlies the data could increase or decrease as the observed stimuli either increase or decrease. This type of latent variable is very useful when the aim is to cluster units around a polarizing dimension, such as the left-right axis in politics or the \\(p\\)-value debate in statistics. While many of the applications of ideal point models are in political science and economics, it certainly can be applied more broadly, especially as the approach is related to latent space models more generally, such as in Hoff, Raftery, and Hancock (2002).\nThere are well-established frequentist (emIRT) and Bayesian (pscl and MCMCpack) packages for ideal point inference with R. However, these packages offer standard models that are limited in scope to particular problems, especially when it comes to including predictors in the latent space. The R package idealstan has three features that set it apart from existing approaches:\nStan offers significant flexibility compared to existing approaches in using non-conjugate priors and a clearer programming interface. This flexibility allows for more options for end users in ideal point modeling that can be used to test new theories and hypotheses instead of being limited in the range and type of models available. In particular, Stan makes it easier to add hierarchical and time-series priors on to parameters, which opens the door to further analysis of dynamic and time-varying social phenomena. Stan scales well with the number of parameters, which is an attribute of most dimension reduction methods. Advances in Hamiltonian Monte Carlo estimation, including the use of variational inference and soon parallel computing within chains, promise to make full Bayesian inference of these models practical even with very large datasets. Increasingly, Stan is being married to helpful and powerful diagnostic packages, including bayesplot, shinystan and loo, which extends the ability of idealstan to provide not only cutting-edge Bayesian inference but also increasingly sophisticated tools for graphical analysis of resulting estimates. Given the complex nature of latent variable models, diagonistics are extremely important for determining when the model is behaving as it should (and what the model should be doing in the first place).  idealstan is an effort to build on prior efforts but also to offer new models that satisfy the increasing variety of applications to which ideal point models are being put. Both pscl and MCMCpack were designed for ideal point modeling of binary (logit) data from legislatures in which the outcome is made up of yes and no votes cast by legislators (Clinton, Jackman, and Rivers, 2004; Martin and Quinn, 2002). More recently scholars have begun applying ideal point models to Twitter data (Barberá, 2015), massive campaign finance datasets (Bonica, 2014) and party campaign manifestos (Slapin and Proksch, 2008). This package offers both the traditional forms of ideal point models along with new extensions, including a version of ideal points that can take into account certain forms of missing data.\nIn this notebook I first introduce ideal-point models and contrast them with “traditional” item response theory (IRT), and then I demonstrate the idealstan package through simulations. I then perform two empirical analyses, the first of coffee product ratings from Amazon and the second with voting data taken from the 114th Senate. In the second example, I also show how idealstan enables new estimation of strategic legislator absence, a type of data that is usually coded as missing in vote datasets.\n Ideal Point Models as a Subset of Statistical Measurement Measurement error models have a long history in applied statistics and are increasingly in demand as the amount of noisy observational data grows with the digital revolution. Canonical statistical models, particularly linear regression, assume that the predictors are measured without error, but in many situations in social science, the variable of interest cannot in fact be measured. Rather, proxies or indicators are used to stand in for the latent construct. Measurement models offer a way to match the indicators with the latent construct while making appropriate assumptions about the relationship.\nIn other words, we suppose that the regression predictor matrix \\(X\\) is itself a function of certain indicators \\(I\\_c \\in \\{1 ... C\\}\\):\n\\[ X = f(\\forall I\\_c \\in \\{1 ... C\\}) \\]\nDifferent latent variable, latent space and measurement models can be distinguished in terms of the function \\(f(\\cdot)\\). Fundamentally, \\(f(\\cdot)\\) has to stipulate how \\(X\\) will change as the indicators change. It is possible to make very precise conditions for this relationship, which is the method applied in confirmatory factor analysis and structural equation modeling. It is also possible to let \\(f(\\cdot)\\) make only minimal assumptions about this relationship, which is the method adopted by exploratory factor analysis and item-response theory (IRT), in addition to the other latent space models in the literature. It is also possible, of course, to have models that fall somewhere in between truly exploratory and truly confirmatory models.\nWhile much has been written about the different kids of \\(f(\\cdot)\\) that can be used in measurement models, in this paper I focus on a metric relevant to ideal point models: as the values of \\(I\\_c\\) increase, does \\(X\\) also increase so that \\(f(\\cdot)\\) must be always non-decreasing? If that is the case, then the model can be thought of as falling into the domain of item-response theory and traditional factor analysis (in fact, IRT is itself a non-linear version of factor analysis per Takane and de Leeuw (1986)). By contrast, ideal point models are based on a different set of assumptions in which \\(X\\) could increase or decrease as the indicators \\(I\\_c\\) increase or decrease; in other words, the relationship is bi-polar instead of uni-polar.\nIdeal point modeling originated from analysis of a common situation in policy research in which different legislators select from among competing policy alternatives based on the utility offered by each policy (Enelow and Hinich, 1984; Poole, Lewis, Lo, and Carroll, 2008). Supposing that the policies are evaluated on a uni-dimensional space, if the utility of the “Yes” position on the policy is greater than the utility of the “No” position to a particular legislator, then that legislator will choose that policy. While very simple, this model has a requirement that is different from much of traditional IRT estimation: the policy outcomes can have different meanings based on their position relative to the legislator in the policy space. For example, if a legislator is very liberal (has a high value on the latent construct), then that legislator is likely to vote yes on bills that are also liberal, such as single-payer health care. But if the bill is conservative, such as national defense, then that legislator is more likely to vote no. In other words, the meaning of the positions of the bills in the latent space depends on the ideal point of the legislator, and thus the mapping to the latent space \\(f(\\cdot)\\) cannot be always non-decreasing. For some votes, a yes position may mean a high value of the latent space, while for others it may mean a lower value.\nBy comparison, in traditional IRT, responses to stimuli (the indicators \\(I\\_c\\)) reflect correct or incorrect answers, such as a student taking a test. Correct answers, which in the legislative context mean yes votes, always equal greater ability, while incorrect answers, which would be no votes, are always a sign of less ability. In this situation, \\(f(\\cdot)\\) is always non-decreasing. Thus while ideal point models share much in common with IRT, they require different assumptions.\nA brief review of the mathematical notation will make the difference clear. The 2-PL IRT model takes the following form:\n\\[ Y\\_{ij} = \\alpha\\_j x\\_i - \\beta\\_j \\]\nwhere \\(Y\\_{ij}\\) can be an outcome of any type (binary, ordinal, Poisson, etc.) that includes the correct or incorrect responses of \\(I\\) test takers to \\(J\\) test items, \\(\\alpha\\_j\\) are the discrimination parameters that control the narrowness, hence discrimination, of each item \\(j\\) on the test in the latent space, \\(x\\_i\\) are the test-taker ability parameters that reflect the ability of a person to answer an item correctly, and \\(\\beta\\_j\\) are the difficulty or average probability/value of a correct response (the intercept). In the traditional IRT format, the \\(\\alpha\\_j\\) discrimination parameters are always positive because a higher value (a correct answer) is always associated with higher ability \\(x\\_i\\), while a lower value (an incorrect answer) is associated with less ability.\nHowever, if the requirement that the discrimination parameters \\(\\alpha\\_j\\) are positive is removed, then this model can also be used for ideal point modeling. For most applications, ideal-point modeling using IRT is built on the standard 2-PL model (Clinton, Jackman, and Rivers, 2004; Bafumi, Gelman, Park, and Kaplan, 2005), with one notable exception: the discrimination parameters must be un-constrained, while in traditional IRT the discrimination parameters are constrained to all be either positive or negative. Constraining discrimination parameters to be positive is also the approach taken in the edstan package, which offers the full range of standard IRT models using Stan.\nLeaving the discrimination parameters un-consrained ensures that a higher or lower value of \\(Y\\_{ij}\\) could be associated with either a “correct” or “incorrect” answer as is necessary in an ideal point model. This happens because the latent space in an ideal point model is fundamentally a Euclidean distance that is rotation-invariant. The following figure makes this clear by showing a version of an ideal point model in which the ideal point (ability parameter) \\(x\\_i\\) is represented by a Normal distribution while the Yes and No points of a proposed policy are vertical lines.\nIf \\(x\\_i\\) is to the right of the Indifference line, the legislator will vote Yes on the proposal, whereas if she is to the left, she will vote No. However, a different policy (item) could have the Yes and No positions switched so that a Yes vote would correspond to -2.5 and vice versa for the No vote. Thus the binary outcomes do not have the same interpretation as the standard IRT model because the relationship between the outcomes and the ability points is contingent.\nIf the discrimination parameters are unconstrained, Clinton, Jackman, and Rivers (2004) showed that the midpoint of the policy/item parameter, which is labeled on the plot as the indifference line, is identified as \\(-\\frac{\\alpha\\_j}{\\beta\\_j}\\) in the original IRT model. This midpoint is important because it represents the point in the latent space in which an actor is indifferent, or has a 50% chance of voting yes or no. Unfortunately, the IRT paramerization of the ideal point model does not return the actual Yes or No positions of a bill/item, but these midpoints are sufficient to characterize the position of the items in the latent space.\nWhile this difference between ideal point models and standard IRT may seem trivial, it has serious consequences for modeling. The basic issue is the non-identifiability of the IRT model, which the switching polarity of the discrimination parameters only makes more difficult. Ultimately, it becomes necessary to constrain the polarity of some of the ability or discrimination parameters to identify the model. However, it is necessary to choose parameters which would avoid “splitting the likelihood”, such as constraining a legislator whose true ideal point is near zero (Bafumi, Gelman, Park, et al., 2005).\nidealstan exploits variational Bayesian (VB) inference (Kucukelbir, Ranganath, Gelman, and Blei, 2015) to assist with finding parameters for identification. idealstan first estimates an IRT model with rotation-unidentified parameters, which is not a problem for a single VB run. Then idealstan can select those parameters which show the highest or lowest values, and these parameters are then constrained in terms of their polarity. This approach can quickly speed up the sometimes painful process of identifying a particular IRT ideal point model, although it is also possible to constrain parameters before estimation based on prior information.\n Missing Data in Ideal Point Models Because of the flexibility of Stan,idealstan is not limited to implementing the standard IRT ideal point model. One of the modifications it offers is a model that can account for one-way censoring in the data, or what is called an absence-inflated ideal point model (Kubinec, 2017a). This model was motivated by the application of ideal point models to legislatures in which legislator actions are more diverse than simply Yes or No votes. Legislators can also be absent on votes and in parliamentary systems as in Europe, abstentions are also common. Abstentions occur when a legislator votes on a policy but refuses to either vote Yes or No, resulting in a vote record of abstention that conventional models will treat as missing data. However, if these two additional vote outcomes–absent and abstention–are removed from the estimation by encoding them as missing data, then the additional information about legislator behavior present in this data is lost.\nAs a solution for abstentions, I proposed in Kubinec (2017a) to model abstentions as a middle category between yes and no votes by treating \\(Y\\_{ij}\\) as an ordered logistic outcome \\(Y\\_{ijk}\\) for \\(k\\) in three possible vote choices: \\(\\{1,2,3\\}\\) (no, abstain, yes).\nWhile switching from a logit to an ordered logistic model is straightforward, modeling the censoring implied by legislator absences requires the estimation of additional parameters. To do so, I model absence as a binary outcome in a separate equation as a hurdle model. Intuitively, legislators only show up to vote if they are able to overcome the hurdle of being present. Because in an ideal point model we only want to estimate a single set of person parameters (ideal points), I include an additional set of item (bill) parameters in the hurdle component that signify the way in which being absent on a policy proposal is related to the ideological value of the legislation.\nThe result is a two-stage model that inflates the ideal points by the probability that a legislator is absent on a particular bill. To create this model, I first start again with the essential 2-PL IRT model except that it now allows for cutpoints \\(c\\_k\\) for the \\(K-1\\) vote outcomes (yes, abstain, no) with \\(\\zeta(\\cdot)\\) representing the logit function:\n\\[ L(\\beta,\\alpha,x|Y\\_{ijk}) = \\prod\\_{i-1}^{I} \\prod\\_{j=1}^{J} \\begin{cases} 1 - \\zeta(x\\_{i}\u0026#39;\\beta\\_j - \\alpha\\_j - c\\_1) \u0026amp; \\text{if } K = 0 \\\\ \\zeta(x\\_{i}\u0026#39;\\beta\\_j - \\alpha\\_j - c\\_{k-1}) - \\zeta(x\\_{i}\u0026#39;\\beta\\_j - \\alpha\\_j - c\\_{k}) \u0026amp; \\text{if } 0 \u0026lt; k \u0026lt; K, \\text{ and} \\\\ \\zeta(x\\_{i}\u0026#39;\\beta\\_j - \\alpha\\_j - c\\_{k-1}) - 0 \u0026amp; \\text{if } k=K \\end{cases} \\]\nExcept for the addition of the vote cutpoints, this model is identical to the standard 2-PL IRT form, with ability parameters \\(x\\_i\\), discriminations \\(\\beta\\_j\\), and difficulties \\(a\\_j\\). The cutpoints carve up the latent space so that as ability rises or falls, the legislator becomes more likely to vote yes or no with abstention falling in between these two categories.\nThis ordered logit has to be embedded in the hurdle model to account for one-way censoring. Suppose that each legislator has a choice \\(r \\in \\{0,1\\}\\) in which \\(r=1\\) if a legislator is absent and \\(r=0\\) otherwise. With this notation, the likelihood of the hurdle model takes the following form:\n\\[ L(\\beta,\\alpha,X,Q,\\gamma,\\omega|Y\\_{k},Y\\_{r}) = \\prod\\_{I}^{i=1} \\prod\\_{J}^{j=1} \\begin{cases} \\zeta(x\\_{i}\u0026#39;\\gamma\\_j - \\omega\\_j ) \u0026amp; \\text{if } r=0, \\text{ and} \\\\ (1-\\zeta({x\\_{i}\u0026#39;\\gamma\\_j - \\omega\\_j}))L(\\beta,\\alpha,X|Y\\_{k1}) \u0026amp; \\text{if } r=1 \\end{cases} \\]\nFor the hurdle model to be able to affect the legislator ideal points, a separate IRT 2-PL model is included to predict the binary outcome of absence or presence. In this secondary model, the \\(\\gamma\\_j\\) discrimination and \\(\\omega\\_j\\) difficulty parameters represent the salience of a particular piece of legislation to an individual legislator \\(x\\_i\\). Only if a bill clears the hurdle of salience will the legislator choose to vote on the legislation, i.e., reach the vote outcome model \\(L(\\beta,\\alpha,X)\\). This device allows for the ideal points \\(x\\_i\\) to adjust for the fact that the decision of a legislator to show up to vote on a particular bill may be strategic instead of random. In addition, this model is more widely applicable to situations in which an ideal point model is employed and missing data may be a function of the persons’ ideal points.\n Model Simulation To identify the model, I placed \\(N(0,1)\\) parameters on the \\(x\\_i\\) ideal points and additional polarity constraints on either the ideal points \\(x\\_i\\) or the discrimination parameters \\(\\gamma\\_j\\) and \\(\\alpha\\_j\\). As a further restriction, I constrain one of the \\(\\beta\\_j\\) to equal zero. These are the standard set of restrictions included in idealstan, although the scales of parameters can be modified. The full set of priors is as follows:\n\\[ c\\_k - c\\_{k-1} \\sim N(0,5)\\\\ \\gamma\\_j \\sim N(0,2)\\\\ \\omega\\_j \\sim N(0,5)\\\\ \\beta\\_j \\sim N(0,2)\\\\ \\alpha\\_j \\sim N(0,5)\\\\ x\\_i \\sim N(0,1) \\]\nThe \\(c\\_k\\) parameters are the cutpoints for the \\(K-1\\) ordinal outcomes. They are given a weakly informative prior on the differences between the cutpoints (see the prior help file in the Stan Development Github site). The rest of the priors are arbitrary as the scale of the latent variables is fixed only in the priors themselves.\nTo demonstrate the package, I first generate data from this data-generating process using a function id_sim_gen() built into idealstan:\n {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"],[1,3,4,4,4,3],[4,1,1,1,4,4],[4,2,3,1,3,4],[1,4,4,4,4,1],[1,4,3,4,4,4],[4,2,3,2,3,2],[1,4,4,4,4,2],[2,4,4,4,4,4],[4,1,1,1,3,4],[4,3,3,3,4,3]],\"container\":\"\\n \\n \\n  \\n 1\\n 2\\n 3\\n 4\\n 5\\n 6\\n 7\\n 8\\n 9\\n 10\\n \\n \\n\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":[1,2,3,4,5,6,7,8,9,10]},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\"}},\"evals\":[],\"jsHooks\":[]} In this matrix, the legislators (persons) are represented by the rows and the bills (items) by the columns. The ordinal vote outcomes are numbered 1 to 3 (1=No,2=Abstain,3=Yes) and 4 represents absence.\nI can then take this simulated data and put it into the id_estimate function. I also use the true values of the legislator ideal points \\(x\\_i\\) for polarity constraints in order to be able to return the “true” latent variables. The id_estimate function loads pre-compiled stan code a la rstantools and then returns an R object containing a compiled stan model.\ntrue_legis \u0026lt;- ord_ideal_sim@simul_data$true_person high_leg \u0026lt;- sort(true_legis,decreasing = T,index.return=T) low_leg \u0026lt;- sort(true_legis,index.return=T) ord_ideal_est \u0026lt;- id_estimate(idealdata=ord_ideal_sim, model_type=4, fixtype=\u0026#39;constrained\u0026#39;, restrict_type=\u0026#39;constrain_twoway\u0026#39;, restrict_ind_high = high_leg$ix[1:2], restrict_ind_low=low_leg$ix[1:2], refresh=500) For testing simulation results, the package contains a residual plot for looking at differences between true and estimated values which are stored in the R object:\nThe recovery of the true parameters is not perfect, but generally the errors sum to zero across the parameters. Recovering the true parameters is not usually of great interest in a latent variable model as the scale and rotation of the true values is rather arbitrary. However, this exercise is a basic test of the Stan model’s correspondence with the simulated data.\n Empirical Example: The Polarization of Coffee To demonstrate the model’s functionality, I first use a dataset of ordinal rankings draw from Amazon food product reviews. McAuley and Leskovec (2013) collected over 500,000 Amazon food reviews from 1999 to 2012 which are all coded on the 1 to 5 ranking scale that users can post on each product. I focus on a subset of this data by collecting all the reviews that mention the word “coffee” at least a handful of times.\njust_coffee \u0026lt;- readRDS(\u0026#39;just_coffee.rds\u0026#39;) Ideal point models use variation between products and users in reviews to identify the latent space. For these reasons, little information is contributed by products and/or users that have very few reviews, and we can remove them from the data first to reduce the dimensionality of the data. Because idealstan is a full Bayesian model, these users and products do not cause any statistical problems, but they will slow estimation considerably as a large number of users only review one or two products.\ncoffee_count_prod \u0026lt;- group_by(just_coffee,ProductId) %\u0026gt;% summarize(tot_unique=length(unique(UserId))) %\u0026gt;% filter(tot_unique\u0026lt;30) coffee_count_user \u0026lt;- group_by(just_coffee,UserId) %\u0026gt;% summarize(tot_unique=length(unique(ProductId))) %\u0026gt;% filter(tot_unique\u0026lt;3) just_coffee \u0026lt;- anti_join(just_coffee,coffee_count_prod,by=\u0026#39;ProductId\u0026#39;) %\u0026gt;% anti_join(coffee_count_user,by=\u0026#39;UserId\u0026#39;) The data is currently in long format. To bring the data into idealstan, it first must be translated to wide format in which each item (product) is a column and each person (user) is a row. For this model, we will drop missing data as we will focus solely on the model’s ordinal rankings without accounting for the fact that users do not have reviews for all products.\nEven with inactive users removed from the data, this model is quite large with 11269 users and 429 products. However, we can take advantage of variational inference in Stan to obtain approximate posterior estimates in a reasonable amount of time. We will also take advantage of idealstan’s use of variational inference for automatic model identification by running a non-identified (i.e., no constraints) model first, determining which of the users has a very high ideal point, and then constraining that ideal point in the final fitted model.\ncoffee_model \u0026lt;- id_estimate(idealdata=coffee_data, model_type=3, fixtype=\u0026#39;vb\u0026#39;, restrict_ind_high = 1, restrict_params=\u0026#39;person\u0026#39;, auto_id = F, use_vb=T) ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## ## ## ## Gradient evaluation took 0.042 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 420 seconds. ## Adjust your expectations accordingly! ## ## ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -6e+004 1.000 1.000 ## 200 -5e+004 0.642 1.000 ## 300 -5e+004 0.441 0.284 ## 400 -5e+004 0.346 0.284 ## 500 -5e+004 0.277 0.059 ## 600 -5e+004 0.234 0.059 ## 700 -5e+004 0.200 0.040 ## 800 -5e+004 0.176 0.040 ## 900 -5e+004 0.157 0.014 ## 1000 -5e+004 0.142 0.014 ## 1100 -5e+004 0.042 0.006 MEDIAN ELBO CONVERGED ## ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. ## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## ## ## ## Gradient evaluation took 0.035 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 350 seconds. ## Adjust your expectations accordingly! ## ## ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -5e+009 1.000 1.000 ## 200 -1e+008 22.582 44.164 ## 300 -9e+010 15.388 1.000 ## 400 -4e+009 17.796 25.020 ## 500 -2e+007 58.526 25.020 ## 600 -2e+006 49.788 25.020 ## 700 -1e+006 42.834 6.097 ## 800 -5e+004 40.303 22.581 ## 900 -5e+004 35.826 6.097 ## 1000 -5e+004 32.244 6.097 ## 1100 -5e+004 32.146 6.097 MAY BE DIVERGING... INSPECT ELBO ## 1200 -5e+004 27.730 1.114 MAY BE DIVERGING... INSPECT ELBO ## 1300 -5e+004 27.630 1.114 MAY BE DIVERGING... INSPECT ELBO ## 1400 -5e+004 25.128 0.016 MAY BE DIVERGING... INSPECT ELBO ## 1500 -5e+004 2.984 0.015 MAY BE DIVERGING... INSPECT ELBO ## 1600 -5e+004 2.374 0.005 MEDIAN ELBO CONVERGED MAY BE DIVERGING... INSPECT ELBO ## ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. What is of primary interest in this model are the discrimination parameters for the products. The discriminations will tell us which coffee products tend to divide the users most strongly into two poles as the model is one-dimensional. We can first look at the distribution of discriminations via a histogram:\nid_plot_all_hist(coffee_model,params = \u0026#39;regular_discrim\u0026#39;) To gain a sense of how polarizing these products, we can plot the product mid-point, or the point at which a user is indifferent between one rating score and a higher rating score, over the ideal point distribution. I color the user points by their actual ratings:\nid_plot_legis(coffee_model,group_color=F,group_overlap = T, person_labels=F,person_ci_alpha = .05, item_plot=429, group_labels=F, point_size=2, show_score = c(\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;4\u0026#39;,\u0026#39;5\u0026#39;), text_size_group = 4) As can be seen, the product midpoints nearly perfectly segment the user base. We also a large number of 1s and 5s in the observed ratings. This distribution shows that users tend to be very divided on this product, and also that their disagreement over the product is itself a reflection of an underlying dimension, their ideal points.\nFinally, I include here the top 10 most polarizing (highly discriminative) products from each end of the ideal point spectrum.\nFirst, the top 10 most positive discrimination:\n {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],[\"Kicking Horse Coffee 454 Horse Power Dark, Whole Bean\",\"Amazing Grass Green Superfood Organic Powder\",\"Starbucks Via Ready Brew Instant Coffee\",\"Stephen's Gourmet Hot Cocoa, Candycane Cocoa\",\"Prince of Peace Organic Tea, Oolong\",\"NOW Foods Erythritol Natural Sweetener\",\"illy Cappucino, 12 pack\",\"Nestle Hot Cocoa Mix\",\"Better than Milk Vegan Soy Powder\",\"Ghirardelli Chocolate Sweet Ground Chocolate \u0026amp; Cocoa\"],[7.87259,7.864585,7.80567,7.805575,7.770375,7.770345,7.727985,7.33822,7.33797,7.28447]],\"container\":\"\\n \\n \\n  \\n Product Description\\n Discrimination\\n \\n \\n\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\"}},\"evals\":[],\"jsHooks\":[]} Second, the top 10 least positive negative discrimination:\n {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],[\"Melitta Cafe de Europa Gourmet Coffee\",\"McCann's Steel Cut Irish Oatmeal\",\"Stash Tea Gunpowder Green Loose Leaf Tea\",\"Nutiva Organic Coconut Oil\",\"Stash Tea Company English Breakfast\",\"Equal Exchange Organic Coffee, Mind Body Soul, Whole Bean\",\"Ghirardelli Premium Hot Beverage Mix, White Mocha, 19-Ounce Cans\",\"Victorian Inn Instant Hot Cappucino, French Vanilla\",\"Medaglia Doro Instant Espresso\",\"Yogi Tea, Green Tea Super Antioxidant\"],[-9.79306,-9.785615,-9.70132,-9.616965,-7.77101,-7.75944,-7.74106,-7.73298,-7.17082,-7.13721]],\"container\":\"\\n \\n \\n  \\n Product Description\\n Discrimination\\n \\n \\n\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\"}},\"evals\":[],\"jsHooks\":[]} While a full interpretation of these results would require significant work at examining the full range of products and their relevant discrimination scores, what is clear is that positive discrimination tends to have more large brands of coffee, including Starbucks and Illy, while the negative discrimination products tend to b esmaller brands, such as Melitta Cafe and Equal Exchange Organic Coffee. The most interesting finding in this analysis is that Ghirardelli products are at both ends of the scale: one type of Ghirardelli powdered drink, Ghirardelli Chocolate, has high positive discrimination, while Ghirardelli White Mocha has high negative discrimination. Intuitivelly, these products are appealing to different reviewers with very diverging preferences.\nIn the next section, I apply the ideal point model to a more traditional domain–the U.S. Congress–and also examine the role of including missing data via inflation.\n Empirical Example: 114th Senate As an empirical example of the model, I use data from the http://www.voteview.com website on the voting record of the 114th US Senate. This dataset comes with the idealstan package, and I then recode the data to correspond to a standard R matrix. However, because abstentions rarely happen in the US Congress, I do not include abstentions in this model and instead estimate a standard binary IRT 2-PL with inflation for missing data (absences). I then pass the matrix of vote records to the id_make function to create an object suitable for running the model.\nGiven a suitable object from id_make, the id_estimate function can estimate a variety of IRT ideal point models, including 2-PL, ordinal, hierarchical and dynamic IRT (using random-walk priors). For this run we set the model_type parameter to 2, which represents a 2-PL model with absence inflation. Because this dataset is of a significant size, I use variational inference instead of the full sampler to demonstrate its use. I do not, however, use the automatic identification option auto_id because it is relatively easy to constrain particular legislator ideal points for the Senate. In this case, I constrain Ben Sasse, Marco Rubio, Berni Sanders, Ted Cruz, Harry Reid and Elizabeth Warren, all senators with pronounced ideological views.\nAfter estimating the model, I can look at a graphical display of the ideal points with the id_plot function. The id_plot function produces a ggplot2 object which can be further modified. The legislators are given colors based on their party affiliation. The general shape of the ideal point distribution reflects the nature of polarization in the US Congress, with relatively few moderates near the center of the distribution.\n## ------------------------------------------------------------ ## EXPERIMENTAL ALGORITHM: ## This procedure has not been thoroughly tested and may be unstable ## or buggy. The interface is subject to change. ## ------------------------------------------------------------ ## ## ## ## Gradient evaluation took 0.026 seconds ## 1000 transitions using 10 leapfrog steps per transition would take 260 seconds. ## Adjust your expectations accordingly! ## ## ## Begin eta adaptation. ## Iteration: 1 / 250 [ 0%] (Adaptation) ## Iteration: 50 / 250 [ 20%] (Adaptation) ## Iteration: 100 / 250 [ 40%] (Adaptation) ## Iteration: 150 / 250 [ 60%] (Adaptation) ## Iteration: 200 / 250 [ 80%] (Adaptation) ## Success! Found best value [eta = 1] earlier than expected. ## ## Begin stochastic gradient ascent. ## iter ELBO delta_ELBO_mean delta_ELBO_med notes ## 100 -2e+004 1.000 1.000 ## 200 -2e+004 0.509 1.000 ## 300 -2e+004 0.341 0.018 ## 400 -2e+004 0.256 0.018 ## 500 -2e+004 0.205 0.003 MEDIAN ELBO CONVERGED ## ## Drawing a sample of size 1000 from the approximate posterior... ## COMPLETED. We can also plot the bill (item) midpoints as a line of indifference that we overlay on top of the ideal points.\nid_plot(sen_est,person_ci_alpha=.1,item_plot=205, group_labels=T, abs_and_reg=\u0026#39;Vote Points\u0026#39;) + scale_colour_brewer(type=\u0026#39;qual\u0026#39;) For this particular piece of legislation, the midpoint is right in the middle of the ideal point distribution, showing that the bill has very high discrimination. Also, the rug lines at the bottom of the plot, which show the HPD for the midpoint, indicate that the model is uncertain about the votes of only a handful of Senators on this bill.\nWe can similarly examine the absence midpoint for the same bill, which signifies the place on the ideal point spectrum at which a legislator is indifferent from showing up to vote. In this case, only very conservative Republicans chose not to show up to vote.\nWe can use the bill absence parameters to also see which bills showed the highest discrimination in terms of absences, or in other words, for which bills did the absence of legislators signify that they intentionally did not show up? To do this, I sort the discrimination parameters from the underlying Stan object and then merge them with the bill labels from voteview.org.\nFirst we can look at the top 10 bills with liberal/Democrat absence discrimination:\n {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],[\"To ensure that the storage and transportation of petroleum coke is regulated in a manner that ensures the protection of public and ecological health.\",\"To continue cleaning up fields and streams while protecting neighborhoods, generating affordable energy, and creating jobs.\",\"To express the sense of the Senate that climate change is real and not a hoax.\",\"To require the use of iron, steel, and manufactured goods produced in the United States in the construction of the Keystone XL Pipeline and facilities.\",\"To express the sense of Congress regarding federally protected land.\",\"To promote energy efficiency.\",\"To provide limits on the designation of new federally protected land.\",\"To express the sense of Congress regarding climate change.\",\"To conform citizen suits under the Endangered Species Act of 1973.\",\"To ensure that oil transported through the Keystone XL pipeline into the United States is used to reduce United States dependence on Middle Eastern oil.\"],[1.819605,1.688775,1.67506,1.66709,1.643,1.549565,1.52296,1.499735,1.483755,1.453105]],\"container\":\"\\n \\n \\n  \\n Bill Description\\n Absence Discrimination\\n \\n \\n\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\"}},\"evals\":[],\"jsHooks\":[]} Interestingly, Democrats appeared to be strategically absent most often on bills about climate change and the Keystone XL pipeline.\nFor conseratives the bills are as follows:\n {\"x\":{\"filter\":\"none\",\"data\":[[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"],[\"To limit the availability of amounts authorized to be appropriated for overseas contingency operations pending relief from the spending limits under the Budget Control Act of 2011.\",\"To strike the FOIA exemption.\",\"To improve the bill.\",\"To improve the requirements relating to removal of personal information from cyber threat indicators before sharing.\",\"To strengthen the Justice for Victims of Trafficking Act by incorporating additional bipartisan amendments.\",\"To protect information that is reasonably believed to be personal information or information that identifies a specific person.\",\"To improve the definitions of cybersecurity threat and cyber threat indicator.\",\"To exempt from the capability and process within the Department of Homeland Security communication between a private entity and the Federal Bureau of Investigation or the United States Secret Service regarding cybersecurity threats.\",\"An original bill to improve cybersecurity in the United States through enhanced sharing of information about cybersecurity threats, and for other purposes.\",\"To modify section105 to require DHS to review all cyber threat indicators and countermeasures in order to remove certain personal information.\"],[-3.956115,-3.639605,-3.63636,-3.57792,-3.516255,-3.42238,-3.15867,-3.12428,-3.075425,-2.992295]],\"container\":\"\\n \\n \\n  \\n Bill Description\\n Absence Discrimination\\n \\n \\n\",\"options\":{\"columnDefs\":[{\"className\":\"dt-right\",\"targets\":2},{\"orderable\":false,\"targets\":0}],\"order\":[],\"autoWidth\":false,\"orderClasses\":false},\"selection\":{\"mode\":\"multiple\",\"selected\":null,\"target\":\"row\"}},\"evals\":[],\"jsHooks\":[]} For Republicans, on the other hand, it appears that cybersecurity and inter-departmental information sharing were the laws in which they chose not to show up for ideological (or political) reasons. This could be due to concerns among the conservative base regarding government over-reach and collecting information.\n Conclusion idealstan is an effort to put state-of-the-art ideal point models along with the power of full Bayesian analysis in the hands of applied researchers in the social sciences. While the empirical examples presented were from the US Congress, the model can be used in any situation in which the assumptions of the ideal point model (unconstrained ideal point space) apply, i.e., situations in which the latent space is fundamentally polarizing between people. While similar in function and form to the edstan package, the idealstan package has to use more complicated forms of identification because of the difficulty of leaving discrimination parameters unconstrained.\nMoving forward, I intend to add in more functionality to smoothly operate with shinystan and bayesplot, as well as add further types of explanatory IRT models. The intention is to have a package on which applied researchers can run different ideal point models and then examine how different modeling choices affect the results. In addition, I will continue to build more visualization options so that the results are easily digestable and publishable.\n References ’ [1] J. M. Enelow and M. J. Hinich. The Spatial Theory of Voting: An Introduction. Cambridge University Press, 1984.\n[2] Y. Takane and J. de Leeuw. “On the Relationship Between Item Response Theory and Factor Analysis of Discretized Variables”. In: Psychometrika 52.3 (1986), pp. 393-408.\n[3] P. D. Hoff, A. E. Raftery and M. S. Hancock. “Latent Space Approaches to Social Network Analysis”. In: Journal of the American Statistical Association 97.460 (2002), pp. 1090-1098.\n[4] A. D. Martin and K. M. Quinn. “Dynamic Ideal Point Estimation via Markov Chain Monte Carlo for the U.S. Supreme Court, 1953-1999”. In: Political Analysis 10.2 (2002), pp. 134-153.\n[5] J. Clinton, S. Jackman and D. Rivers. “The Statistical Analysis of Rollcall Data”. In: American Political Science Review 98.2 (2004), pp. 355-370.\n[6] J. Bafumi, A. Gelman, D. K. Park, et al. “Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation”. In: Political Analysis 13.2 (2005), pp. 171-187.\n[7] K. T. Poole, J. B. Lewis, J. Lo, et al. Scaling Roll Call Votes with W-NOMINATE in R. Working Paper. Social Science Research Network, Oct. 06, 2008. URL: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082.\n[8] J. B. Slapin and S. Proksch. “A Scaling Model for Estimating Time-Series Party Positions from Texts”. In: American Journal of Political Science 52.3 (2008), pp. 705-722.\n[9] R. Carroll, J. B. Lewis, J. Lo, et al. “The Structure of Utility in Spatial Models of Voting”. In: American Journal of Political Science 57.4 (2013), pp. 1008-1028.\n[10] J. McAuley and J. Leskovec. “From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews”. In: WWW (2013).\n[11] D. A. Armstrong, R. Bakker, R. Carroll, et al. Analyzing Spatial Models of Choice and Judgment with R. CRC Press, 2014.\n[12] A. Bonica. “Mapping the Ideological Marketplace”. In: American Journal of Political Science 58.2 (2014), pp. 367-386.\n[13] P. BarberÃ¡. “Birds of the Same Feather Tweet Together: Bayesian Ideal Point Estimation Using Twitter Data”. In: Political Analysis 23 (2015), pp. 76-91.\n[14] A. Kucukelbir, R. Ranganath, A. Gelman, et al. “Automatic Variational Inference in Stan”. In: Advances in Neural Information Processing Systems 28. Ed. by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama and R. Garnett. Curran Associates, Inc., 2015, pp. 568-576. URL: http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf.\n[15] R. Kubinec. Absence Makes the Ideal Points Sharper. Poster. Political Methodology Society, 2017.\n ","date":1520985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520985600,"objectID":"b0827bff4add668f9c2cc711417ba99a","permalink":"http://www.robertkubinec.com/post/stancon2018_paper/","publishdate":"2018-03-14T00:00:00Z","relpermalink":"/post/stancon2018_paper/","section":"post","summary":"This is a paper that was presented at the StanCon2018 conference on Bayesian inference with the Stan Hamiltonian Markov Chain Monte Carlo (MCMC) method. Video of my talk is available here.","tags":null,"title":"idealstan: an R Package for Ideal Point Modeling with Stan","type":"post"},{"authors":null,"categories":null,"content":"","date":1515628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515628800,"objectID":"9b9c1bb519192ae16a1e169d17a6fbd5","permalink":"http://www.robertkubinec.com/talk/stancon2018/","publishdate":"2018-01-11T00:00:00Z","relpermalink":"/talk/stancon2018/","section":"talk","summary":"At StanCon 2018 I will present my new R package for ideal-point modeling with the [Stan Markov Chain Monte Carlo sampler for Bayesian inference](http://mc-stan.org/). This package allows for clustering of data around polarized axes to understand how and why people polarize themselves, whether in terms of political opinions or market products. I illustrate the package with applications drawn from the U.S. Senate and from Amazon product reviews of coffee products. I show how ideal point models can help us understand who the most polarized Senators and coffee products are, and also how the Bayesian version of the model adds powerful new features, including censoring for missing data.","tags":null,"title":"idealstan: A Package for Bayesian Ideal-Point Modeling with Stan","type":"talk"},{"authors":null,"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"63c87b2a9c118e082f0fc87ceba23310","permalink":"http://www.robertkubinec.com/talk/apsa_twitter/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/talk/apsa_twitter/","section":"talk","summary":"Scholars continue to disagree as to how far contentious politics diffuses within and across states and by what mechanisms it does so.  We use new data and empirical measures to test polarization during and after the Arab Uprisings of 2010-12.  After authoritarian governments began to fall, populations in several states began to polarize between secularists and Islamists over what kind of regime was to replace the ousted one.  We hypothesize that this Islamist-secularist polarization was triggered by catalytic events (such as Muslim Brotherhood electoral victory in Egypt) and diffused transnationally owing to social media and satellite television, dividing anti-status-quo actors throughout the region.  To examine polarization over time, we collect a comprehensive dataset on elite and citizen Twitter accounts across Arab countries after the Arab Spring. Using item-response models, we model polarization as the difference in latent ideological position between elites, and we show how polarization within countries changes over time in response to exogenous political shocks. By doing so we are the first to offer compelling statistical evidence of the endogenous process of polarization across competing ideological groups and states.","tags":null,"title":"Islamist-Secularist Polarization during the Arab Uprisings","type":"talk"},{"authors":null,"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"6a26a4437310ac2b39cd5e3f17c9b4d5","permalink":"http://www.robertkubinec.com/talk/apsa_tunisia/","publishdate":"2017-09-01T00:00:00Z","relpermalink":"/talk/apsa_tunisia/","section":"talk","summary":"Middle East political economy, particularly close relationships between dictators and business elites, has long been a pillar of authoritarian stability in the region. However, recent regime transitions in Egypt and Tunisia have forced business elites to react to a rapidly changing political system and prompting a re-examination of the political-economic foundations of these regimes. Using data from in-person interviews with business leaders and an online survey of businesses in Algeria, Egypt and Tunisia, I show that business collective action for regime stability depends on the existence of an institutional actor capable of punishing defection. Because of differing trajectories in the formation of political-economic coalitions, Tunisian businesspeople are far less able to act collectively then their neighbors, which may have helped save the country's democratic transition from incumbent takeover. In Algeria and Egypt, by contrast, military dominance of economic institutions has encouraged business collective action and cemented authoritarian coalitions.","tags":null,"title":"Making Democracy Safe for Business: Business Collective Action in Algeria, Egypt and Tunisia","type":"talk"}]