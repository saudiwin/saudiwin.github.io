<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>http://www.robertkubinec.com/</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018 Robert Kubinec</copyright>
    <lastBuildDate>Wed, 20 Feb 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>idealstan Vignette: Time-Varying Ideal Points</title>
      <link>http://www.robertkubinec.com/post/time_series/</link>
      <pubDate>Wed, 20 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/post/time_series/</guid>
      <description>


&lt;p&gt;&lt;em&gt;Note: To report bugs with the package, please file an issue on the &lt;a href=&#34;https://github.com/saudiwin/idealstan/issues&#34;&gt;Github page&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If you use this package, please cite the following:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kubinec, Robert. “Generalized Ideal Point Models for Time-Varying and Missing-Data Inference”. Working Paper.&lt;/p&gt;
&lt;p&gt;This package implements to kinds of time-varying ideal point models. Because these time-varying models are independent of the specific outcome used, time-varying ideal point models can be fit with any outcome/response supported by the package, including binary, ordinal, counts, continuous and positive-continuous data, in addition to the latent space model for binary data. This vignette demonstrates the use of the two time-varying ideal point models and how to decide between them with example data drawn from Delaware’s state legislature.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(&amp;#39;delaware&amp;#39;)
knitr::kable(slice(delaware,1:10))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;outcome&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;item_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;person_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;group_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;time_id&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Atkins, John 1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Atkins, John 2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Yes&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Banning 3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Barbieri, Michael A. 4&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Baumbach, Paul S. 5&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Bennett, Andria 6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Bennett, E. 7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Bentz, David 8&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Blakey, Donald 9&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;NA&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Bolden, Stephanie 10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The process to create a time-varying ideal point model is no different than that for creating a static model, except that a column should exist in the data with dates, preferably in &lt;code&gt;date&lt;/code&gt; or &lt;code&gt;date-time&lt;/code&gt; format. If you have a character vector of dates that you need to convert to R’s &lt;code&gt;date&lt;/code&gt; format, check out the excellent &lt;a href=&#34;https://lubridate.tidyverse.org/&#34;&gt;lubridate package&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are three time-varying models included in &lt;code&gt;idealstan&lt;/code&gt; package, each of which makes different assumptions about how ideal points change over time. It is important to note that none of these models is superior to the other. Ideal points do not have any natural time process as they are a latent, unobserved construct, so the question is more about which time process is most relevant to the social or physical process being studied. The models can be differentiated by whether they permit general description of time series versus inference on specific aspects, and also in terms of complexity.&lt;/p&gt;
&lt;p&gt;The first kind of model included in &lt;code&gt;idealstan&lt;/code&gt; is known as a random-walk process (also known as non-stationary time-series, Brownian motion and I(1)). This simple model of time implies that the location of an ideal point in the current time point is equal to the position of the ideal point in the prior time point plus some random noise. A helpful analogy is to imagine a frog hopping around a room. It could head in virtually any direction.&lt;/p&gt;
&lt;p&gt;The advantage of the random-walk model is that it allows ideal points to move in any direction. The downside is that it can assume too much change in the ideal point process. It also does not provide a great deal of information about the time series other than the variance parameter of the time series that indicate the average rate of change over time (i.e., how bouncy the time series is). Furthermore, random-walk models change significantly when other covariates are included in the model, as an additional covariate that has a constant effect over time will push the time-series in a single direction, making it less than ideal for testing the effect of time-varying covariates.&lt;/p&gt;
&lt;p&gt;Despite these limitations, this model is still useful, especially in two situations. First, when little is known about the time process/social situation, this model makes the most minimal assumptions about how ideal points change. Second, when the time series is of a relatively long time period, then the time series is likely to have some kind of random-walk nature, especially if there is no natural limit. For example, when looking at legislature voting data, ideal points may follow a random-walk pattern when looking at a legislator’s entire career over decades. In general, a random walk provides a good descriptive inference of where the ideal points are moving; it just won’t tell you a lot about &lt;em&gt;why&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The second model included in &lt;code&gt;idealstan&lt;/code&gt; is a stationary time series model (also called an AR(1) or first-order autoregressive time series). A stationary time-series is so called because it must return over time to a long-term average or mean. Change over time is conceived of as shocks that push the time series away from its long-term average. The AR(1) model includes additional parameters that measure how fast a time-series will return to its long-term average. A good empirical example for this model is economic growth over time. There are periods when “growth shocks” occur, such as recessions and boom times. Overall, though, economic growth for a specific country will tend towards some long-term average rate of growth. Economic growth can’t simply move off in any direction, especially in an upward direction, as that would over-heat the economy and result in massive inflation.&lt;/p&gt;
&lt;p&gt;The third model is known as a Gaussian process. Fully explaining how Gaussian processes work is beyond the scope of this vignette, but I refer readers to &lt;a href=&#34;https://betanalpha.github.io/assets/case_studies/gp_part1/part1.html&#34;&gt;this case study&lt;/a&gt; as a very helpful introduction. A Gaussian process is similar to a random walk in that it can in principle move in any direction. Unlike a random walk, the prior position of the time series doesn’t necessarily constrain the position of the time series in the present position. Rather, a Gaussian process is a generalized smoother: it will find a smooth path between the points, but can take any shape in principle.&lt;/p&gt;
&lt;p&gt;One major advantage of the Gaussian process is that it is a so-called continuous time series model. In practice this means that the time series does not have to be sequential. For example, if legislators only vote at irregular intervals, and the fact that some bills are separated than more time than others is important, then a Gaussian process will take into account the actual length of time between each vote. Random walks and stationary models, on the other hand, consider each time point to be sequential to the previous time point, effectively ignoring any big gaps.&lt;/p&gt;
&lt;p&gt;The main disadvantage of the Gaussian process is that the power and flexbility require &lt;em&gt;a lot&lt;/em&gt; more data. It is not a useful model unless you have considerable numbers of bills/items. The model can handle additional time-varying covariates although their meaning is not as precise as the stationary model.&lt;/p&gt;
&lt;p&gt;The stationary model, by contrast, assumes that ideal points have a single long-term average. The ideal points may receive “shocks” that force them away from the long-term mean, but they will inevitably return. While this is a more specific set of assumptions than the random walk or Gaussian process, stationary models have the significant advantage of allowing us to fit covariates that have a more meaningful interpretation: the estimates of covariates represent shocks to the ideal points away from their long-term average. We can even measure the time it takes for an ideal point process to return to its long-term average after experiencing the shock.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To show what these models look like, we will fit each model to the &lt;code&gt;delaware&lt;/code&gt; data in turn. We use the &lt;code&gt;vb&lt;/code&gt; option to produce variational estimates of the true posterior; these approximations are much faster to fit than the full model but usually have some distortions. For finished analysis we would want to use the full sampler (&lt;code&gt;use_vb=FALSE&lt;/code&gt;), unless we have so much data that processing time for the full model is infeasible.&lt;/p&gt;
&lt;div id=&#34;random-walk-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random-Walk Model&lt;/h2&gt;
&lt;p&gt;To fit the random walk model, we first create data in which we pass the name of the column of dates for each bill/item to the &lt;code&gt;time_id&lt;/code&gt; option of &lt;code&gt;id_make&lt;/code&gt;. One important thing to note about this model is that we code missing values as &lt;code&gt;&#39;Absent&#39;&lt;/code&gt;, but we leave &lt;code&gt;NA&lt;/code&gt; values in the outcome. These &lt;code&gt;NA&lt;/code&gt; values will be dropped before running the model. They represent periods of time when legislators were not in office, and hence it is reasonable to exclude these periods of time from analysis.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;delaware_data &amp;lt;- id_make(delaware,outcome = &amp;#39;outcome&amp;#39;,
                       person_id = &amp;#39;person_id&amp;#39;,
                       item_id = &amp;#39;item_id&amp;#39;,
                       group_id= &amp;#39;group_id&amp;#39;,
                       time_id=&amp;#39;time_id&amp;#39;,
                       miss_val=&amp;#39;Absent&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then pass this object to the &lt;code&gt;id_estimate&lt;/code&gt; function and specify &lt;code&gt;&#39;random_walk&#39;&lt;/code&gt; in the &lt;code&gt;vary_ideal_pts&lt;/code&gt; option. We also use &lt;code&gt;model_type=2&lt;/code&gt; to select a binary model (yes/no votes) that adjust for the missing data (legislator absences). We pass the names of two legislators to restrict their ideal points for identification. For the random walk model, only the first time points for these legislators will be fixed. If this method of identification did not work, we could also add a constraint to one of the time series by setting the &lt;code&gt;restrict_mean&lt;/code&gt; option to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;del_est &amp;lt;- id_estimate(delaware_data,
                model_type = 2,
                 use_vb = T,
                fixtype=&amp;#39;vb_partial&amp;#39;,
                vary_ideal_pts=&amp;#39;random_walk&amp;#39;,
                 restrict_ind_high = &amp;quot;Brady, David 13&amp;quot;,
                 restrict_ind_low=&amp;quot;Blakey, Donald 9&amp;quot;,
            seed=84520,
            refresh=0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;(First Step): Estimating model with variational inference to identify modes to constrain.&amp;quot;
## [1] &amp;quot;Estimating model with variational inference (approximation of true posterior).&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given the fitted model, we can now plot the ideal points. We will turn off the option for showing the uncertainty interval as there are a lot of lines, one for each legislator:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis_dyn(del_est,use_ci = F) +
  scale_color_manual(values=c(R=&amp;#39;red&amp;#39;,
                              D=&amp;#39;blue&amp;#39;,
                              X=&amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Time_Series_files/figure-html/plo_rw1-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot does now show very much that is particularly interesting. Most of the ideal points are not changing over time, except for some of the moderate Democrats that become slightly more conservative over time. This small amount of change is not surprising as the Senate has become highly polarized and people are not shifting their policy positions.&lt;/p&gt;
&lt;p&gt;However, we can also change the model’s parameters to induce more change over time. By default, &lt;code&gt;idealstan&lt;/code&gt; restricts the over-time change in ideal points to have an SD of no more than .1. Restricting the variance this low helps with identification, however, it also prevents the ideal points from changing too much, such as switching signs from one time point to the next. We can relax that parameter and see if we get slightly more variation by increasing the &lt;code&gt;restrict_var_high&lt;/code&gt; option have an SD of .5:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;del_est_rw2 &amp;lt;- id_estimate(delaware_data,
                model_type = 2,
                 use_vb = T,
                fixtype=&amp;#39;vb_partial&amp;#39;,
                restrict_var_high = .5,
                vary_ideal_pts=&amp;#39;random_walk&amp;#39;,
                 restrict_ind_high = &amp;quot;Brady, David 13&amp;quot;,
                 restrict_ind_low=&amp;quot;Blakey, Donald 9&amp;quot;,
            seed=84520,
            refresh=0)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;(First Step): Estimating model with variational inference to identify modes to constrain.&amp;quot;
## [1] &amp;quot;Estimating model with variational inference (approximation of true posterior).&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis_dyn(del_est_rw2,use_ci = F) + ggtitle(&amp;#39;Time-Varying Ideal Points of Delaware State Legislature&amp;#39;,subtitle=&amp;#39;Ideal Points Vary with Random-Walk Process&amp;#39;) +
  scale_color_manual(values=c(R=&amp;#39;red&amp;#39;,
                              D=&amp;#39;blue&amp;#39;,
                              X=&amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Time_Series_files/figure-html/more_rw_var-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now we can see a lot more movement, especially among Democrats, some of whom over the course of their careers moved from the liberal to conservative side of the ideal point spectrum.&lt;/p&gt;
&lt;p&gt;We can also look at the variance of the ideal points to see which of the legislators had the highest variance in their ideal points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis_var(del_est_rw2) + ggtitle(&amp;#39;Variances of Time-Varying Ideal Points in Delaware State Legislature&amp;#39;,subtitle=&amp;#39;Higher Variances Indicate Less Stable Ideal Points&amp;#39;) +
  scale_color_manual(values=c(R=&amp;#39;red&amp;#39;,
                              D=&amp;#39;blue&amp;#39;,
                              X=&amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Time_Series_files/figure-html/rw_var_est-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can access the actual estimates of the variances by passing the &lt;code&gt;return_data=TRUE&lt;/code&gt; option to the plot function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;out_d &amp;lt;- id_plot_legis_var(del_est_rw2,return_data = T)
knitr::kable(head(out_d$plot_data))&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;legis&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;low_pt&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;high_pt&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;median_pt&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;id_num&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;person_id&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;group_id&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;time_var_restrict[1]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0531250&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1604078&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0975428&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Adams, Thurman Jr. 113&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;time_var_restrict[10]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0038068&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1222249&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0235292&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Bennett, E. 7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;time_var_restrict[100]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0050196&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1941502&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0335895&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;100&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Plant, Hazel 77&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;time_var_restrict[101]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0077223&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1818735&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0379175&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;101&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Poore, Nicole 138&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;time_var_restrict[102]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0054159&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1411707&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0307767&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;102&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Postles, Charles S. 78&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;time_var_restrict[103]&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0065383&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.1652703&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0345820&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;103&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Potter, Charles Jr. 79&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;stationary-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stationary Model&lt;/h2&gt;
&lt;p&gt;We now fit a stationary version of the model by passing &lt;code&gt;&#39;AR1&#39;&lt;/code&gt; to &lt;code&gt;vary_ideal_pts&lt;/code&gt;. By default, this model does not put a hard upper limit on the over-time variance, but rather puts a tight prior on over-time variance that biases the variances to zero. We will put a hard upper limit as there are some legislators that move across the entire ideal point space, and we don’t want oscillation to occur–i.e, a legislator bouncing from one side of the ideal point spectrum to another every time point. While this behavior is entire possible within the model’s assumptions, it is unrealistic in the domain we are studying–legislative behavior. For that reason I put a hard &lt;code&gt;.05&lt;/code&gt; limit on the variance, however, even with this constraint, there is still some oscillation in ideal points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;del_est_ar1 &amp;lt;- id_estimate(delaware_data,
                model_type = 2,
                 use_vb = T,
                restrict_var_high = .05,
                restrict_var = T,
                fixtype=&amp;#39;vb_partial&amp;#39;,
                vary_ideal_pts=&amp;#39;AR1&amp;#39;,
                 restrict_ind_high = &amp;quot;Brady, David 13&amp;quot;,
                 restrict_ind_low=&amp;quot;Blakey, Donald 9&amp;quot;,
            seed=84520)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;(First Step): Estimating model with variational inference to identify modes to constrain.&amp;quot;
## [1] &amp;quot;Estimating model with variational inference (approximation of true posterior).&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis_dyn(del_est_ar1,use_ci = F) +
  scale_color_manual(values=c(R=&amp;#39;red&amp;#39;,
                              D=&amp;#39;blue&amp;#39;,
                              X=&amp;#39;green&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Time_Series_files/figure-html/ar1_1-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;While most of the ideal points are relatively stationary, a handful oscillate across the ideal point spectrum, likely a sign of limited information about the true location of their ideal points. We could further crank down the variance to prevent oscillation, but I left this plot in as an illustration of when model-fitting produces a model that doesn’t work for all legislators. This model could still work, though, if the sole purpose was to test the effect of covariates.&lt;/p&gt;
&lt;p&gt;Finally, we can also examine the individual ideal points by each time point using the summary function:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(del_est_ar1,pars=&amp;#39;ideal_pts&amp;#39;) %&amp;gt;% 
  head %&amp;gt;% 
  knitr::kable(.)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Person&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Group&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Time_Point&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Low Posterior Interval&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Posterior Median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;High Posterior Interval&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Parameter Name&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Adams, Thurman Jr. 113&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.5342575&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4423305&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.3537755&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;L_tp1[1,1]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Bennett, E. 7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.4980810&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.0447937&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3606878&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;L_tp1[1,10]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Plant, Hazel 77&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0851007&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3366560&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5914152&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;L_tp1[1,100]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Poore, Nicole 138&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.7450457&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.2859085&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.3501720&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;L_tp1[1,101]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Postles, Charles S. 78&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;R&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.3773894&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9764350&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.5565870&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;L_tp1[1,102]&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Potter, Charles Jr. 79&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;D&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1995-01-01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-0.7526390&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0269366&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8013608&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;L_tp1[1,103]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To examine trace plots of the actual MCMC sampling, we can use the &lt;code&gt;stan_plot&lt;/code&gt; function to look at posterior sampling for the first time point for Lamar Alexander based on the value shown in &lt;code&gt;Parameter Name&lt;/code&gt; in the table above:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;stan_trace(del_est_ar1,&amp;#39;L_tp1[1,1]&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Time_Series_files/figure-html/mcmc_stan-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;group-level-time-varying-ideal-points&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Group-level Time-varying Ideal Points&lt;/h2&gt;
&lt;p&gt;Finally, we can also change the model’s parameters to look at group-level, i.e. party-level, ideal points. To do so we need to specify the &lt;code&gt;use_groups=T&lt;/code&gt; option in the &lt;code&gt;id_estimate&lt;/code&gt; function, and we change the restricted parameters to parties:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;del_est_rw3 &amp;lt;- id_estimate(delaware_data,
                model_type = 2,
                 use_vb = T,
                time_sd=0.2,
                use_groups = T,
                fixtype=&amp;#39;vb_partial&amp;#39;,
                vary_ideal_pts=&amp;#39;random_walk&amp;#39;,
                 restrict_ind_high = &amp;quot;D&amp;quot;,
                 restrict_ind_low=&amp;quot;R&amp;quot;,
            seed=84520)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;(First Step): Estimating model with variational inference to identify modes to constrain.&amp;quot;
## [1] &amp;quot;Estimating model with variational inference (approximation of true posterior).&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis_dyn(del_est_rw3,
                  include=c(&amp;#39;D&amp;#39;,&amp;#39;R&amp;#39;)) + scale_colour_manual(values=c(R=&amp;#39;red&amp;#39;,
                                                          D=&amp;#39;blue&amp;#39;,
                                                          I=&amp;#39;green&amp;#39;),
                                                 name=&amp;quot;Parties&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Time_Series_files/figure-html/stationary_groups-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also overlay a bill/item midpoint to see where the line of indifference in voting is relative to party positions. In a dynamic ideal point model, the bill/item midpoint will be a straight line as the bill-item midpoint was only voted on in one time point, and hence has only one parameter:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis_dyn(del_est_rw3,item_plot=&amp;#39;342&amp;#39;,
                  text_size_label = 5,
                  include=c(&amp;#39;D&amp;#39;,&amp;#39;R&amp;#39;)) + scale_colour_manual(values=c(R=&amp;#39;red&amp;#39;,
                                                          D=&amp;#39;blue&amp;#39;,
                                                          I=&amp;#39;green&amp;#39;),
                                                 name=&amp;quot;Parties&amp;quot;) +
  ggtitle(&amp;#39;Time-Varying Party-level Ideal Points for the Delaware State Legislature&amp;#39;,
          subtitle = &amp;#39;Midpoint (Line of Indifference to Voting) for 342nd Roll-call Vote as Dotted Line&amp;#39;) +
  guides(color=&amp;#39;none&amp;#39;) +
  annotate(geom=&amp;#39;text&amp;#39;,
           x = ymd(&amp;#39;2016-01-01&amp;#39;),
           y=-1,
           label=&amp;#39;Confirmation Vote for Wilhelmina Wright as U.S. District Judge&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Time_Series_files/figure-html/party_mid-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As this plot shows, the line of indifference is in a no-person’s zone in the middle of the plot, signifying the lack of overlap and consensus on legislation in the current Senate.&lt;/p&gt;
&lt;p&gt;I will now estimate additional AR(1) and a Gaussian process model to use as comparison points to the random-walk model for parties:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;del_est_ar2 &amp;lt;- id_estimate(delaware_data,
                model_type = 2,
                 use_vb = T,
                time_sd=0.2,
                use_groups = T,
                fixtype=&amp;#39;vb_partial&amp;#39;,
                vary_ideal_pts=&amp;#39;AR1&amp;#39;,
                 restrict_ind_high = &amp;quot;D&amp;quot;,
                 restrict_ind_low=&amp;quot;R&amp;quot;,
            seed=84520)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;(First Step): Estimating model with variational inference to identify modes to constrain.&amp;quot;
## [1] &amp;quot;Estimating model with variational inference (approximation of true posterior).&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;del_est_gp1 &amp;lt;- id_estimate(delaware_data,
                           model_type = 2,
                           use_vb = T,
                           tol_rel_obj = .0005,
                           use_groups = T,
                           gp_sd_par = 0.025,
                           gp_m_sd_par =c(0.3,10),
                           gp_num_diff=c(3,.01),
                           gp_min_length = 0,
                           fixtype=&amp;#39;vb_partial&amp;#39;,
                           vary_ideal_pts=&amp;#39;GP&amp;#39;,
                           restrict_ind_high = &amp;quot;D&amp;quot;,
                           restrict_ind_low=&amp;quot;R&amp;quot;,
                            seed=84520)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;(First Step): Estimating model with variational inference to identify modes to constrain.&amp;quot;
## [1] &amp;quot;Estimating model with variational inference (approximation of true posterior).&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can the use an option in the &lt;code&gt;id_plot_legis_dyn&lt;/code&gt; function to pass in all three models as a list to see how they compare to each other over the same groups:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis_dyn(list(RW=del_est_rw3,
                       AR1=del_est_ar2,
                       GP=del_est_gp1),
                  include=c(&amp;#39;D&amp;#39;,&amp;#39;R&amp;#39;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Time_Series_files/figure-html/compare_mods-1.png&#34; width=&#34;576&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows that are substantial divergences between the different models. Broadly speaking, the random-walk and GP models are close to each other, as we might expect given that both models impose relatively little structure on the time series. The AR model, on the other hand, constrains change over time, and it is not surprising that it shows the Democrats and Republicans much closer to each other. The advantage of the AR1 model, of course, is that it can estimate the effect of covariates on the long-term mean of the series, a very precise interpretation.&lt;/p&gt;
&lt;p&gt;The GP model has several parameters that control the ability of the time series to wiggle or adjust to over-time change in the ideal points. These parameters are given defaults that restrict movement in the ideal points to help ensure identification. As such, they tend to be fairly conservative, and can be adjusted to see if the ideal points can handle greater variation.&lt;/p&gt;
&lt;p&gt;The advantage of the GP model is that it combines the relatively minimalistic structure of the random walk with the ability to fit covariates on the time series. I refer the reader to the covariate vignette for more information about how to fit covariate models using &lt;code&gt;idealstan&lt;/code&gt;. It is important to note, however, that the GP interpretation of a covariate is different than the AR1 covariate. In an AR1 model, the value of the covariate represents the shift in the long-term mean of the series, whereas in a GP model the covariate is a non-parametric effect of the covariate on the time series averaged over the entire series. These parameters are explained in the table below:&lt;/p&gt;
&lt;table&gt;
&lt;caption&gt;Parameters in the Gaussian Process Time-Series Model&lt;/caption&gt;
&lt;colgroup&gt;
&lt;col width=&#34;18%&#34; /&gt;
&lt;col width=&#34;81%&#34; /&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;center&#34;&gt;Parameter&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;code&gt;gp_sd_par&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;This parameter represents residual variation in the series that the GP does not
account for. As such, its default is a very low value, as increasing it will
generally increase oscillation in the series.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;code&gt;gp_num_diff&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;This parameter is a multiplier that is used to set the prior for the length-scale
of the GP. Loosely speaking, the length-scale determines the number of times
that the time-series can cross zero, and so lowering this parameter will
decrease the length-scale and subsequently increase the number of times the series
can cross zero. The length-scale is given a prior equal to the difference
between the maximum and minimum length of the
time series in whatever units it is recorded in (days, weeks, etc) times the
parameter &lt;code&gt;gp_num_diff&lt;/code&gt;. The second numeric value of this parameter represents
the standard deviation of the log-normal prior of the length scale. Increasing the
standard deviation will put more weight on the data in determining the amount of
flexibility in the time series.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;center&#34;&gt;&lt;code&gt;gp_m_sd_par&lt;/code&gt;&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;This parameter has two values that set the GP’s marginal standard deviation.
This parameter loosely represents the amount of time-series variation in the
series. The first numeric value represents the hard upper limit for this
parameter to prevent the series oscillating. The second numeric value is equal
to the shape of an inverse-gamma prior defined over the interval between
0 and the first numeric value (the hard upper limit). It is a weakly
informative prior that pulls values away from zero to prevent divergences.
Increasing the first numeric value (the upper limit) will increase marginal
standard deviation, while the second numeric value can increase marginal
standard deviation by decreasing its value, resulting in a flatter
inverse-gamma prior.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;This is known as calculating the impulse response function. While these can be readily calculated from the raw model output of an &lt;code&gt;idealstan&lt;/code&gt; object, they are not currently implemented as functions in the package.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Conjoint Survey Experiments</title>
      <link>http://www.robertkubinec.com/post/conjoint_power_simulation/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/post/conjoint_power_simulation/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Conjoint survey experiments have become more popular in political science since the publication of &lt;a href=&#34;http://hdl.handle.net/1721.1/84064&#34;&gt;Hainmueller, Hopkins and Yamamoto (2014)&lt;/a&gt;. However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see &lt;a href=&#34;https://osf.io/preprints/socarxiv/mrfcu/&#34;&gt;SocArchiv Draft&lt;/a&gt;). I employ both traditional power measures and newer statistics from &lt;a href=&#34;http://journals.sagepub.com/doi/abs/10.1177/1745691614551642&#34;&gt;Gelman and Carlin (2014)&lt;/a&gt; reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).&lt;/p&gt;
&lt;p&gt;The original Rmarkdown and saved simulation files can be downloaded from the &lt;a href=&#34;https://github.com/saudiwin/saudiwin.github.io/tree/sources/content&#34;&gt;site’s Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The packages required to run this simulation are listed in the code block below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Required packages
require(ggplot2)
require(dplyr)
require(tidyr)
require(multiwayvcov)
require(lmtest)
require(stringr)
require(kableExtra)

# package MASS also used but not loaded

# != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows.
# to install parallelsugar:
# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_github(&amp;#39;nathanvan/parallelsugar&amp;#39;)

# If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used

if(.Platform$OS.type==&amp;#39;windows&amp;#39;) {
  parallelfunc &amp;lt;- parallelsugar::mclapply_socket
} else {
  parallelfunc &amp;lt;- parallel::mclapply
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation Set-up&lt;/h2&gt;
&lt;p&gt;The following parameters control the range of coefficients tested and the number of simulations. The survey experiment design employs vignettes in which appeals and the actors making appeals are allowed to vary between respondents. Any one vignette has one actor and one appeal. The probability of assignment is assumed to be a simple random fraction of the number of appeal-actor combinations (14). If &lt;code&gt;run_sim&lt;/code&gt; is set to &lt;code&gt;TRUE&lt;/code&gt;, the simulation is run, otherwise the simulation results are loaded from an RDS file and plotted. Running the simulation will take approximately 6 to 12 hours depending on the number of cores and speed of the CPU.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Actually run the simulation or just load the data and look at it?
run_sim &amp;lt;- FALSE
# Max number of respondents fixed at 2700
num_resp &amp;lt;- 2700
# Number of iterations (breaks in sample size)
num_breaks &amp;lt;- 300
# Number of simulations to run per iteration
n_sims &amp;lt;- 1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then create a grid of all possible actor-appeal combinations as I am using simple randomization of profiles before presenting them to respondents. There are two vectors of treatments (actors and appeals) that each have 7 separate treatments for a total of 14 separate possible treatments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Two treatment variables producing a cross-product of 7x7
treatments1 &amp;lt;- c(&amp;#39;military&amp;#39;,&amp;#39;MOI&amp;#39;,&amp;#39;president&amp;#39;,&amp;#39;MOJ&amp;#39;,&amp;#39;parliament&amp;#39;,&amp;#39;municipality&amp;#39;,&amp;#39;government&amp;#39;)
treatments2 &amp;lt;- c(&amp;#39;exprop.firm&amp;#39;,&amp;#39;exprop.income&amp;#39;,&amp;#39;permit.reg&amp;#39;,&amp;#39;contracts.supply&amp;#39;,&amp;#39;permit.export&amp;#39;,&amp;#39;permit.import&amp;#39;,&amp;#39;reforms&amp;#39;)
total_treat &amp;lt;- length(c(treatments1,treatments2))
grid_pair &amp;lt;- as.matrix(expand.grid(treatments1,treatments2))
print(head(grid_pair))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Var1           Var2         
## [1,] &amp;quot;military&amp;quot;     &amp;quot;exprop.firm&amp;quot;
## [2,] &amp;quot;MOI&amp;quot;          &amp;quot;exprop.firm&amp;quot;
## [3,] &amp;quot;president&amp;quot;    &amp;quot;exprop.firm&amp;quot;
## [4,] &amp;quot;MOJ&amp;quot;          &amp;quot;exprop.firm&amp;quot;
## [5,] &amp;quot;parliament&amp;quot;   &amp;quot;exprop.firm&amp;quot;
## [6,] &amp;quot;municipality&amp;quot; &amp;quot;exprop.firm&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;To simulate the data, I first sample 14 coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; (one for each treatment &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;) from a normal distribution with mean zero and standard deviation one. I then randomly sample from two profile combinations for each of the &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; respondents in accordance with simple random sampling. Two profile combinations, for a total of four tasks &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, are selected to reflect the fact that paired vignettes will be shown to each respondent as in the study design. I also sample a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Z_I\)&lt;/span&gt; that is a random binomial vector with probability of 0.2 (thus 20% of respondents will fall into this cell). A treatment interaction effect &lt;span class=&#34;math inline&#34;&gt;\(\beta_z\)&lt;/span&gt; is sampled from a normal distribution with mean 0.5 and standard deviation of 0.3 to provide a sampling distribution for the true effect, instead of assuming that the true effect is a fixed population value. Adding a distribution for &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; reflects additional uncertainty beyond standard sampling distribution uncertainty. In this case, it represents additional measurement error between the true concept and the indicators used in the survey design.&lt;/p&gt;
&lt;p&gt;I also post-stratify some estimates with a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_I\)&lt;/span&gt; from a binomial distribution of probability .5 that has a constant effect on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(+1\)&lt;/span&gt; (representing a fixed effect).&lt;/p&gt;
&lt;p&gt;I then randomly sample a pair of outcomes, for a total of four tasks &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; in the range of &lt;span class=&#34;math inline&#34;&gt;\([1,10]\)&lt;/span&gt; by drawing a number from a multivariate normal distribution. The mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_{it}\)&lt;/span&gt; of this normal distribution is equal to a linear model with an intercept of 5, the 14 dummy variables for treatment indicators &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; with associated coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt;, the interaction &lt;span class=&#34;math inline&#34;&gt;\(\beta_z\)&lt;/span&gt; between the pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{ij}\)&lt;/span&gt;, and a post-stratification covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_i\)&lt;/span&gt;. To simplify matters, &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; is not given its own constituent term as I am not interested in the unconditional effect of &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt;, only the effect of &lt;span class=&#34;math inline&#34;&gt;\(X_{ijt}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Z_{i}\)&lt;/span&gt;. Finally, I draw correlated errors from a multivariate normal distribution with mean of zero and length of 4 (equal to the number of tasks per respondent) to produce a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; variance matrix &lt;span class=&#34;math inline&#34;&gt;\(\varSigma_i\)&lt;/span&gt; with a diagonal of 4 and intra-respondent covariation of 1 (correlation of 0.5).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X_{ITJ} &amp;amp;\sim  \mathrm{B} \Big( \frac{1}{J \times 2} \Big)\\
B_{J} &amp;amp;\sim  \mathrm{N}(0,2)\\
\beta_z &amp;amp;\sim \mathrm{N}(0.5,0.3)\\
Z_I &amp;amp;\sim  \mathrm{B}(0.2)\\
Q_I &amp;amp;\sim  \mathrm{B}(0.5)\\
\mu_{it} &amp;amp;=  5 + \sum_{j=1}^{J} \sum_{t=1}^{T} \beta_j * X_{itj} + \beta_z * X_{it1} *Z_i + Q_i\\
Y_{it} &amp;amp;\sim  \mathrm{N}(\mu_{it},\varSigma_i)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This process will produce some numbers outside the &lt;span class=&#34;math inline&#34;&gt;\([1,10]\)&lt;/span&gt; range; however, it is better to leave these values in as explicit truncation will violate the assumptions of the underlying causal model.&lt;/p&gt;
&lt;p&gt;I run 1000 simulations for each of 300 sequential sample sizes ranging from 100 to 2700. I then take the mean significant effect and report that as the likely significant effect size for that sample size. I also record the ratio of draws for which the effect is significant (the power). However, given that the true effect is not fixed, I interpret power as the ability detect a true effect greater than zero. I record both unadjusted p-values and p-values adjusted using the &lt;code&gt;cluster.vcov&lt;/code&gt; function from the multiwayvcov package by clustering around respondent ID to reflect the pairing of vignettes. I also use separate results when post-stratifying on a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In addition, I included M-errors (error of absolute magnitude of significant coefficients) and S-errors (incorrect sign of significant coefficients). M-errors provide an estimate of publication bias given that the &lt;span class=&#34;math inline&#34;&gt;\(p=0.05\)&lt;/span&gt; threshold is a hard boundary and will necessarily result in smaller effects being reported as statistically insignificant when in fact they are greater than zero. S-errors help determine the probability that an estimated effect is the correct sign even if it is significant. S-errors are particularly problematic in small samples when sampling error can produce large negative deviations that may be statistically significant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(run_sim==TRUE) {
  file.create(&amp;#39;output_log.txt&amp;#39;,showWarnings = FALSE)
    
    # Need to randomize over the simulations so that parallelization works correctly on windows
    
    sampled_seq &amp;lt;- sample(seq(100,num_resp,length.out = num_breaks))
    
  all_sims &amp;lt;- parallelfunc(sampled_seq,function(x) {
  
    out_probs &amp;lt;- 1:n_sims
    cat(paste0(&amp;quot;Now running simulation on data sample size &amp;quot;,x),file=&amp;#39;output_log.txt&amp;#39;,sep=&amp;#39;\n&amp;#39;,append=TRUE)
  
    out_data &amp;lt;- lapply(1:n_sims, function(j) {
      
      total_probs &amp;lt;- sapply(1:x,function(x) {
        treat_rows &amp;lt;- sample(1:nrow(grid_pair),4)
        treatments_indiv &amp;lt;- c(grid_pair[treat_rows,])
        return(treatments_indiv)
      })
      
      by_resp &amp;lt;- t(total_probs)
      by_resp &amp;lt;- as_data_frame(by_resp)
      names(by_resp) &amp;lt;- c(paste0(&amp;#39;actor.&amp;#39;,1:4,&amp;quot;_cluster&amp;quot;,c(1,1,2,2)),paste0(&amp;#39;gift.&amp;#39;,1:4,&amp;quot;_cluster&amp;quot;,c(1,1,2,2)))
      by_resp$respondent &amp;lt;- paste0(&amp;#39;Respondent_&amp;#39;,1:nrow(by_resp))
      by_resp &amp;lt;- gather(by_resp,attribute,indicator,-respondent) %&amp;gt;% separate(attribute,into=c(&amp;#39;attribute&amp;#39;,&amp;#39;cluster&amp;#39;),sep=&amp;#39;_&amp;#39;) %&amp;gt;% 
        separate(attribute,into=c(&amp;#39;attribute&amp;#39;,&amp;#39;task&amp;#39;)) %&amp;gt;% spread(attribute,indicator)
      
      
      # Assign true coefficients for treatments
  
      #Beta_js
      
      coefs &amp;lt;- data_frame(coef_val=rnorm(n=length(c(treatments1,treatments2)),mean=0,sd=1),
                          treat_label=c(treatments1,treatments2))
      
      # Create cluster covariance in the errors
      
      sigma_matrix &amp;lt;- matrix(2,nrow=4,ncol=4)
      diag(sigma_matrix) &amp;lt;- 4
  
      # Add on the outcome as a normal draw, treatment coefficients, interaction coefficient, group errors/interaction by respondent
      
      by_resp &amp;lt;- gather(by_resp,treatment,appeal_type,actor,gift) %&amp;gt;% 
        left_join(coefs,by=c(&amp;#39;appeal_type&amp;#39;=&amp;#39;treat_label&amp;#39;))
      
      # Record interaction coefficient (true estimate of interest)
      
      true_effect &amp;lt;- rnorm(n=1,mean=0.5,sd=0.3)
      
      by_resp &amp;lt;- select(by_resp,-treatment) %&amp;gt;% spread(appeal_type,coef_val) %&amp;gt;%  group_by(respondent) %&amp;gt;% mutate(error=MASS::mvrnorm(1,mu=rep(0,4),Sigma=sigma_matrix)) %&amp;gt;% ungroup
      
      # interaction coefficient only in function if military==TRUE
      
      by_resp &amp;lt;- mutate(by_resp,int_coef=true_effect*rbinom(n = n(),prob = 0.2,size=1),
                        int_coef=if_else(military!=0,int_coef,0))
      by_resp &amp;lt;- lapply(by_resp, function(x) {
        if(is.double(x)) {
          x[is.na(x)] &amp;lt;- 0
        }
        return(x)
      }) %&amp;gt;% as_data_frame
      
      # To make the outcome, need to turn the dataset long
      # However, we now need to drop the reference categories
      # Drop one dummy from actor/gift to prevent multicollinearity = reforms + government combination
      
      out_var &amp;lt;- gather(by_resp,var_name,var_value,-respondent,-task,-cluster) %&amp;gt;% 
         filter(!(var_name %in% c(&amp;#39;reforms&amp;#39;,&amp;#39;government&amp;#39;))) %&amp;gt;% 
        group_by(respondent,task) %&amp;gt;% summarize(outcome=sum(var_value)+5)
      
      combined_data &amp;lt;- left_join(out_var,by_resp,by=c(&amp;#39;respondent&amp;#39;,&amp;#39;task&amp;#39;))
      
      
      # Re-estimate with a blocking variable
      
  
      combined_data$Q &amp;lt;- c(rep(1,floor(nrow(combined_data)/2)),
                           rep(0,ceiling(nrow(combined_data)/2)))
      
      combined_data$outcome &amp;lt;- if_else(combined_data$Q==1,combined_data$outcome+1,
                                       combined_data$outcome)
      
      # # Create data predictor matrix and estimate coefficients from the simulated dataset
      # 
      to_lm &amp;lt;- ungroup(combined_data) %&amp;gt;% select(contracts.supply:reforms,int_coef,Q)
      to_lm &amp;lt;- mutate_all(to_lm,funs(if_else(.!=0,1,.))) %&amp;gt;% mutate(outcome=combined_data$outcome)
      
      #No post-stratification
      # I don&amp;#39;t estimate a constituent term for int_coef because it is assumed to be zero
      
      results &amp;lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +
                      parliament + permit.export + permit.import + permit.reg + president + 
                      int_coef:military,data=to_lm)
      
      results_clust &amp;lt;- cluster.vcov(results,cluster = combined_data$respondent)
      pvals_adj &amp;lt;- coeftest(results,vcov.=results_clust)[-1,4]&amp;lt;0.05
      pvals_orig &amp;lt;- coeftest(results)[-1,4]&amp;lt;0.05
      
      total_sig_orig &amp;lt;- mean(pvals_orig)
      total_sig_adj &amp;lt;- mean(pvals_adj)
      
      int_sig_orig &amp;lt;- pvals_orig[&amp;#39;military:int_coef&amp;#39;]
      int_sig_adj &amp;lt;- pvals_adj[&amp;#39;military:int_coef&amp;#39;]
      
      
      # Now run the poststratification model
      
      results_ps &amp;lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +
                      parliament + permit.export + permit.import + permit.reg + president + 
                      int_coef:military + Q,data=to_lm)
      
      results_clust &amp;lt;- cluster.vcov(results,cluster = combined_data$respondent)
      pvals_adj &amp;lt;- coeftest(results_ps,vcov.=results_clust)[-1,4]&amp;lt;0.05
      pvals_orig &amp;lt;- coeftest(results_ps)[-1,4]&amp;lt;0.05
      
      total_sig_orig_blocker &amp;lt;- mean(pvals_orig)
      total_sig_adj_blocker &amp;lt;- mean(pvals_adj)
      
      int_sig_orig_blocker &amp;lt;- pvals_orig[&amp;#39;military:int_coef&amp;#39;]
      int_sig_adj_blocker &amp;lt;- pvals_adj[&amp;#39;military:int_coef&amp;#39;]
      
      out_results &amp;lt;- data_frame(int_sig_adj,int_sig_orig,int_sig_adj_blocker,int_sig_orig_blocker,
                                total_sig_adj,total_sig_orig,total_sig_adj_blocker,
                                total_sig_orig_blocker,abs_true_effect=abs(true_effect),
                                true_effect=true_effect,
                                est_effect=coef(results)[&amp;#39;military:int_coef&amp;#39;],
                                est_effect_ps=coef(results)[&amp;#39;military:int_coef&amp;#39;])
    })
    out_data &amp;lt;- bind_rows(out_data)
    
    return(out_data)
  },mc.cores=parallel::detectCores(),mc.preschedule=FALSE)
  #save the data for inspection
  
  all_sims_data &amp;lt;- bind_rows(all_sims) %&amp;gt;% mutate(sample_size=rep(sampled_seq,each=n_sims),
                                                  iter=rep(1:n_sims,times=num_breaks))

}

if(run_sim==TRUE) {
saveRDS(object = all_sims_data,file=&amp;#39;all_sims_data.rds&amp;#39;)
} else {
  all_sims_data &amp;lt;- readRDS(&amp;#39;all_sims_data.rds&amp;#39;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simulation yields a row with the significant effect of the interaction term for that simulation for a total of &lt;code&gt;n_sims&lt;/code&gt; draws. From this raw data I am able to calculate all of the necessary statistics mentioned above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add in different calculations

all_sims_data &amp;lt;- group_by(all_sims_data,sample_size)  %&amp;gt;% mutate(sigeffVorig=ifelse(int_sig_orig,
                                                                                     est_effect,
                                                                                     NA),
sigeffVadj=ifelse(int_sig_adj,est_effect,NA),
sigeffVps_orig=ifelse(int_sig_orig_blocker,est_effect_ps,NA),
sigeffVps_adj=ifelse(int_sig_adj_blocker,est_effect_ps,NA),
powerVorig=int_sig_orig &amp;amp; (true_effect&amp;gt;0),
powerVadj=int_sig_adj &amp;amp; (true_effect&amp;gt;0),
powerVps_orig=int_sig_orig_blocker &amp;amp; (true_effect &amp;gt; 0),
powerVps_adj=int_sig_adj_blocker &amp;amp; (true_effect &amp;gt; 0),
SerrVorig=ifelse(int_sig_orig,1-(sign(est_effect)==sign(true_effect)),NA),
SerrVadj=ifelse(int_sig_adj,1-(sign(est_effect)==sign(true_effect)),NA),
SerrVps_orig=ifelse(int_sig_orig_blocker,
                    1-(sign(est_effect_ps)==sign(true_effect)),NA),
SerrVps_adj=ifelse(int_sig_adj_blocker,
                   1-(sign(est_effect_ps)==sign(true_effect)),NA),
MerrVorig=ifelse(int_sig_orig,abs(est_effect)/abs_true_effect,NA),
MerrVadj=ifelse(int_sig_adj,abs(est_effect)/abs_true_effect,NA),
MerrVps_orig=ifelse(int_sig_orig_blocker,abs(est_effect_ps)/abs_true_effect,NA),
MerrVps_adj=ifelse(int_sig_adj_blocker,abs(est_effect_ps)/abs_true_effect,NA))

long_data &amp;lt;- select(all_sims_data,matches(&amp;#39;V|sample|iter&amp;#39;)) %&amp;gt;% gather(effect_type,result,-sample_size,-iter) %&amp;gt;% separate(effect_type,into=c(&amp;#39;estimate&amp;#39;,&amp;#39;estimation&amp;#39;),sep=&amp;#39;V&amp;#39;) %&amp;gt;% 
  mutate(estimate=factor(estimate,levels=c(&amp;#39;sigeff&amp;#39;,&amp;#39;power&amp;#39;,&amp;#39;Serr&amp;#39;,&amp;#39;Merr&amp;#39;),
                         labels=c(&amp;#39;Mean\nSignificant\nEffect&amp;#39;,
                                  &amp;#39;Mean\nPower&amp;#39;,
                                  &amp;#39;S-Error\nRate&amp;#39;,
                                  &amp;#39;M-Error\nRate&amp;#39;)),
         estimation=factor(estimation,levels=c(&amp;#39;adj&amp;#39;,&amp;#39;orig&amp;#39;,&amp;#39;ps_adj&amp;#39;,&amp;#39;ps_orig&amp;#39;),
                           labels=c(&amp;#39;No Post-Stratification\nClustered Errors\n&amp;#39;,
                                    &amp;#39;No Post-Stratification\nUn-clustered Errors\n&amp;#39;,
                                    &amp;#39;Post-Stratification\nClustered Errors\n&amp;#39;,
                                    &amp;#39;Post-Stratification\nUn-clustered Errors\n&amp;#39;)))

long_data_treatment &amp;lt;- select(all_sims_data,matches(&amp;#39;total|iter|sample&amp;#39;)) %&amp;gt;% gather(effect_type,result,-sample_size,-iter) %&amp;gt;%
mutate(effect_type=factor(effect_type,levels=c(&amp;#39;total_sig_adj&amp;#39;,
                                               &amp;#39;total_sig_orig&amp;#39;,
                                               &amp;#39;total_sig_adj_blocker&amp;#39;,
                                               &amp;#39;total_sig_orig_blocker&amp;#39;),
                          labels=c(&amp;#39;No Post-Stratification\nClustered Errors\n&amp;#39;,
                                   &amp;#39;No Post-Stratification\nUn-clustered Errors\n&amp;#39;,
                                   &amp;#39;Post-Stratification\nClustered Errors\n&amp;#39;,
                                   &amp;#39;Post-Stratification\nUn-clustered Errors\n&amp;#39;)))



# Plot a sample of the data (too big to display all of it)

long_data %&amp;gt;% ungroup %&amp;gt;% 
  slice(1:10) %&amp;gt;% 
  select(-estimation) %&amp;gt;% 
  mutate(estimate=str_replace(estimate,&amp;quot;\\n&amp;quot;,&amp;quot; &amp;quot;)) %&amp;gt;% 
  knitr::kable(.) %&amp;gt;% 
  kable_styling(font_size = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 8px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sample_size
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
iter
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
estimate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
result
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6298429
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3874088
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1438379
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5653086
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2689594
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting&lt;/h2&gt;
&lt;p&gt;I use the &lt;code&gt;gam&lt;/code&gt; function in the ggplot2 package to plot a smoothed regression line of the simulation draws for each sample size.&lt;/p&gt;
&lt;p&gt;First we can look at the difference that clustered errors makes across the different statistics. The only noticeable differences are at sample sizes smaller than 500. Clustering on respondents tends to result in smaller average significant effects, but it also results in increases in sign errors. This finding differs from the literature that considers clustering important to control for intra-respondent correlation, which in this simulation was fixed at 0.5. At sample sizes larger than 500, there does not appear to be any difference between clustered and un-clustered estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,grepl(&amp;#39;No Post&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;clust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I look at post-stratification as an option to improve the precision of estimates. For unclustered errors reported below, post-stratified estimates do have higher power and slightly lower average significant effects, and importantly, the post-stratified estimates worsen neither type S nor type M errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,grepl(&amp;#39;Un-clustered&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;post_unclust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Post-stratification appears to have a similar effect on clustered error estimations, although the differences are smaller. In smaller samples, post-stratified estimates do have smaller M-errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,!grepl(&amp;#39;Un-clustered&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Conjoint_Power_Simulation_files/figure-html/unclustered_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;post_clust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I also report average numbers of significant coefficients for the 14 treatments. Given that the 14 treatments were sampled from a normal distribution with prior density in the positive values with a meaan of 0.5, in expectation 95% of estimates should be statisticall significant. While that upper limit is reached only in high sample numbers, it looks like the ratio for treatment effects reaches an acceptable level of 70 percent at about 500 sample respondents. Also, post-stratifying un-clustered models results in effects that are reported as significant at much higher rates, as would follow from the previous results about post-stratification.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
ggplot(long_data_treatment,aes(y=result,x=sample_size,linetype=effect_type,colour=effect_type)) +
  theme_minimal() + stat_smooth() +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Dark2&amp;#39;) + guides(linetype=g_title,colour=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;all_treat_rate.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This simulation study shows that a sample size of approximately 1,000 respondents is enough to obtain high power while also lowering both the S and M-error rates for treatment interaction effects in this conjoint experiment. The treatment effects themselves are generally of high quality once the sample size reaches 500 because the total number of respondents in each treatment cell is considerably higher than in an interaction. Post-stratification appears to be a useful strategy to increase precision without inducing S or M errors; at the very least, post-stratification does not appear to have any adverse effects on the estimation.&lt;/p&gt;
&lt;p&gt;On the other hand, it appears that clustering errors increases the S-error rate at small sample sizes, a surprising finding considering that clustering methods are designed to inflate, not deflate, standard errors. Given that the S-error rate reveals the likelihood of making an error about the sign of the treatment effect, this is a potentially serious problem. For that reason I intend to report both clustered and un-clustered estimates in my analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Fragmentation of Autocratic Successor Parties: The Case of Tunisia</title>
      <link>http://www.robertkubinec.com/talk/apsa_2018_tunisia/</link>
      <pubDate>Fri, 31 Aug 2018 16:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/talk/apsa_2018_tunisia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When Groups Fall Apart: Measuring Transnational Polarization during the Arab Uprisings</title>
      <link>http://www.robertkubinec.com/talk/preconf_2018/</link>
      <pubDate>Wed, 29 Aug 2018 12:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/talk/preconf_2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When Groups Fall Apart: Measuring Transnational Polarization with Twitter from the Arab Uprisings</title>
      <link>http://www.robertkubinec.com/project/islam-sec/</link>
      <pubDate>Wed, 08 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/project/islam-sec/</guid>
      <description>

&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;Scholars continue to disagree as to what extent international social connections act as a conduit to influence contentious politics within states. To answer this question, we provide the first rigorous and real-time measure of transnational ideological diffusion across sectarian groups by employing a novel statistical method and new data to capture the transnational dynamics of polarization after the Arab Uprisings of 2011. As authoritarian governments fell, populations in several states polarized between secularists and Islamists over what kind of regime was to replace the ousted one. To examine these endogenous processes, we collected a comprehensive dataset on elite and citizen Twitter accounts in Cairo and Alexandria (Egypt) and Tunis (Tunisia) for a ten-month period during the critical year of 2013. Given the difficulty in directly measuring polarization, we also developed a new model, item response theory-vector autoregression (IRT-VAR), that allows us to incorporate measurement uncertainty while providing over-time estimates of transnational polarization. We show through our model that following catalytic events like regime ousters (such as the military coup against the Muslim Brotherhood in Egypt), we can separate the direct effects of these events on group polarization within a country from indirect transnational feedback happening through the channel of social media.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Paper link&lt;/em&gt;: [&lt;a href=&#34;https://osf.io/preprints/socarxiv/wykmj/&#34; target=&#34;_blank&#34;&gt;https://osf.io/preprints/socarxiv/wykmj/&lt;/a&gt;]&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>idealstan: an R Package for Ideal Point Modeling with Stan</title>
      <link>http://www.robertkubinec.com/post/stancon2018_paper/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/post/stancon2018_paper/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/datatables-binding/datatables.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;This is a paper that was presented at the &lt;a href=&#34;http://mc-stan.org/events/stancon2018/&#34;&gt;StanCon2018 conference&lt;/a&gt; on Bayesian inference with the Stan Hamiltonian Markov Chain Monte Carlo (MCMC) method. Video of my talk is available &lt;a href=&#34;https://www.youtube.com/watch?v=0ZjrLOosXwk&amp;amp;t=4s&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This notebook introduces &lt;a href=&#34;https://cran.r-project.org/package=idealstan&#34;&gt;&lt;code&gt;idealstan&lt;/code&gt;&lt;/a&gt;, a new R package front-end to &lt;a href=&#34;http://www.mc-stan.org&#34;&gt;&lt;code&gt;Stan&lt;/code&gt;&lt;/a&gt; that allows for flexible modeling of a class of latent variable models known as ideal point models. Ideal point modeling is a form of dimension reduction, and shares similarities with multi-dimensional scaling, factor analysis and item-response theory. While the parameterization employed by &lt;code&gt;idealstan&lt;/code&gt; is a derivation based on item-response theory, it is important to note that several other parameterizations are possible &lt;a name=cite-carroll2013&gt;&lt;/a&gt;&lt;a name=cite-armstrong2014&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-carroll2013&#34;&gt;Carroll, Lewis, Lo, Poole, and Rosenthal, 2013&lt;/a&gt;; &lt;a href=&#34;#bib-armstrong2014&#34;&gt;Armstrong, Bakker, Carroll, Hare, Poole, and Rosenthal, 2014&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;What distinguishes ideal point modeling from other latent space models is that the latent factor is bi-directional: in other words, the unobserved latent construct that underlies the data could increase or decrease as the observed stimuli either increase or decrease. This type of latent variable is very useful when the aim is to cluster units around a polarizing dimension, such as the left-right axis in politics or the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value debate in statistics. While many of the applications of ideal point models are in political science and economics, it certainly can be applied more broadly, especially as the approach is related to latent space models more generally, such as in &lt;a name=cite-hoff2002&gt;&lt;/a&gt;&lt;a href=&#34;#bib-hoff2002&#34;&gt;Hoff, Raftery, and Hancock (2002)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are well-established frequentist (&lt;a href=&#34;https://cran.r-project.org/web/packages/emIRT/index.html&#34;&gt;&lt;code&gt;emIRT&lt;/code&gt;&lt;/a&gt;) and Bayesian (&lt;a href=&#34;https://cran.r-project.org/web/packages/pscl/pscl.pdf&#34;&gt;&lt;code&gt;pscl&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/MCMCpack/index.html&#34;&gt;&lt;code&gt;MCMCpack&lt;/code&gt;&lt;/a&gt;) packages for ideal point inference with R. However, these packages offer standard models that are limited in scope to particular problems, especially when it comes to including predictors in the latent space. The R package &lt;code&gt;idealstan&lt;/code&gt; has three features that set it apart from existing approaches:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Stan offers significant flexibility compared to existing approaches in using non-conjugate priors and a clearer programming interface. This flexibility allows for more options for end users in ideal point modeling that can be used to test new theories and hypotheses instead of being limited in the range and type of models available. In particular, Stan makes it easier to add hierarchical and time-series priors on to parameters, which opens the door to further analysis of dynamic and time-varying social phenomena.&lt;/li&gt;
&lt;li&gt;Stan scales well with the number of parameters, which is an attribute of most dimension reduction methods. Advances in Hamiltonian Monte Carlo estimation, including the use of variational inference and soon parallel computing within chains, promise to make full Bayesian inference of these models practical even with very large datasets.&lt;/li&gt;
&lt;li&gt;Increasingly, Stan is being married to helpful and powerful diagnostic packages, including &lt;a href=&#34;https://cran.r-project.org/web/packages/bayesplot/index.html&#34;&gt;&lt;code&gt;bayesplot&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/shinystan/index.html&#34;&gt;&lt;code&gt;shinystan&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/loo/index.html&#34;&gt;&lt;code&gt;loo&lt;/code&gt;&lt;/a&gt;, which extends the ability of &lt;code&gt;idealstan&lt;/code&gt; to provide not only cutting-edge Bayesian inference but also increasingly sophisticated tools for graphical analysis of resulting estimates. Given the complex nature of latent variable models, diagonistics are extremely important for determining when the model is behaving as it should (and what the model should be doing in the first place).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/saudiwin/idealstan&#34;&gt;&lt;code&gt;idealstan&lt;/code&gt;&lt;/a&gt; is an effort to build on prior efforts but also to offer new models that satisfy the increasing variety of applications to which ideal point models are being put. Both &lt;code&gt;pscl&lt;/code&gt; and &lt;code&gt;MCMCpack&lt;/code&gt; were designed for ideal point modeling of binary (logit) data from legislatures in which the outcome is made up of yes and no votes cast by legislators &lt;a name=cite-jackman2004&gt;&lt;/a&gt;&lt;a name=cite-quinn2002&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers, 2004&lt;/a&gt;; &lt;a href=&#34;#bib-quinn2002&#34;&gt;Martin and Quinn, 2002&lt;/a&gt;). More recently scholars have begun applying ideal point models to Twitter data &lt;a name=cite-barbera2015&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-barbera2015&#34;&gt;Barberá, 2015&lt;/a&gt;), massive campaign finance datasets &lt;a name=cite-bonica2014&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-bonica2014&#34;&gt;Bonica, 2014&lt;/a&gt;) and party campaign manifestos &lt;a name=cite-slapin2008&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-slapin2008&#34;&gt;Slapin and Proksch, 2008&lt;/a&gt;). This package offers both the traditional forms of ideal point models along with new extensions, including a version of ideal points that can take into account certain forms of missing data.&lt;/p&gt;
&lt;p&gt;In this notebook I first introduce ideal-point models and contrast them with “traditional” item response theory (IRT), and then I demonstrate the &lt;code&gt;idealstan&lt;/code&gt; package through simulations. I then perform two empirical analyses, the first of coffee product ratings from Amazon and the second with voting data taken from the 114th Senate. In the second example, I also show how &lt;code&gt;idealstan&lt;/code&gt; enables new estimation of strategic legislator absence, a type of data that is usually coded as missing in vote datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ideal-point-models-as-a-subset-of-statistical-measurement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ideal Point Models as a Subset of Statistical Measurement&lt;/h2&gt;
&lt;p&gt;Measurement error models have a long history in applied statistics and are increasingly in demand as the amount of noisy observational data grows with the digital revolution. Canonical statistical models, particularly linear regression, assume that the predictors are measured without error, but in many situations in social science, the variable of interest cannot in fact be measured. Rather, proxies or indicators are used to stand in for the latent construct. Measurement models offer a way to match the indicators with the latent construct while making appropriate assumptions about the relationship.&lt;/p&gt;
&lt;p&gt;In other words, we suppose that the regression predictor matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is itself a function of certain indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c \in \{1 ... C\}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
X = f(\forall I\_c \in \{1 ... C\})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Different latent variable, latent space and measurement models can be distinguished in terms of the function &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt;. Fundamentally, &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; has to stipulate how &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; will change as the indicators change. It is possible to make very precise conditions for this relationship, which is the method applied in confirmatory factor analysis and structural equation modeling. It is also possible to let &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; make only minimal assumptions about this relationship, which is the method adopted by exploratory factor analysis and item-response theory (IRT), in addition to the other latent space models in the literature. It is also possible, of course, to have models that fall somewhere in between truly exploratory and truly confirmatory models.&lt;/p&gt;
&lt;p&gt;While much has been written about the different kids of &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; that can be used in measurement models, in this paper I focus on a metric relevant to ideal point models: as the values of &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt; increase, does &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; also increase so that &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; must be always non-decreasing? If that is the case, then the model can be thought of as falling into the domain of item-response theory and traditional factor analysis (in fact, IRT is itself a non-linear version of factor analysis per &lt;a name=cite-takane1986&gt;&lt;/a&gt;&lt;a href=&#34;#bib-takane1986&#34;&gt;Takane and de Leeuw (1986)&lt;/a&gt;). By contrast, ideal point models are based on a different set of assumptions in which &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; could increase or decrease as the indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt; increase or decrease; in other words, the relationship is bi-polar instead of uni-polar.&lt;/p&gt;
&lt;p&gt;Ideal point modeling originated from analysis of a common situation in policy research in which different legislators select from among competing policy alternatives based on the utility offered by each policy &lt;a name=cite-enelow198&gt;&lt;/a&gt;&lt;a name=cite-poole2008&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-enelow198&#34;&gt;Enelow and Hinich, 1984&lt;/a&gt;; &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&#34;&gt;Poole, Lewis, Lo, and Carroll, 2008&lt;/a&gt;). Supposing that the policies are evaluated on a uni-dimensional space, if the utility of the “Yes” position on the policy is greater than the utility of the “No” position to a particular legislator, then that legislator will choose that policy. While very simple, this model has a requirement that is different from much of traditional IRT estimation: the policy outcomes can have different meanings based on their position relative to the legislator in the policy space. For example, if a legislator is very liberal (has a high value on the latent construct), then that legislator is likely to vote yes on bills that are also liberal, such as single-payer health care. But if the bill is conservative, such as national defense, then that legislator is more likely to vote no. In other words, the meaning of the positions of the bills in the latent space depends on the ideal point of the legislator, and thus the mapping to the latent space &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; cannot be always non-decreasing. For some votes, a yes position may mean a high value of the latent space, while for others it may mean a lower value.&lt;/p&gt;
&lt;p&gt;By comparison, in traditional IRT, responses to stimuli (the indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt;) reflect correct or incorrect answers, such as a student taking a test. Correct answers, which in the legislative context mean yes votes, always equal greater ability, while incorrect answers, which would be no votes, are always a sign of less ability. In this situation, &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; is always non-decreasing. Thus while ideal point models share much in common with IRT, they require different assumptions.&lt;/p&gt;
&lt;p&gt;A brief review of the mathematical notation will make the difference clear. The 2-PL IRT model takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y\_{ij} = \alpha\_j x\_i - \beta\_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; can be an outcome of any type (binary, ordinal, Poisson, etc.) that includes the correct or incorrect responses of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; test takers to &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; test items, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; are the discrimination parameters that control the narrowness, hence discrimination, of each item &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; on the test in the latent space, &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; are the test-taker ability parameters that reflect the ability of a person to answer an item correctly, and &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt; are the difficulty or average probability/value of a correct response (the intercept). In the traditional IRT format, the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; discrimination parameters are always positive because a higher value (a correct answer) is always associated with higher ability &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;, while a lower value (an incorrect answer) is associated with less ability.&lt;/p&gt;
&lt;p&gt;However, if the requirement that the discrimination parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; are positive is removed, then this model can also be used for ideal point modeling. For most applications, ideal-point modeling using IRT is built on the standard 2-PL model &lt;a name=cite-gelman2005&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers, 2004&lt;/a&gt;; &lt;a href=&#34;#bib-gelman2005&#34;&gt;Bafumi, Gelman, Park, and Kaplan, 2005&lt;/a&gt;), with one notable exception: the discrimination parameters must be un-constrained, while in traditional IRT the discrimination parameters are constrained to all be either positive or negative. Constraining discrimination parameters to be positive is also the approach taken in the &lt;a href=&#34;https://cran.r-project.org/web/packages/edstan/vignettes/briefmanual.html&#34;&gt;&lt;code&gt;edstan&lt;/code&gt;&lt;/a&gt; package, which offers the full range of standard IRT models using Stan.&lt;/p&gt;
&lt;p&gt;Leaving the discrimination parameters un-consrained ensures that a higher or lower value of &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; could be associated with either a “correct” or “incorrect” answer as is necessary in an ideal point model. This happens because the latent space in an ideal point model is fundamentally a Euclidean distance that is rotation-invariant. The following figure makes this clear by showing a version of an ideal point model in which the ideal point (ability parameter) &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; is represented by a Normal distribution while the Yes and No points of a proposed policy are vertical lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; is to the right of the Indifference line, the legislator will vote Yes on the proposal, whereas if she is to the left, she will vote No. However, a different policy (item) could have the Yes and No positions switched so that a Yes vote would correspond to -2.5 and vice versa for the No vote. Thus the binary outcomes do not have the same interpretation as the standard IRT model because the relationship between the outcomes and the ability points is contingent.&lt;/p&gt;
&lt;p&gt;If the discrimination parameters are unconstrained, &lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers (2004)&lt;/a&gt; showed that the midpoint of the policy/item parameter, which is labeled on the plot as the indifference line, is identified as &lt;span class=&#34;math inline&#34;&gt;\(-\frac{\alpha\_j}{\beta\_j}\)&lt;/span&gt; in the original IRT model. This midpoint is important because it represents the point in the latent space in which an actor is indifferent, or has a 50% chance of voting yes or no. Unfortunately, the IRT paramerization of the ideal point model does not return the actual Yes or No positions of a bill/item, but these midpoints are sufficient to characterize the position of the items in the latent space.&lt;/p&gt;
&lt;p&gt;While this difference between ideal point models and standard IRT may seem trivial, it has serious consequences for modeling. The basic issue is the non-identifiability of the IRT model, which the switching polarity of the discrimination parameters only makes more difficult. Ultimately, it becomes necessary to constrain the polarity of some of the ability or discrimination parameters to identify the model. However, it is necessary to choose parameters which would avoid “splitting the likelihood”, such as constraining a legislator whose true ideal point is near zero (&lt;a href=&#34;#bib-gelman2005&#34;&gt;Bafumi, Gelman, Park, et al., 2005&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;idealstan&lt;/code&gt; exploits variational Bayesian (VB) inference &lt;a name=cite-NIPS2015_5758&gt;&lt;/a&gt;(&lt;a href=&#34;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&#34;&gt;Kucukelbir, Ranganath, Gelman, and Blei, 2015&lt;/a&gt;) to assist with finding parameters for identification. &lt;code&gt;idealstan&lt;/code&gt; first estimates an IRT model with rotation-unidentified parameters, which is not a problem for a single VB run. Then &lt;code&gt;idealstan&lt;/code&gt; can select those parameters which show the highest or lowest values, and these parameters are then constrained in terms of their polarity. This approach can quickly speed up the sometimes painful process of identifying a particular IRT ideal point model, although it is also possible to constrain parameters before estimation based on prior information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data-in-ideal-point-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing Data in Ideal Point Models&lt;/h2&gt;
&lt;p&gt;Because of the flexibility of Stan,&lt;code&gt;idealstan&lt;/code&gt; is not limited to implementing the standard IRT ideal point model. One of the modifications it offers is a model that can account for one-way censoring in the data, or what is called an absence-inflated ideal point model &lt;a name=cite-kubinec2017&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-kubinec2017&#34;&gt;Kubinec, 2017a&lt;/a&gt;). This model was motivated by the application of ideal point models to legislatures in which legislator actions are more diverse than simply Yes or No votes. Legislators can also be absent on votes and in parliamentary systems as in Europe, abstentions are also common. Abstentions occur when a legislator votes on a policy but refuses to either vote Yes or No, resulting in a vote record of abstention that conventional models will treat as missing data. However, if these two additional vote outcomes–absent and abstention–are removed from the estimation by encoding them as missing data, then the additional information about legislator behavior present in this data is lost.&lt;/p&gt;
&lt;p&gt;As a solution for abstentions, I proposed in &lt;a href=&#34;#bib-kubinec2017&#34;&gt;Kubinec (2017a)&lt;/a&gt; to model abstentions as a middle category between yes and no votes by treating &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; as an ordered logistic outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ijk}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; in three possible vote choices: &lt;span class=&#34;math inline&#34;&gt;\(\{1,2,3\}\)&lt;/span&gt; (no, abstain, yes).&lt;/p&gt;
&lt;p&gt;While switching from a logit to an ordered logistic model is straightforward, modeling the censoring implied by legislator absences requires the estimation of additional parameters. To do so, I model absence as a binary outcome in a separate equation as a hurdle model. Intuitively, legislators only show up to vote if they are able to overcome the hurdle of being present. Because in an ideal point model we only want to estimate a single set of person parameters (ideal points), I include an additional set of item (bill) parameters in the hurdle component that signify the way in which being absent on a policy proposal is related to the ideological value of the legislation.&lt;/p&gt;
&lt;p&gt;The result is a two-stage model that inflates the ideal points by the probability that a legislator is absent on a particular bill. To create this model, I first start again with the essential 2-PL IRT model except that it now allows for cutpoints &lt;span class=&#34;math inline&#34;&gt;\(c\_k\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; vote outcomes (yes, abstain, no) with &lt;span class=&#34;math inline&#34;&gt;\(\zeta(\cdot)\)&lt;/span&gt; representing the logit function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
    L(\beta,\alpha,x|Y\_{ijk}) = \prod\_{i-1}^{I} \prod\_{j=1}^{J}
    \begin{cases} 
    1 -  \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_1) &amp;amp; \text{if } K = 0 \\
    \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k-1}) - \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k})       &amp;amp; \text{if } 0 &amp;lt; k &amp;lt; K, \text{ and} \\
    \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k-1}) - 0 &amp;amp; \text{if } k=K
    \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Except for the addition of the vote cutpoints, this model is identical to the standard 2-PL IRT form, with ability parameters &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;, discriminations &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt;, and difficulties &lt;span class=&#34;math inline&#34;&gt;\(a\_j\)&lt;/span&gt;. The cutpoints carve up the latent space so that as ability rises or falls, the legislator becomes more likely to vote yes or no with abstention falling in between these two categories.&lt;/p&gt;
&lt;p&gt;This ordered logit has to be embedded in the hurdle model to account for one-way censoring. Suppose that each legislator has a choice &lt;span class=&#34;math inline&#34;&gt;\(r \in \{0,1\}\)&lt;/span&gt; in which &lt;span class=&#34;math inline&#34;&gt;\(r=1\)&lt;/span&gt; if a legislator is absent and &lt;span class=&#34;math inline&#34;&gt;\(r=0\)&lt;/span&gt; otherwise. With this notation, the likelihood of the hurdle model takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
    L(\beta,\alpha,X,Q,\gamma,\omega|Y\_{k},Y\_{r}) = 
    \prod\_{I}^{i=1} \prod\_{J}^{j=1}
    \begin{cases}
    \zeta(x\_{i}&amp;#39;\gamma\_j - \omega\_j ) &amp;amp; \text{if } r=0, \text{ and} \\
    (1-\zeta({x\_{i}&amp;#39;\gamma\_j - \omega\_j}))L(\beta,\alpha,X|Y\_{k1}) &amp;amp; \text{if } r=1
    \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the hurdle model to be able to affect the legislator ideal points, a separate IRT 2-PL model is included to predict the binary outcome of absence or presence. In this secondary model, the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\_j\)&lt;/span&gt; discrimination and &lt;span class=&#34;math inline&#34;&gt;\(\omega\_j\)&lt;/span&gt; difficulty parameters represent the salience of a particular piece of legislation to an individual legislator &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;. Only if a bill clears the hurdle of salience will the legislator choose to vote on the legislation, i.e., reach the vote outcome model &lt;span class=&#34;math inline&#34;&gt;\(L(\beta,\alpha,X)\)&lt;/span&gt;. This device allows for the ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; to adjust for the fact that the decision of a legislator to show up to vote on a particular bill may be strategic instead of random. In addition, this model is more widely applicable to situations in which an ideal point model is employed and missing data may be a function of the persons’ ideal points.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Simulation&lt;/h2&gt;
&lt;p&gt;To identify the model, I placed &lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt; parameters on the &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; ideal points and additional polarity constraints on either the ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; or the discrimination parameters &lt;span class=&#34;math inline&#34;&gt;\(\gamma\_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt;. As a further restriction, I constrain one of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt; to equal zero. These are the standard set of restrictions included in &lt;code&gt;idealstan&lt;/code&gt;, although the scales of parameters can be modified. The full set of priors is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
        c\_k - c\_{k-1} \sim N(0,5)\\
        \gamma\_j \sim N(0,2)\\
        \omega\_j \sim N(0,5)\\
        \beta\_j \sim N(0,2)\\
        \alpha\_j \sim N(0,5)\\
        x\_i \sim N(0,1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(c\_k\)&lt;/span&gt; parameters are the cutpoints for the &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; ordinal outcomes. They are given a weakly informative prior on the differences between the cutpoints (see the prior help file in the Stan Development Github site). The rest of the priors are arbitrary as the scale of the latent variables is fixed only in the priors themselves.&lt;/p&gt;
&lt;p&gt;To demonstrate the package, I first generate data from this data-generating process using a function &lt;code&gt;id_sim_gen()&lt;/code&gt; built into &lt;code&gt;idealstan&lt;/code&gt;:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;],[1,3,4,4,4,3],[4,1,1,1,4,4],[4,2,3,1,3,4],[1,4,4,4,4,1],[1,4,3,4,4,4],[4,2,3,2,3,2],[1,4,4,4,4,2],[2,4,4,4,4,4],[4,1,1,1,3,4],[4,3,3,3,4,3]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;1&lt;\/th&gt;\n      &lt;th&gt;2&lt;\/th&gt;\n      &lt;th&gt;3&lt;\/th&gt;\n      &lt;th&gt;4&lt;\/th&gt;\n      &lt;th&gt;5&lt;\/th&gt;\n      &lt;th&gt;6&lt;\/th&gt;\n      &lt;th&gt;7&lt;\/th&gt;\n      &lt;th&gt;8&lt;\/th&gt;\n      &lt;th&gt;9&lt;\/th&gt;\n      &lt;th&gt;10&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:[1,2,3,4,5,6,7,8,9,10]},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;In this matrix, the legislators (persons) are represented by the rows and the bills (items) by the columns. The ordinal vote outcomes are numbered 1 to 3 (1=No,2=Abstain,3=Yes) and 4 represents absence.&lt;/p&gt;
&lt;p&gt;I can then take this simulated data and put it into the &lt;code&gt;id_estimate&lt;/code&gt; function. I also use the true values of the legislator ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; for polarity constraints in order to be able to return the “true” latent variables. The &lt;code&gt;id_estimate&lt;/code&gt; function loads pre-compiled stan code a la &lt;a href=&#34;https://cran.r-project.org/web/packages/rstantools/index.html&#34;&gt;&lt;code&gt;rstantools&lt;/code&gt;&lt;/a&gt; and then returns an R object containing a compiled stan model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;true_legis &amp;lt;- ord_ideal_sim@simul_data$true_person
high_leg &amp;lt;- sort(true_legis,decreasing = T,index.return=T)
low_leg &amp;lt;- sort(true_legis,index.return=T)

ord_ideal_est &amp;lt;- id_estimate(idealdata=ord_ideal_sim,
                             model_type=4,
                             fixtype=&amp;#39;constrained&amp;#39;,
                             restrict_type=&amp;#39;constrain_twoway&amp;#39;,
                             restrict_ind_high = high_leg$ix[1:2],
                             restrict_ind_low=low_leg$ix[1:2],
                             refresh=500)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For testing simulation results, the package contains a residual plot for looking at differences between true and estimated values which are stored in the R object:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/check_true-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The recovery of the true parameters is not perfect, but generally the errors sum to zero across the parameters. Recovering the true parameters is not usually of great interest in a latent variable model as the scale and rotation of the true values is rather arbitrary. However, this exercise is a basic test of the Stan model’s correspondence with the simulated data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-the-polarization-of-coffee&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical Example: The Polarization of Coffee&lt;/h2&gt;
&lt;p&gt;To demonstrate the model’s functionality, I first use a dataset of ordinal rankings draw from Amazon food product reviews. &lt;a name=cite-mcauley2013&gt;&lt;/a&gt;&lt;a href=&#34;#bib-mcauley2013&#34;&gt;McAuley and Leskovec (2013)&lt;/a&gt; collected over 500,000 Amazon food reviews from 1999 to 2012 which are all coded on the 1 to 5 ranking scale that users can post on each product. I focus on a subset of this data by collecting all the reviews that mention the word “coffee” at least a handful of times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;just_coffee &amp;lt;- readRDS(&amp;#39;just_coffee.rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ideal point models use variation between products and users in reviews to identify the latent space. For these reasons, little information is contributed by products and/or users that have very few reviews, and we can remove them from the data first to reduce the dimensionality of the data. Because &lt;code&gt;idealstan&lt;/code&gt; is a full Bayesian model, these users and products do not cause any statistical problems, but they will slow estimation considerably as a large number of users only review one or two products.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coffee_count_prod &amp;lt;- group_by(just_coffee,ProductId) %&amp;gt;% summarize(tot_unique=length(unique(UserId))) %&amp;gt;% 
  filter(tot_unique&amp;lt;30)
coffee_count_user &amp;lt;- group_by(just_coffee,UserId) %&amp;gt;% summarize(tot_unique=length(unique(ProductId))) %&amp;gt;% 
  filter(tot_unique&amp;lt;3)
just_coffee &amp;lt;- anti_join(just_coffee,coffee_count_prod,by=&amp;#39;ProductId&amp;#39;) %&amp;gt;% 
  anti_join(coffee_count_user,by=&amp;#39;UserId&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is currently in long format. To bring the data into &lt;code&gt;idealstan&lt;/code&gt;, it first must be translated to wide format in which each item (product) is a column and each person (user) is a row. For this model, we will drop missing data as we will focus solely on the model’s ordinal rankings without accounting for the fact that users do not have reviews for all products.&lt;/p&gt;
&lt;p&gt;Even with inactive users removed from the data, this model is quite large with 11269 users and 429 products. However, we can take advantage of variational inference in Stan to obtain approximate posterior estimates in a reasonable amount of time. We will also take advantage of &lt;code&gt;idealstan&lt;/code&gt;’s use of variational inference for automatic model identification by running a non-identified (i.e., no constraints) model first, determining which of the users has a very high ideal point, and then constraining that ideal point in the final fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coffee_model &amp;lt;- id_estimate(idealdata=coffee_data,
                            model_type=3,
                            fixtype=&amp;#39;vb&amp;#39;,
                            restrict_ind_high = 1,
                            restrict_params=&amp;#39;person&amp;#39;,
                            auto_id = F,
                            use_vb=T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.042 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 420 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -6e+004             1.000            1.000
##    200    -5e+004             0.642            1.000
##    300    -5e+004             0.441            0.284
##    400    -5e+004             0.346            0.284
##    500    -5e+004             0.277            0.059
##    600    -5e+004             0.234            0.059
##    700    -5e+004             0.200            0.040
##    800    -5e+004             0.176            0.040
##    900    -5e+004             0.157            0.014
##   1000    -5e+004             0.142            0.014
##   1100    -5e+004             0.042            0.006   MEDIAN ELBO CONVERGED
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.
## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.035 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 350 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -5e+009             1.000            1.000
##    200    -1e+008            22.582           44.164
##    300    -9e+010            15.388            1.000
##    400    -4e+009            17.796           25.020
##    500    -2e+007            58.526           25.020
##    600    -2e+006            49.788           25.020
##    700    -1e+006            42.834            6.097
##    800    -5e+004            40.303           22.581
##    900    -5e+004            35.826            6.097
##   1000    -5e+004            32.244            6.097
##   1100    -5e+004            32.146            6.097   MAY BE DIVERGING... INSPECT ELBO
##   1200    -5e+004            27.730            1.114   MAY BE DIVERGING... INSPECT ELBO
##   1300    -5e+004            27.630            1.114   MAY BE DIVERGING... INSPECT ELBO
##   1400    -5e+004            25.128            0.016   MAY BE DIVERGING... INSPECT ELBO
##   1500    -5e+004             2.984            0.015   MAY BE DIVERGING... INSPECT ELBO
##   1600    -5e+004             2.374            0.005   MEDIAN ELBO CONVERGED   MAY BE DIVERGING... INSPECT ELBO
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is of primary interest in this model are the discrimination parameters for the products. The discriminations will tell us which coffee products tend to divide the users most strongly into two poles as the model is one-dimensional. We can first look at the distribution of discriminations via a histogram:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_all_hist(coffee_model,params = &amp;#39;regular_discrim&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/plot_discrim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To gain a sense of how polarizing these products, we can plot the product mid-point, or the point at which a user is indifferent between one rating score and a higher rating score, over the ideal point distribution. I color the user points by their actual ratings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis(coffee_model,group_color=F,group_overlap = T,
              person_labels=F,person_ci_alpha = .05,
              item_plot=429,
              group_labels=F,
              point_size=2,
              show_score = c(&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;,&amp;#39;4&amp;#39;,&amp;#39;5&amp;#39;),
              text_size_group = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/high_pos_discrim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, the product midpoints nearly perfectly segment the user base. We also a large number of 1s and 5s in the observed ratings. This distribution shows that users tend to be very divided on this product, and also that their disagreement over the product is itself a reflection of an underlying dimension, their ideal points.&lt;/p&gt;
&lt;p&gt;Finally, I include here the top 10 most polarizing (highly discriminative) products from each end of the ideal point spectrum.&lt;/p&gt;
&lt;p&gt;First, the top 10 most positive discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;Kicking Horse Coffee 454 Horse Power Dark, Whole Bean&#34;,&#34;Amazing Grass Green Superfood Organic Powder&#34;,&#34;Starbucks Via Ready Brew Instant Coffee&#34;,&#34;Stephen&#39;s Gourmet Hot Cocoa, Candycane Cocoa&#34;,&#34;Prince of Peace Organic Tea, Oolong&#34;,&#34;NOW Foods Erythritol Natural Sweetener&#34;,&#34;illy Cappucino, 12 pack&#34;,&#34;Nestle Hot Cocoa Mix&#34;,&#34;Better than Milk Vegan Soy Powder&#34;,&#34;Ghirardelli Chocolate Sweet Ground Chocolate &amp;amp; Cocoa&#34;],[7.87259,7.864585,7.80567,7.805575,7.770375,7.770345,7.727985,7.33822,7.33797,7.28447]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Product Description&lt;\/th&gt;\n      &lt;th&gt;Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Second, the top 10 least positive negative discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;Melitta Cafe de Europa Gourmet Coffee&#34;,&#34;McCann&#39;s Steel Cut Irish Oatmeal&#34;,&#34;Stash Tea Gunpowder Green Loose Leaf Tea&#34;,&#34;Nutiva Organic Coconut Oil&#34;,&#34;Stash Tea Company English Breakfast&#34;,&#34;Equal Exchange Organic Coffee, Mind Body Soul, Whole Bean&#34;,&#34;Ghirardelli Premium Hot Beverage Mix, White Mocha, 19-Ounce Cans&#34;,&#34;Victorian Inn Instant Hot Cappucino, French Vanilla&#34;,&#34;Medaglia Doro Instant Espresso&#34;,&#34;Yogi Tea, Green Tea Super Antioxidant&#34;],[-9.79306,-9.785615,-9.70132,-9.616965,-7.77101,-7.75944,-7.74106,-7.73298,-7.17082,-7.13721]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Product Description&lt;\/th&gt;\n      &lt;th&gt;Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;While a full interpretation of these results would require significant work at examining the full range of products and their relevant discrimination scores, what is clear is that positive discrimination tends to have more large brands of coffee, including Starbucks and Illy, while the negative discrimination products tend to b esmaller brands, such as Melitta Cafe and Equal Exchange Organic Coffee. The most interesting finding in this analysis is that Ghirardelli products are at both ends of the scale: one type of Ghirardelli powdered drink, Ghirardelli Chocolate, has high positive discrimination, while Ghirardelli White Mocha has high negative discrimination. Intuitivelly, these products are appealing to different reviewers with very diverging preferences.&lt;/p&gt;
&lt;p&gt;In the next section, I apply the ideal point model to a more traditional domain–the U.S. Congress–and also examine the role of including missing data via inflation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-114th-senate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical Example: 114th Senate&lt;/h2&gt;
&lt;p&gt;As an empirical example of the model, I use data from the &lt;a href=&#34;http://www.voteview.com&#34; class=&#34;uri&#34;&gt;http://www.voteview.com&lt;/a&gt; website on the voting record of the 114th US Senate. This dataset comes with the &lt;code&gt;idealstan&lt;/code&gt; package, and I then recode the data to correspond to a standard R matrix. However, because abstentions rarely happen in the US Congress, I do not include abstentions in this model and instead estimate a standard binary IRT 2-PL with inflation for missing data (absences). I then pass the matrix of vote records to the &lt;code&gt;id_make&lt;/code&gt; function to create an object suitable for running the model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/use_senate-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given a suitable object from &lt;code&gt;id_make&lt;/code&gt;, the &lt;code&gt;id_estimate&lt;/code&gt; function can estimate a variety of IRT ideal point models, including 2-PL, ordinal, hierarchical and dynamic IRT (using random-walk priors). For this run we set the &lt;code&gt;model_type&lt;/code&gt; parameter to &lt;code&gt;2&lt;/code&gt;, which represents a 2-PL model with absence inflation. Because this dataset is of a significant size, I use variational inference instead of the full sampler to demonstrate its use. I do not, however, use the automatic identification option &lt;code&gt;auto_id&lt;/code&gt; because it is relatively easy to constrain particular legislator ideal points for the Senate. In this case, I constrain Ben Sasse, Marco Rubio, Berni Sanders, Ted Cruz, Harry Reid and Elizabeth Warren, all senators with pronounced ideological views.&lt;/p&gt;
&lt;p&gt;After estimating the model, I can look at a graphical display of the ideal points with the &lt;code&gt;id_plot&lt;/code&gt; function. The &lt;code&gt;id_plot&lt;/code&gt; function produces a &lt;code&gt;ggplot2&lt;/code&gt; object which can be further modified. The legislators are given colors based on their party affiliation. The general shape of the ideal point distribution reflects the nature of polarization in the US Congress, with relatively few moderates near the center of the distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.026 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 260 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -2e+004             1.000            1.000
##    200    -2e+004             0.509            1.000
##    300    -2e+004             0.341            0.018
##    400    -2e+004             0.256            0.018
##    500    -2e+004             0.205            0.003   MEDIAN ELBO CONVERGED
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/run_114_model-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also plot the bill (item) midpoints as a line of indifference that we overlay on top of the ideal points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot(sen_est,person_ci_alpha=.1,item_plot=205,
        group_labels=T,
        abs_and_reg=&amp;#39;Vote Points&amp;#39;) + scale_colour_brewer(type=&amp;#39;qual&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/bill_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this particular piece of legislation, the midpoint is right in the middle of the ideal point distribution, showing that the bill has very high discrimination. Also, the rug lines at the bottom of the plot, which show the HPD for the midpoint, indicate that the model is uncertain about the votes of only a handful of Senators on this bill.&lt;/p&gt;
&lt;p&gt;We can similarly examine the absence midpoint for the same bill, which signifies the place on the ideal point spectrum at which a legislator is indifferent from showing up to vote. In this case, only very conservative Republicans chose not to show up to vote.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/abs_bill_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use the bill absence parameters to also see which bills showed the highest discrimination in terms of absences, or in other words, for which bills did the absence of legislators signify that they intentionally did not show up? To do this, I sort the discrimination parameters from the underlying Stan object and then merge them with the bill labels from voteview.org.&lt;/p&gt;
&lt;p&gt;First we can look at the top 10 bills with liberal/Democrat absence discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-4&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-4&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;To ensure that the storage and transportation of petroleum coke is regulated in a manner that ensures the protection of public and ecological health.&#34;,&#34;To continue cleaning up fields and streams while protecting neighborhoods, generating affordable energy, and creating jobs.&#34;,&#34;To express the sense of the Senate that climate change is real and not a hoax.&#34;,&#34;To require the use of iron, steel, and manufactured goods produced in the United States in the construction of the Keystone XL Pipeline and facilities.&#34;,&#34;To express the sense of Congress regarding federally protected land.&#34;,&#34;To promote energy efficiency.&#34;,&#34;To provide limits on the designation of new federally protected land.&#34;,&#34;To express the sense of Congress regarding climate change.&#34;,&#34;To conform citizen suits under the Endangered Species Act of 1973.&#34;,&#34;To ensure that oil transported through the Keystone XL pipeline into the United States is used to reduce United States dependence on Middle Eastern oil.&#34;],[1.819605,1.688775,1.67506,1.66709,1.643,1.549565,1.52296,1.499735,1.483755,1.453105]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Bill Description&lt;\/th&gt;\n      &lt;th&gt;Absence Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Interestingly, Democrats appeared to be strategically absent most often on bills about climate change and the Keystone XL pipeline.&lt;/p&gt;
&lt;p&gt;For conseratives the bills are as follows:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-5&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-5&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;To limit the availability of amounts authorized to be appropriated for overseas contingency operations pending relief from the spending limits under the Budget Control Act of 2011.&#34;,&#34;To strike the FOIA exemption.&#34;,&#34;To improve the bill.&#34;,&#34;To improve the requirements relating to removal of personal information from cyber threat indicators before sharing.&#34;,&#34;To strengthen the Justice for Victims of Trafficking Act by incorporating additional bipartisan amendments.&#34;,&#34;To protect information that is reasonably believed to be personal information or information that identifies a specific person.&#34;,&#34;To improve the definitions of cybersecurity threat and cyber threat indicator.&#34;,&#34;To exempt from the capability and process within the Department of Homeland Security communication between a private entity and the Federal Bureau of Investigation or the United States Secret Service regarding cybersecurity threats.&#34;,&#34;An original bill to improve cybersecurity in the United States through enhanced sharing of information about cybersecurity threats, and for other purposes.&#34;,&#34;To modify section105 to require DHS to review all cyber threat indicators and countermeasures in order to remove certain personal information.&#34;],[-3.956115,-3.639605,-3.63636,-3.57792,-3.516255,-3.42238,-3.15867,-3.12428,-3.075425,-2.992295]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Bill Description&lt;\/th&gt;\n      &lt;th&gt;Absence Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;For Republicans, on the other hand, it appears that cybersecurity and inter-departmental information sharing were the laws in which they chose not to show up for ideological (or political) reasons. This could be due to concerns among the conservative base regarding government over-reach and collecting information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;idealstan&lt;/code&gt; is an effort to put state-of-the-art ideal point models along with the power of full Bayesian analysis in the hands of applied researchers in the social sciences. While the empirical examples presented were from the US Congress, the model can be used in any situation in which the assumptions of the ideal point model (unconstrained ideal point space) apply, i.e., situations in which the latent space is fundamentally polarizing between people. While similar in function and form to the &lt;code&gt;edstan&lt;/code&gt; package, the &lt;code&gt;idealstan&lt;/code&gt; package has to use more complicated forms of identification because of the difficulty of leaving discrimination parameters unconstrained.&lt;/p&gt;
&lt;p&gt;Moving forward, I intend to add in more functionality to smoothly operate with &lt;code&gt;shinystan&lt;/code&gt; and &lt;code&gt;bayesplot&lt;/code&gt;, as well as add further types of explanatory IRT models. The intention is to have a package on which applied researchers can run different ideal point models and then examine how different modeling choices affect the results. In addition, I will continue to build more visualization options so that the results are easily digestable and publishable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;’ &lt;a name=bib-enelow198&gt;&lt;/a&gt;&lt;a href=&#34;#cite-enelow198&#34;&gt;[1]&lt;/a&gt; J. M. Enelow and M. J. Hinich. &lt;em&gt;The Spatial Theory of Voting: An Introduction&lt;/em&gt;. Cambridge University Press, 1984.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-takane1986&gt;&lt;/a&gt;&lt;a href=&#34;#cite-takane1986&#34;&gt;[2]&lt;/a&gt; Y. Takane and J. de Leeuw. “On the Relationship Between Item Response Theory and Factor Analysis of Discretized Variables”. In: &lt;em&gt;Psychometrika&lt;/em&gt; 52.3 (1986), pp. 393-408.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-hoff2002&gt;&lt;/a&gt;&lt;a href=&#34;#cite-hoff2002&#34;&gt;[3]&lt;/a&gt; P. D. Hoff, A. E. Raftery and M. S. Hancock. “Latent Space Approaches to Social Network Analysis”. In: &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 97.460 (2002), pp. 1090-1098.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-quinn2002&gt;&lt;/a&gt;&lt;a href=&#34;#cite-quinn2002&#34;&gt;[4]&lt;/a&gt; A. D. Martin and K. M. Quinn. “Dynamic Ideal Point Estimation via Markov Chain Monte Carlo for the U.S. Supreme Court, 1953-1999”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 10.2 (2002), pp. 134-153.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-jackman2004&gt;&lt;/a&gt;&lt;a href=&#34;#cite-jackman2004&#34;&gt;[5]&lt;/a&gt; J. Clinton, S. Jackman and D. Rivers. “The Statistical Analysis of Rollcall Data”. In: &lt;em&gt;American Political Science Review&lt;/em&gt; 98.2 (2004), pp. 355-370.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-gelman2005&gt;&lt;/a&gt;&lt;a href=&#34;#cite-gelman2005&#34;&gt;[6]&lt;/a&gt; J. Bafumi, A. Gelman, D. K. Park, et al. “Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 13.2 (2005), pp. 171-187.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-poole2008&gt;&lt;/a&gt;&lt;a href=&#34;#cite-poole2008&#34;&gt;[7]&lt;/a&gt; K. T. Poole, J. B. Lewis, J. Lo, et al. &lt;em&gt;Scaling Roll Call Votes with W-NOMINATE in R&lt;/em&gt;. Working Paper. Social Science Research Network, Oct. 06, 2008. URL: &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&#34; class=&#34;uri&#34;&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-slapin2008&gt;&lt;/a&gt;&lt;a href=&#34;#cite-slapin2008&#34;&gt;[8]&lt;/a&gt; J. B. Slapin and S. Proksch. “A Scaling Model for Estimating Time-Series Party Positions from Texts”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 52.3 (2008), pp. 705-722.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-carroll2013&gt;&lt;/a&gt;&lt;a href=&#34;#cite-carroll2013&#34;&gt;[9]&lt;/a&gt; R. Carroll, J. B. Lewis, J. Lo, et al. “The Structure of Utility in Spatial Models of Voting”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 57.4 (2013), pp. 1008-1028.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-mcauley2013&gt;&lt;/a&gt;&lt;a href=&#34;#cite-mcauley2013&#34;&gt;[10]&lt;/a&gt; J. McAuley and J. Leskovec. “From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews”. In: &lt;em&gt;WWW&lt;/em&gt; (2013).&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-armstrong2014&gt;&lt;/a&gt;&lt;a href=&#34;#cite-armstrong2014&#34;&gt;[11]&lt;/a&gt; D. A. Armstrong, R. Bakker, R. Carroll, et al. &lt;em&gt;Analyzing Spatial Models of Choice and Judgment with R&lt;/em&gt;. CRC Press, 2014.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-bonica2014&gt;&lt;/a&gt;&lt;a href=&#34;#cite-bonica2014&#34;&gt;[12]&lt;/a&gt; A. Bonica. “Mapping the Ideological Marketplace”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 58.2 (2014), pp. 367-386.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-barbera2015&gt;&lt;/a&gt;&lt;a href=&#34;#cite-barbera2015&#34;&gt;[13]&lt;/a&gt; P. BarberÃ¡. “Birds of the Same Feather Tweet Together: Bayesian Ideal Point Estimation Using Twitter Data”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 23 (2015), pp. 76-91.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-NIPS2015_5758&gt;&lt;/a&gt;&lt;a href=&#34;#cite-NIPS2015_5758&#34;&gt;[14]&lt;/a&gt; A. Kucukelbir, R. Ranganath, A. Gelman, et al. “Automatic Variational Inference in Stan”. In: &lt;em&gt;Advances in Neural Information Processing Systems 28&lt;/em&gt;. Ed. by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama and R. Garnett. Curran Associates, Inc., 2015, pp. 568-576. URL: &lt;a href=&#34;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&#34; class=&#34;uri&#34;&gt;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-kubinec2017&gt;&lt;/a&gt;&lt;a href=&#34;#cite-kubinec2017&#34;&gt;[15]&lt;/a&gt; R. Kubinec. &lt;em&gt;Absence Makes the Ideal Points Sharper&lt;/em&gt;. Poster. Political Methodology Society, 2017.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>idealstan: A Package for Bayesian Ideal-Point Modeling with Stan</title>
      <link>http://www.robertkubinec.com/talk/stancon2018/</link>
      <pubDate>Thu, 11 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/talk/stancon2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Islamist-Secularist Polarization during the Arab Uprisings</title>
      <link>http://www.robertkubinec.com/talk/apsa_twitter/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/talk/apsa_twitter/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Making Democracy Safe for Business: Business Collective Action in Algeria, Egypt and Tunisia</title>
      <link>http://www.robertkubinec.com/talk/apsa_tunisia/</link>
      <pubDate>Fri, 01 Sep 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/talk/apsa_tunisia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Business Influence on Democratization</title>
      <link>http://www.robertkubinec.com/project/dissertation/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/project/dissertation/</guid>
      <description>&lt;p&gt;In my dissertation, I theorize that pro-authoritarian coalitions are more successful at undermining transitional democracies when elites who make up the coalition are strongly unified around the goal of regime change as opposed to lesser political aims. In Tunisia, where I undertook field research for eight months, I found that there are plenty of elites who pine for a return to the days of the dictator Ben Ali, and that these elites even went so far as to form a political party, Nidaa Tounes, that obtained the greatest numbers of seats in Tunisia&amp;rsquo;s parliament in 2014. However, this party has failed in many of its anti-democratic aims, such as passing legislation granting amnesty to corrupt businessmen from the prior regime. By contrast, in Egypt, a pro-authoritarian coalition led by the military has brought about a stable and increasingly repressive dictatorship that has stamped out all dissent. I argue that in Egypt, the economic influence of the military helped keep potential rivals, namely powerful businesspeople, strictly in-line with the military-led coalition, securing a stable transition to dictatorship. Tunisia, by contrast, lacks such an actor capable of preventing elites from squabbling with each other and undermining the success of the pro-authoritarian Nidaa Tounes.&lt;/p&gt;

&lt;p&gt;Based on my field research, I implemented an online survey targeted at businesspeople in Tunisia, Algeria and Egypt with a sample of nearly 2,500 business managers and employees. I use an embedded experiment to see how party appeals to businesses, such as for protection from expropriation or for perks from government licenses and contracts, stimulate businesspeople to offer support to parties, such as electoral funding or instructing their employees to vote for a candidate. I find that firm&amp;rsquo;s incentives to engage politically vary on the firm&amp;rsquo;s market position, size and relationship vis-a-vis government agencies, but that political-economic institutions, especially the Egyptian military, have a pronounced effect on increasing business political engagement in the aggregate.&lt;/p&gt;

&lt;p&gt;My work sheds light on the processes through which powerful elite coalitions are built during the chaotic periods of revolutions and democratic transitions. By so doing, I advance the field by pinpointing the conditions under which anti-democratic coalitions are successful, and when they are likely to fail and let democracy survive.&lt;/p&gt;

&lt;p&gt;Link to online survey paper &lt;a href=&#34;https://osf.io/preprints/socarxiv/mrfcu/&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why the Two-Way Fixed Effects Model Is Difficult to Interpret, and What to Do About It</title>
      <link>http://www.robertkubinec.com/project/fixed-effects/</link>
      <pubDate>Sat, 05 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/project/fixed-effects/</guid>
      <description>

&lt;h1 id=&#34;abstract&#34;&gt;Abstract&lt;/h1&gt;

&lt;p&gt;The two-way fixed effects (FE) model, an increasingly popular method for modeling time-series cross-section (TSCS) data, is substantively difficult to interpret because the model&amp;rsquo;s estimates are a complex amalgamation of variation in the over-time and cross-sectional effects. We demonstrate this complexity in the two-way FE estimate through mathematical exposition. As an illustration, we develop a novel simulation that enables us to generate TSCS data with varying over-time and cross-sectional effects and examine the behavior of the two-way FE model as these effects change. We demonstrate that the two-way FE model makes specific assumptions about TSCS datasets, and if these assumptions are not met, the model may be unidentified even if substantial variation exists along both dimensions. Because of the difficulty in interpretation, we do not recommend that applied researchers rely on the two-way FE model except for situations in which the assumptions are well-understood, such as the canonical difference-in-difference design.&lt;/p&gt;

&lt;p&gt;Link to paper &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3062619&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Absence Makes the Ideal Points Sharper: An Ordinal IRT Model for Complete-Data Rollcall Vote Modeling</title>
      <link>http://www.robertkubinec.com/project/bayesian-irt/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/project/bayesian-irt/</guid>
      <description>&lt;p&gt;Estimating legislator ideal points from roll-call voting data has become a powerful tool for understanding legislative behavior and polarization within legislatures over time. However, the dominant approaches in the field tend to ignore both absences and abstentions in roll-call votes even though it is very likely that absences and abstentions represent a form of strategic behavior. In emerging democracies like Tunisia, this lacuna is particularly problematic because legislator absence may occur very frequently, and resulting estimates that exclude absences are either very imprecise or significantly biased.&lt;/p&gt;

&lt;p&gt;As a remedy for this situation, I put forward a Bayesian IRT model that can handle legislators absences as a separate category of data for determining legislator ideal points. The estimation uses the concept of a hurdle model to deflate the probabilities of legislator&amp;rsquo;s votes by the probability of absences. The model produces a single set of ideal points, but utilizes different parameters for bill absence points. Compared to DW-NOMINATE, this model tends to produce more moderate estimates of US Congresspeople&amp;rsquo;s ideal points because it can model roll-call votes where there are very few opposing votes. For parliamentary data, the model provides much more precise estimates, especially in legislatures with very high rates of absence. Additionally, the model can incorporate absentions as a middle category between yes and no votes for legislatures with high rates of abstentions.&lt;/p&gt;

&lt;p&gt;An R package providing this model, along with other standard IRT ideal point models, is under development. Current source code is available from my &lt;a href=&#34;https://github.com/saudiwin/idealstan&#34; target=&#34;_blank&#34;&gt;Github page&lt;/a&gt;. The estimation is done via MCMC within the Stan modeling language.&lt;/p&gt;

&lt;p&gt;Link to paper &lt;a href=&#34;https://virginia.box.com/shared/static/lpot1njqfu7mlm1h5zjppn0xqhy8din6.pdf&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How foreign aid could hurt Tunisia’s transition to democracy</title>
      <link>http://www.robertkubinec.com/publication/tunisia-monkey-cage/</link>
      <pubDate>Mon, 19 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://www.robertkubinec.com/publication/tunisia-monkey-cage/</guid>
      <description>&lt;p&gt;This article was written for the Washington Post&amp;rsquo;s Monkey Cage blog based on my field research in Tunisia in 2016.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
