<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Academic</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Robert Kubinec</copyright><lastBuildDate>Wed, 22 Apr 2020 15:00:00 +0000</lastBuildDate>
    <image>
      <url>/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>What Panel Data Is Really All About</title>
      <link>/post/fixed_effects/</link>
      <pubDate>Wed, 22 Apr 2020 15:00:00 +0000</pubDate>
      <guid>/post/fixed_effects/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;We’ve all been in that seminar where the author puts up a slide containing regression coefficients, and buried in the bottom line of the table we can see little Ys and Ns indicating the kind of panel data model employed. Quite often, these brief descriptions of models are taken as a mere statistical ritual, believed to be efficacious at appeasing the mercurial statistical deities, but rarely if ever investigated more thoroughly. Furthermore, no one wants to be in the difficult position of discussing panel data models, which inevitably boils down to a conversation laced with econometric terms generating more heat than light.&lt;/p&gt;
&lt;p&gt;So what if I told you … panel data, or data with two dimensions, such as repeated observations on multiple cases over time, is really not that complicated. In fact, there are only two basic ways to analyze panel data, which I will explain briefly in this piece, just as every panel dataset has two basic dimensions (cases and time). However, when we confuse these dimensions, bad things can happen. In fact, one of the most popular panel data models, the two-way fixed effects model–widely used in the social sciences–is in fact statistical nonsense because it does not clearly distinguish between these two dimensions. This statement should sound implausible to you–really?, but it’s quite easy to demonstrate, as I’ll show you in this post.&lt;/p&gt;
&lt;p&gt;This blog post is based on a recent publication with Jonathan Kropko which you can &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231349&#34;&gt;access here&lt;/a&gt;. In this post I provide a more reader-friendly overview of the article, dropping the academic-ese and focusing on substance as I think there are important issues for many people doing research.&lt;/p&gt;
&lt;p&gt;In short: &lt;strong&gt;there are an untold number of analyses of panel data affected by an issue that is almost impossible to identify because R and Stata obscure the problem.&lt;/strong&gt; Thanks to multi-collinearity checks that automatically drop predictors in regression models, a two-way fixed effects model can produce sensible-looking results that are not just irrelevant to the question at hand, but practically nonsense. Instead, we would all be better served by using simpler 1-way fixed effects models (intercepts on time points or cases/subjects, but not both).&lt;/p&gt;
&lt;div id=&#34;how-we-think-about-panel-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How We Think About Panel Data&lt;/h2&gt;
&lt;p&gt;Panel data is one of the richest resources we have for observational inference in the social and even natural sciences. By comparing cases to each other as they change over time, such as countries and gross domestic product (GDP), we can learn much more than we can by only examining isolated cases. However, researchers for decades have been told by econometricians and applied statisticians that they &lt;em&gt;must&lt;/em&gt; use a certain kind of panel data model or risk committing grave inferential errors. As a result, probably the most common panel data model is to include case fixed effects, or a unique intercept (i.e. a dummy variable) for every case in the panel data model. The second most likely is to include a fixed effect or intercept for every time point &lt;em&gt;and&lt;/em&gt; case in the data. The belief is that including all these intercepts will control for omitted variables because there are factors unique to cases or time points that will be “controlled for” by these intercepts.&lt;/p&gt;
&lt;p&gt;The result of this emphasis on the lurking dangers of panel data inference results in what I have decided to call the Cheeseburger Syndrome. This phenomenon was first studied by Saturday Night Live in a classic 1980s skit:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/puJePACBoIo&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Applied researchers are in the position of customers in the video, continually asking for models that will analyze the variation in their panel data which actually exists. Applied statisticians are often the cooks, informing customers that regardless of what particular dataset they have, they will only ever be able to get a “cheeseburger.” As a result, there is remarkable uniformity of application of panel data models in the social sciences, even if these models don’t always fit the data very well.&lt;/p&gt;
&lt;p&gt;What we put forward in our piece linked above is that any statistical model should have, as its first requirement, that it match the researcher’s question. Problems of omitted variables are important, but necessarily secondary. It does not matter how good the cheeseburger is if the researcher really wants eggs easy over.&lt;/p&gt;
&lt;p&gt;In addition, fixed effects models &lt;em&gt;do not&lt;/em&gt; control for omitted variables. What fixed effect models do is isolate one dimension of variance in the model. As a result, any variables that don’t vary on that dimension are by definition removed from the model. This side-effect is trumpeted as the great inferential benefit of fixed effect models, but it has nothing to do with inference. Fixed effects (or their cousin, random effects/hierarchical models) are simply about selecting which part of the panel dataset is most germane to the analysis.&lt;/p&gt;
&lt;p&gt;The rule of thumb that we put forward in our paper is that fixed effects/dummy variables/intercepts on cases correspond to the following research question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How much does a case change in relation to itself over time?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While fixed effects/dummy variables/intercepts on time points correspond to the following research question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How much does a case change relative to other cases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some questions are fruitfully asked when comparing cases to themselves over time. For example, if a case is a human being, we might want to know whether obtaining more education leads to higher earnings. Conversely, if a case is a country, we might want to know if wealthier countries tend to be more democratic than poorer countries. Some questions primarily employ within-case variation, while others look at cross-sectional variation. Both are present in any panel dataset, and both are potentially interesting.&lt;/p&gt;
&lt;p&gt;If fixed effects enable &lt;em&gt;comparisons&lt;/em&gt;, then what happens if we have dummy variables/intercepts for every case and time point (the so-called two-way fixed effects model)? What comparison are we then making? As it turns out, the answer to this question is not very clear at all. I refer you to the paper above for a full exposition, but in essence, because cases and time points are nested, we end up making comparisons across both dimensions simultaneously, and this is just as obtuse as it sounds. There is no clear research question that matches this model.&lt;/p&gt;
&lt;p&gt;Furthermore, if the original aim was to remove omitted variables, these omitted variables inevitably end up in the estimate again because a two-way estimate necessarily relates to both dimensions of variance simultaneously. As a result, it is not very clear what the point is. The one known use of the model is for difference-in-difference estimation, but only with two time points (see our paper for more detail).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exposition-with-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exposition with Data&lt;/h2&gt;
&lt;p&gt;This all may sound to you like a nice story. But how can you really know we are telling the truth? There are plenty of equations in our paper, but there is nothing like being able to see the data. One of the contributions of our paper is to create a simulation for panel data where we can control the within-case and cross-sectional variation separately for the same panel data set. This allows us to compare all of the possible panel data models with the same dataset while including or excluding omitted variables and other issues.&lt;/p&gt;
&lt;p&gt;To show you how this works, I will generate a dataset with a single covariate in which the effect of that covariate is +1 for within-case comparisons and -3 for cross-sectional comparisons. There is noise in the data, but the effect is the same for all cases and for all cross-sections. The following code simulates fifty cases and fifty time points using our &lt;code&gt;panelsim&lt;/code&gt; package (currently available only via &lt;a href=&#34;http://www.github.com/saudiwin/panelsim&#34;&gt;Github&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to install this package, use 
# remotes::install_github(&amp;quot;saudiwin/panelsim&amp;quot;)

require(panelsim)

# generate dataset with fixed coefficients within cases and cross-section
# no variation in effects across cases or time

gen_data &amp;lt;- tw_data(N=50,
                    T=50,
                    case.eff.mean = 1,
                    cross.eff.mean = -3,
                    cross.eff.sd = 0,
                    case.eff.sd = 0,
                    noise.sd=.25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because there is only one covariate, we can pretty easily visualize the relationships in the cross sectional and within-case variation. First, the value of the outcome/response &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is shown for five cases, where one dot represents the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; for each time point for that case:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_data$data %&amp;gt;%
  filter(case&amp;lt;5) %&amp;gt;% 
  ggplot(aes(x=x,y=y)) +
           geom_point() +
           stat_smooth(method=&amp;quot;lm&amp;quot;) +
           facet_wrap(~case,scales=&amp;quot;free&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fixed_effects_files/figure-html/case_out-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, there is a consistent positive relationship of approximately +1 between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; within cases. We can also examine the relationship for the cross-section by subsetting the data to each time point and plotting the cases for five of the time points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_data$data %&amp;gt;%
  filter(time&amp;lt;5) %&amp;gt;% 
  ggplot(aes(x=x,y=y)) +
           geom_point() +
           stat_smooth(method=&amp;quot;lm&amp;quot;) +
           facet_wrap(~time,scales=&amp;quot;free&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/fixed_effects_files/figure-html/time_out-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a separate and quite distinct relationship in the cross section between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Both components are present in the outcome. To find the actual coefficient, we can simply fit linear regression models on the generated data, first with intercepts/dummies for cases:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y ~ x + factor(case),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(case), data = gen_data$data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8932 -0.1599 -0.0015  0.1682  0.8304 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     0.773045   0.035151  21.992  &amp;lt; 2e-16 ***
## x               1.002403   0.018501  54.182  &amp;lt; 2e-16 ***
## factor(case)2  -0.184927   0.049529  -3.734 0.000193 ***
## factor(case)3   1.466627   0.050043  29.307  &amp;lt; 2e-16 ***
## factor(case)4  -1.363988   0.049898 -27.335  &amp;lt; 2e-16 ***
## factor(case)5  -0.884801   0.049660 -17.817  &amp;lt; 2e-16 ***
## factor(case)6   1.097485   0.049812  22.032  &amp;lt; 2e-16 ***
## factor(case)7  -0.992114   0.049705 -19.960  &amp;lt; 2e-16 ***
## factor(case)8  -1.597938   0.050057 -31.922  &amp;lt; 2e-16 ***
## factor(case)9  -2.126106   0.050445 -42.147  &amp;lt; 2e-16 ***
## factor(case)10 -0.028781   0.049525  -0.581 0.561205    
## factor(case)11 -1.171168   0.049831 -23.503  &amp;lt; 2e-16 ***
## factor(case)12 -0.132703   0.049526  -2.679 0.007423 ** 
## factor(case)13 -0.612141   0.049588 -12.344  &amp;lt; 2e-16 ***
## factor(case)14  0.866259   0.049705  17.428  &amp;lt; 2e-16 ***
## factor(case)15 -0.867871   0.049670 -17.473  &amp;lt; 2e-16 ***
## factor(case)16  0.514860   0.049600  10.380  &amp;lt; 2e-16 ***
## factor(case)17 -0.633858   0.049621 -12.774  &amp;lt; 2e-16 ***
## factor(case)18 -0.696507   0.049617 -14.038  &amp;lt; 2e-16 ***
## factor(case)19  0.806729   0.049680  16.239  &amp;lt; 2e-16 ***
## factor(case)20 -0.764169   0.049643 -15.393  &amp;lt; 2e-16 ***
## factor(case)21 -0.597111   0.049591 -12.041  &amp;lt; 2e-16 ***
## factor(case)22 -1.405450   0.049928 -28.150  &amp;lt; 2e-16 ***
## factor(case)23  0.261895   0.049546   5.286 1.36e-07 ***
## factor(case)24  0.973181   0.049738  19.566  &amp;lt; 2e-16 ***
## factor(case)25 -0.710288   0.049618 -14.315  &amp;lt; 2e-16 ***
## factor(case)26 -0.718412   0.049631 -14.475  &amp;lt; 2e-16 ***
## factor(case)27 -0.946765   0.049698 -19.050  &amp;lt; 2e-16 ***
## factor(case)28 -3.194416   0.051604 -61.902  &amp;lt; 2e-16 ***
## factor(case)29  0.120342   0.049531   2.430 0.015184 *  
## factor(case)30 -0.965050   0.049712 -19.413  &amp;lt; 2e-16 ***
## factor(case)31 -1.108207   0.049794 -22.256  &amp;lt; 2e-16 ***
## factor(case)32 -0.888764   0.049668 -17.894  &amp;lt; 2e-16 ***
## factor(case)33 -0.920184   0.049700 -18.515  &amp;lt; 2e-16 ***
## factor(case)34  0.588541   0.049613  11.863  &amp;lt; 2e-16 ***
## factor(case)35 -0.008302   0.049525  -0.168 0.866881    
## factor(case)36 -1.068951   0.049757 -21.484  &amp;lt; 2e-16 ***
## factor(case)37 -2.534691   0.050831 -49.865  &amp;lt; 2e-16 ***
## factor(case)38 -0.699826   0.049616 -14.105  &amp;lt; 2e-16 ***
## factor(case)39  0.389961   0.049578   7.866 5.46e-15 ***
## factor(case)40 -1.291089   0.049878 -25.885  &amp;lt; 2e-16 ***
## factor(case)41 -2.527182   0.050822 -49.726  &amp;lt; 2e-16 ***
## factor(case)42 -1.718180   0.050056 -34.325  &amp;lt; 2e-16 ***
## factor(case)43  0.539154   0.049589  10.872  &amp;lt; 2e-16 ***
## factor(case)44  0.946498   0.049741  19.029  &amp;lt; 2e-16 ***
## factor(case)45 -1.672768   0.050093 -33.393  &amp;lt; 2e-16 ***
## factor(case)46 -1.924235   0.050325 -38.236  &amp;lt; 2e-16 ***
## factor(case)47  0.297708   0.049551   6.008 2.16e-09 ***
## factor(case)48  0.045939   0.049527   0.928 0.353731    
## factor(case)49  1.051133   0.049777  21.117  &amp;lt; 2e-16 ***
## factor(case)50 -1.102510   0.049764 -22.155  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2476 on 2449 degrees of freedom
## Multiple R-squared:  0.9173, Adjusted R-squared:  0.9156 
## F-statistic: 543.2 on 50 and 2449 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see in the regression coefficient above that the coefficient for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is almost exactly +1.&lt;/p&gt;
&lt;p&gt;Next we can fit a model with cases/intercepts for time points (cross-sectional variation):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y ~ x + factor(time),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(time), data = gen_data$data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9114 -0.1674 -0.0046  0.1699  0.7753 
## 
## Coefficients:
##                Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## (Intercept)    -0.68479    0.03530  -19.400  &amp;lt; 2e-16 ***
## x              -2.99280    0.01924 -155.534  &amp;lt; 2e-16 ***
## factor(time)2   1.51868    0.05005   30.345  &amp;lt; 2e-16 ***
## factor(time)3   2.26458    0.05073   44.637  &amp;lt; 2e-16 ***
## factor(time)4  -1.04731    0.04975  -21.050  &amp;lt; 2e-16 ***
## factor(time)5   1.73263    0.05025   34.482  &amp;lt; 2e-16 ***
## factor(time)6   1.26293    0.04988   25.319  &amp;lt; 2e-16 ***
## factor(time)7   0.06635    0.04952    1.340  0.18044    
## factor(time)8   1.67967    0.05018   33.472  &amp;lt; 2e-16 ***
## factor(time)9  -0.75507    0.04964  -15.211  &amp;lt; 2e-16 ***
## factor(time)10 -0.55308    0.04957  -11.157  &amp;lt; 2e-16 ***
## factor(time)11  1.34417    0.04996   26.905  &amp;lt; 2e-16 ***
## factor(time)12  2.20623    0.05064   43.565  &amp;lt; 2e-16 ***
## factor(time)13  0.02975    0.04952    0.601  0.54803    
## factor(time)14  0.10429    0.04952    2.106  0.03530 *  
## factor(time)15  0.27429    0.04955    5.536 3.43e-08 ***
## factor(time)16  1.05558    0.04981   21.193  &amp;lt; 2e-16 ***
## factor(time)17 -0.40503    0.04954   -8.175 4.68e-16 ***
## factor(time)18  0.86367    0.04971   17.375  &amp;lt; 2e-16 ***
## factor(time)19  1.57862    0.05013   31.492  &amp;lt; 2e-16 ***
## factor(time)20 -0.01728    0.04952   -0.349  0.72717    
## factor(time)21  0.62057    0.04961   12.508  &amp;lt; 2e-16 ***
## factor(time)22 -1.38478    0.04995  -27.721  &amp;lt; 2e-16 ***
## factor(time)23  0.31574    0.04954    6.374 2.20e-10 ***
## factor(time)24  2.96579    0.05157   57.515  &amp;lt; 2e-16 ***
## factor(time)25  0.24178    0.04953    4.881 1.12e-06 ***
## factor(time)26  2.83592    0.05144   55.133  &amp;lt; 2e-16 ***
## factor(time)27 -0.86284    0.04968  -17.369  &amp;lt; 2e-16 ***
## factor(time)28  0.92505    0.04975   18.594  &amp;lt; 2e-16 ***
## factor(time)29  1.05202    0.04980   21.125  &amp;lt; 2e-16 ***
## factor(time)30  0.15122    0.04953    3.053  0.00229 ** 
## factor(time)31  1.81587    0.05033   36.082  &amp;lt; 2e-16 ***
## factor(time)32  1.38787    0.04999   27.763  &amp;lt; 2e-16 ***
## factor(time)33  1.87706    0.05038   37.257  &amp;lt; 2e-16 ***
## factor(time)34  1.76956    0.05029   35.184  &amp;lt; 2e-16 ***
## factor(time)35  1.55739    0.05016   31.048  &amp;lt; 2e-16 ***
## factor(time)36  0.57794    0.04960   11.653  &amp;lt; 2e-16 ***
## factor(time)37  0.10647    0.04952    2.150  0.03166 *  
## factor(time)38 -0.72418    0.04962  -14.594  &amp;lt; 2e-16 ***
## factor(time)39  1.80861    0.05033   35.938  &amp;lt; 2e-16 ***
## factor(time)40  0.24163    0.04953    4.879 1.14e-06 ***
## factor(time)41  1.27448    0.04994   25.521  &amp;lt; 2e-16 ***
## factor(time)42  1.82843    0.05029   36.361  &amp;lt; 2e-16 ***
## factor(time)43  1.57770    0.05011   31.487  &amp;lt; 2e-16 ***
## factor(time)44  1.50850    0.05006   30.135  &amp;lt; 2e-16 ***
## factor(time)45  0.23318    0.04953    4.708 2.64e-06 ***
## factor(time)46 -0.92491    0.04969  -18.613  &amp;lt; 2e-16 ***
## factor(time)47  1.97346    0.05044   39.125  &amp;lt; 2e-16 ***
## factor(time)48  0.60207    0.04961   12.135  &amp;lt; 2e-16 ***
## factor(time)49  0.75674    0.04964   15.244  &amp;lt; 2e-16 ***
## factor(time)50 -1.38077    0.04996  -27.637  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2476 on 2449 degrees of freedom
## Multiple R-squared:  0.9173, Adjusted R-squared:  0.9156 
## F-statistic: 543.4 on 50 and 2449 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the estimated coefficient is almost exactly what we generated. Success!&lt;/p&gt;
&lt;p&gt;However, this brings us back to one of the questions we started with. We know the within-case relationship and the cross-sectional relationship, so what happens if we put dummies/intercepts on both cases and time? Well let’s find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y~x + factor(case) + factor(time),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.87158 -0.16705 -0.00396  0.16311  0.78026 
## 
## Coefficients: (1 not defined because of singularities)
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     0.806372   0.087811   9.183  &amp;lt; 2e-16 ***
## x               1.000661   0.143392   6.978 3.84e-12 ***
## factor(case)2  -0.184865   0.049834  -3.710 0.000212 ***
## factor(case)3   1.465951   0.074525  19.671  &amp;lt; 2e-16 ***
## factor(case)4  -1.363415   0.068454 -19.917  &amp;lt; 2e-16 ***
## factor(case)5  -0.884457   0.057091 -15.492  &amp;lt; 2e-16 ***
## factor(case)6   1.096983   0.064585  16.985  &amp;lt; 2e-16 ***
## factor(case)7  -0.991716   0.059427 -16.688  &amp;lt; 2e-16 ***
## factor(case)8  -1.597253   0.075111 -21.265  &amp;lt; 2e-16 ***
## factor(case)9  -2.125203   0.089352 -23.785  &amp;lt; 2e-16 ***
## factor(case)10 -0.028795   0.049592  -0.581 0.561545    
## factor(case)11 -1.170649   0.065430 -17.892  &amp;lt; 2e-16 ***
## factor(case)12 -0.132674   0.049635  -2.673 0.007568 ** 
## factor(case)13 -0.611905   0.053240 -11.493  &amp;lt; 2e-16 ***
## factor(case)14  0.865860   0.059440  14.567  &amp;lt; 2e-16 ***
## factor(case)15 -0.867514   0.057639 -15.051  &amp;lt; 2e-16 ***
## factor(case)16  0.514605   0.053863   9.554  &amp;lt; 2e-16 ***
## factor(case)17 -0.633567   0.055045 -11.510  &amp;lt; 2e-16 ***
## factor(case)18 -0.696223   0.054818 -12.701  &amp;lt; 2e-16 ***
## factor(case)19  0.806361   0.058129  13.872  &amp;lt; 2e-16 ***
## factor(case)20 -0.763847   0.056206 -13.590  &amp;lt; 2e-16 ***
## factor(case)21 -0.596870   0.053399 -11.178  &amp;lt; 2e-16 ***
## factor(case)22 -1.404855   0.069741 -20.144  &amp;lt; 2e-16 ***
## factor(case)23  0.261759   0.050838   5.149 2.83e-07 ***
## factor(case)24  0.972749   0.061053  15.933  &amp;lt; 2e-16 ***
## factor(case)25 -0.710003   0.054856 -12.943  &amp;lt; 2e-16 ***
## factor(case)26 -0.718107   0.055565 -12.924  &amp;lt; 2e-16 ***
## factor(case)27 -0.946375   0.059073 -16.020  &amp;lt; 2e-16 ***
## factor(case)28 -3.193052   0.122837 -25.994  &amp;lt; 2e-16 ***
## factor(case)29  0.120274   0.049902   2.410 0.016019 *  
## factor(case)30 -0.964644   0.059789 -16.134  &amp;lt; 2e-16 ***
## factor(case)31 -1.107721   0.063746 -17.377  &amp;lt; 2e-16 ***
## factor(case)32 -0.888409   0.057538 -15.440  &amp;lt; 2e-16 ***
## factor(case)33 -0.919792   0.059160 -15.548  &amp;lt; 2e-16 ***
## factor(case)34  0.588263   0.054612  10.772  &amp;lt; 2e-16 ***
## factor(case)35 -0.008290   0.049589  -0.167 0.867240    
## factor(case)36 -1.068500   0.061960 -17.245  &amp;lt; 2e-16 ***
## factor(case)37 -2.533613   0.101653 -24.924  &amp;lt; 2e-16 ***
## factor(case)38 -0.699544   0.054758 -12.775  &amp;lt; 2e-16 ***
## factor(case)39  0.389746   0.052653   7.402 1.84e-13 ***
## factor(case)40 -1.290532   0.067565 -19.101  &amp;lt; 2e-16 ***
## factor(case)41 -2.526108   0.101383 -24.916  &amp;lt; 2e-16 ***
## factor(case)42 -1.717495   0.075071 -22.878  &amp;lt; 2e-16 ***
## factor(case)43  0.538917   0.053290  10.113  &amp;lt; 2e-16 ***
## factor(case)44  0.946063   0.061202  15.458  &amp;lt; 2e-16 ***
## factor(case)45 -1.672061   0.076523 -21.850  &amp;lt; 2e-16 ***
## factor(case)46 -1.923394   0.085197 -22.576  &amp;lt; 2e-16 ***
## factor(case)47  0.297558   0.051103   5.823 6.56e-09 ***
## factor(case)48  0.045901   0.049675   0.924 0.355567    
## factor(case)49  1.050663   0.062931  16.696  &amp;lt; 2e-16 ***
## factor(case)50 -1.102052   0.062325 -17.682  &amp;lt; 2e-16 ***
## factor(time)2   0.009865   0.089885   0.110 0.912618    
## factor(time)3  -0.027398   0.115374  -0.237 0.812314    
## factor(time)4  -0.039227   0.044426  -0.883 0.377342    
## factor(time)5  -0.038629   0.098266  -0.393 0.694277    
## factor(time)6   0.015206   0.081771   0.186 0.852496    
## factor(time)7  -0.052421   0.051843  -1.011 0.312051    
## factor(time)8  -0.009808   0.095634  -0.103 0.918319    
## factor(time)9  -0.029774   0.042955  -0.693 0.488287    
## factor(time)10 -0.073201   0.043597  -1.679 0.093275 .  
## factor(time)11 -0.034386   0.085805  -0.401 0.688639    
## factor(time)12  0.003032   0.112421   0.027 0.978486    
## factor(time)13 -0.092023   0.051904  -1.773 0.076362 .  
## factor(time)14  0.017896   0.051201   0.350 0.726720    
## factor(time)15 -0.099884   0.057487  -1.738 0.082424 .  
## factor(time)16 -0.059046   0.077744  -0.759 0.447632    
## factor(time)17 -0.066464   0.044756  -1.485 0.137669    
## factor(time)18 -0.039199   0.071529  -0.548 0.583733    
## factor(time)19 -0.040240   0.093375  -0.431 0.666546    
## factor(time)20 -0.019978   0.049628  -0.403 0.687311    
## factor(time)21 -0.021785   0.064293  -0.339 0.734760    
## factor(time)22 -0.016429   0.049358  -0.333 0.739273    
## factor(time)23  0.016278   0.055739   0.292 0.770279    
## factor(time)24 -0.021054   0.138843  -0.152 0.879486    
## factor(time)25 -0.016068   0.054798  -0.293 0.769382    
## factor(time)26 -0.054303   0.135548  -0.401 0.688739    
## factor(time)27 -0.034151   0.043223  -0.790 0.429545    
## factor(time)28 -0.075377   0.074360  -1.014 0.310839    
## factor(time)29 -0.046340   0.077258  -0.600 0.548686    
## factor(time)30 -0.062577   0.053830  -1.162 0.245150    
## factor(time)31 -0.049767   0.101325  -0.491 0.623362    
## factor(time)32 -0.037785   0.087273  -0.433 0.665091    
## factor(time)33 -0.052008   0.103393  -0.503 0.614999    
## factor(time)34 -0.059567   0.100140  -0.595 0.552009    
## factor(time)35 -0.104873   0.094761  -1.107 0.268534    
## factor(time)36 -0.010170   0.062857  -0.162 0.871484    
## factor(time)37 -0.082164   0.053289  -1.542 0.123243    
## factor(time)38 -0.057402   0.042945  -1.337 0.181464    
## factor(time)39 -0.056943   0.101323  -0.562 0.574169    
## factor(time)40  0.007376   0.054276   0.136 0.891908    
## factor(time)41 -0.070988   0.084778  -0.837 0.402484    
## factor(time)42  0.010645   0.099772   0.107 0.915043    
## factor(time)43 -0.013565   0.092496  -0.147 0.883418    
## factor(time)44 -0.015229   0.090355  -0.169 0.866166    
## factor(time)45  0.010920   0.054013   0.202 0.839797    
## factor(time)46 -0.060932   0.043387  -1.404 0.160328    
## factor(time)47 -0.020632   0.105521  -0.196 0.844996    
## factor(time)48 -0.038896   0.064256  -0.605 0.545015    
## factor(time)49  0.028808   0.066612   0.432 0.665437    
## factor(time)50        NA         NA      NA       NA    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2479 on 2401 degrees of freedom
## Multiple R-squared:  0.9187, Adjusted R-squared:  0.9154 
## F-statistic:   277 on 98 and 2401 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might expect that perhaps the two-way coefficient would be somewhere between the cross-sectional and within-case coefficients. Nope. It’s equal in this simulation to roughly 1, but that depends on how much variance there is in the cross-section and the within-case components of the dataset as they are now being combined in a non-linear fashion (if you can’t wrap your mind around this, don’t worry, it’s nearly impossible to do and unfruitful if you do accomplish it). However, the coefficient is still equal to a value that appears reasonable, although it is much less precise, and we might be willing to traipse along with it. However, you should pay attention to what happened in the model summary above–one of the time dummies (&lt;code&gt;factor(time)50&lt;/code&gt;) came back with a missing (&lt;code&gt;NA&lt;/code&gt;) coefficient! What is that about?&lt;/p&gt;
&lt;p&gt;I’m so glad you asked. As we mentioned earlier, R has the ability to check for multi-collinearity in the predictor variables to avoid getting errors when attempting to estimate linear regression. Multi-collinearity can arise for reasons that have nothing to do with fixed effects, such as accidentally including variables that are sums of each other, etc. It often happens in fixed effects regression models when people try to include variables that only vary in the cross-section when using case fixed effects (a symptom of the Cheeseburger Syndrome mentioned previously). In this instance, though, there is no reason to think that there is multi-collinearity as we &lt;em&gt;generated&lt;/em&gt; the data with two continuous variables (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;). The data are perfect with no missingness. So how could this happen?&lt;/p&gt;
&lt;p&gt;This discovery is one of the central contributions of our paper. Whenever panel data has a single effect for the cross-section and for within-case variation–as is the dominant way of thinking about panel data–the two-way fixed effects coefficient is statistically unidentified. That is, trying to estimate it is equivalent to saying &lt;code&gt;2 + 2 = 5&lt;/code&gt; or the earth is as round as a square. As we show in the paper, it algebraically reduces to dividing by zero. R magically saved the day by dropping a variable, but we can show what would happen if R had not intervened.&lt;/p&gt;
&lt;p&gt;In the code below I manually estimate a regression using the matrix inversion formula, &lt;span class=&#34;math inline&#34;&gt;\((X^TX)^{-1}X^Ty\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; in this case is all of our predictor variables, including the case/time dummies:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(y~x + factor(case) + factor(time),data=gen_data$data)
y &amp;lt;- gen_data$data$y
try(solve(t(X)%*%X)%*%t(X)%*%y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in solve.default(t(X) %*% X) : 
##   system is computationally singular: reciprocal condition number = 5.63975e-19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the error message: the system (matrix computation of OLS regression) is computationally singular. You’ll need to see the paper for how this all breaks down, but essentially the question you are putting to R is nonsensical. You simply cannot estimate a single joint effect while including all the variables. Instead, you need to drop one, which essentially means the effect is relative to the whichever intercept happened to be dropped (in this case time point 50). The coefficient could change if we simply re-arrange the labeling of the time fixed effects, as in the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_data$data$time &amp;lt;- as.numeric(factor(gen_data$data$time,levels=sample(unique(gen_data$data$time))))
summary(lm(y~x + factor(case) + factor(time),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.87158 -0.16705 -0.00396  0.16311  0.78026 
## 
## Coefficients: (1 not defined because of singularities)
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     0.827062   0.058816  14.062  &amp;lt; 2e-16 ***
## x               0.892381   0.169360   5.269 1.49e-07 ***
## factor(case)2  -0.181065   0.049934  -3.626 0.000294 ***
## factor(case)3   1.423934   0.082322  17.297  &amp;lt; 2e-16 ***
## factor(case)4  -1.327772   0.074606 -17.797  &amp;lt; 2e-16 ***
## factor(case)5  -0.863081   0.059798 -14.433  &amp;lt; 2e-16 ***
## factor(case)6   1.065727   0.069627  15.306  &amp;lt; 2e-16 ***
## factor(case)7  -0.966975   0.062893 -15.375  &amp;lt; 2e-16 ***
## factor(case)8  -1.554645   0.083062 -18.717  &amp;lt; 2e-16 ***
## factor(case)9  -2.069070   0.100829 -20.521  &amp;lt; 2e-16 ***
## factor(case)10 -0.029652   0.049597  -0.598 0.549997    
## factor(case)11 -1.138408   0.070718 -16.098  &amp;lt; 2e-16 ***
## factor(case)12 -0.130897   0.049657  -2.636 0.008442 ** 
## factor(case)13 -0.597254   0.054619 -10.935  &amp;lt; 2e-16 ***
## factor(case)14  0.841102   0.062910  13.370  &amp;lt; 2e-16 ***
## factor(case)15 -0.845316   0.060527 -13.966  &amp;lt; 2e-16 ***
## factor(case)16  0.498709   0.055464   8.992  &amp;lt; 2e-16 ***
## factor(case)17 -0.615509   0.057060 -10.787  &amp;lt; 2e-16 ***
## factor(case)18 -0.678563   0.056754 -11.956  &amp;lt; 2e-16 ***
## factor(case)19  0.783446   0.061177  12.806  &amp;lt; 2e-16 ***
## factor(case)20 -0.743853   0.058618 -12.690  &amp;lt; 2e-16 ***
## factor(case)21 -0.581894   0.054834 -10.612  &amp;lt; 2e-16 ***
## factor(case)22 -1.367817   0.076249 -17.939  &amp;lt; 2e-16 ***
## factor(case)23  0.253267   0.051327   4.934 8.59e-07 ***
## factor(case)24  0.945844   0.065030  14.545  &amp;lt; 2e-16 ***
## factor(case)25 -0.692276   0.056805 -12.187  &amp;lt; 2e-16 ***
## factor(case)26 -0.699163   0.057758 -12.105  &amp;lt; 2e-16 ***
## factor(case)27 -0.922121   0.062427 -14.771  &amp;lt; 2e-16 ***
## factor(case)28 -3.108184   0.141697 -21.936  &amp;lt; 2e-16 ***
## factor(case)29  0.115992   0.050029   2.318 0.020507 *  
## factor(case)30 -0.939410   0.063371 -14.824  &amp;lt; 2e-16 ***
## factor(case)31 -1.077463   0.068540 -15.720  &amp;lt; 2e-16 ***
## factor(case)32 -0.866360   0.060394 -14.345  &amp;lt; 2e-16 ***
## factor(case)33 -0.895419   0.062541 -14.317  &amp;lt; 2e-16 ***
## factor(case)34  0.570972   0.056476  10.110  &amp;lt; 2e-16 ***
## factor(case)35 -0.007546   0.049593  -0.152 0.879081    
## factor(case)36 -1.040438   0.066215 -15.713  &amp;lt; 2e-16 ***
## factor(case)37 -2.466600   0.115948 -21.273  &amp;lt; 2e-16 ***
## factor(case)38 -0.681990   0.056674 -12.034  &amp;lt; 2e-16 ***
## factor(case)39  0.376359   0.053819   6.993 3.47e-12 ***
## factor(case)40 -1.255870   0.073466 -17.095  &amp;lt; 2e-16 ***
## factor(case)41 -2.459329   0.115618 -21.271  &amp;lt; 2e-16 ***
## factor(case)42 -1.674928   0.083011 -20.177  &amp;lt; 2e-16 ***
## factor(case)43  0.524163   0.054686   9.585  &amp;lt; 2e-16 ***
## factor(case)44  0.918966   0.065225  14.089  &amp;lt; 2e-16 ***
## factor(case)45 -1.628044   0.084840 -19.190  &amp;lt; 2e-16 ***
## factor(case)46 -1.871074   0.095680 -19.556  &amp;lt; 2e-16 ***
## factor(case)47  0.288205   0.051692   5.575 2.75e-08 ***
## factor(case)48  0.043567   0.049713   0.876 0.380918    
## factor(case)49  1.021395   0.067481  15.136  &amp;lt; 2e-16 ***
## factor(case)50 -1.073533   0.066692 -16.097  &amp;lt; 2e-16 ***
## factor(time)2  -0.069009   0.100571  -0.686 0.492671    
## factor(time)3  -0.108184   0.073336  -1.475 0.140295    
## factor(time)4   0.002837   0.043759   0.065 0.948307    
## factor(time)5  -0.011388   0.082981  -0.137 0.890857    
## factor(time)6  -0.079838   0.110453  -0.723 0.469861    
## factor(time)7  -0.066293   0.048598  -1.364 0.172660    
## factor(time)8  -0.135578   0.123915  -1.094 0.274015    
## factor(time)9  -0.149671   0.096133  -1.557 0.119623    
## factor(time)10 -0.135415   0.137057  -0.988 0.323244    
## factor(time)11 -0.046205   0.101477  -0.455 0.648917    
## factor(time)12 -0.094439   0.062068  -1.522 0.128252    
## factor(time)13 -0.056940   0.061062  -0.933 0.351172    
## factor(time)14 -0.074651   0.076729  -0.973 0.330690    
## factor(time)15 -0.064301   0.086107  -0.747 0.455286    
## factor(time)16 -0.148654   0.105819  -1.405 0.160210    
## factor(time)17 -0.109373   0.139416  -0.785 0.432820    
## factor(time)18 -0.023932   0.052512  -0.456 0.648610    
## factor(time)19 -0.054156   0.088109  -0.615 0.538842    
## factor(time)20 -0.035535   0.098979  -0.359 0.719611    
## factor(time)21 -0.116553   0.143594  -0.812 0.417053    
## factor(time)22 -0.010896   0.065124  -0.167 0.867139    
## factor(time)23 -0.030352   0.055015  -0.552 0.581207    
## factor(time)24 -0.097372   0.166084  -0.586 0.557742    
## factor(time)25 -0.033847   0.056850  -0.595 0.551644    
## factor(time)26 -0.144292   0.145023  -0.995 0.319857    
## factor(time)27 -0.076492   0.070012  -1.093 0.274696    
## factor(time)28 -0.081450   0.086159  -0.945 0.344576    
## factor(time)29 -0.059062   0.059659  -0.990 0.322275    
## factor(time)30 -0.069904   0.049340  -1.417 0.156679    
## factor(time)31 -0.116713   0.102264  -1.141 0.253863    
## factor(time)32 -0.050535   0.050595  -0.999 0.317977    
## factor(time)33 -0.025185   0.043191  -0.583 0.559876    
## factor(time)34 -0.026497   0.046296  -0.572 0.567147    
## factor(time)35 -0.059114   0.048596  -1.216 0.223940    
## factor(time)36 -0.109133   0.105935  -1.030 0.303025    
## factor(time)37 -0.119734   0.053185  -2.251 0.024457 *  
## factor(time)38 -0.042987   0.101938  -0.422 0.673288    
## factor(time)39 -0.035869   0.047662  -0.753 0.451788    
## factor(time)40 -0.113464   0.165575  -0.685 0.493239    
## factor(time)41 -0.059933   0.110559  -0.542 0.587805    
## factor(time)42 -0.009158   0.057266  -0.160 0.872963    
## factor(time)43 -0.126494   0.150871  -0.838 0.401878    
## factor(time)44 -0.059636   0.047396  -1.258 0.208424    
## factor(time)45 -0.039694   0.107192  -0.370 0.711183    
## factor(time)46 -0.056278   0.054291  -1.037 0.300026    
## factor(time)47 -0.146145   0.129554  -1.128 0.259404    
## factor(time)48 -0.088757   0.069468  -1.278 0.201491    
## factor(time)49 -0.136982   0.103233  -1.327 0.184661    
## factor(time)50        NA         NA      NA       NA    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2479 on 2401 degrees of freedom
## Multiple R-squared:  0.9187, Adjusted R-squared:  0.9154 
## F-statistic:   277 on 98 and 2401 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the coefficient changed because we arbitrarily swapped the order of the time fixed effects, and as R will drop the last one in the case of multi-collinearity, the estimate changed.&lt;/p&gt;
&lt;p&gt;At this point, &lt;strong&gt;you should be concerned.&lt;/strong&gt; One of our frustrations with this piece is that although the working paper has been in circulation for years, no one seems to have cared about this &lt;em&gt;apparently quite important problem.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You might wonder though that you’ve seen two-way fixed effects models where this didn’t happen. As we show in the paper, the two-way FE model &lt;em&gt;is&lt;/em&gt; identified if we generate the data differently. What we have to do is use a different &lt;em&gt;effect&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for each time point/case, or what can be called a &lt;em&gt;varying slopes&lt;/em&gt; model (slope for regression coefficient). We are not varying the fixed effects/intercepts themselves, rather we are varying the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; across time points and cases. We must do this to prevent the two-way FE model from being unidentified.&lt;/p&gt;
&lt;p&gt;To demonstrate this, we will generate new data except the coefficients will vary across case and time points randomly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make the slopes vary with the .sd parameters
gen_data &amp;lt;- tw_data(N=20,
                    T=50,
                    case.eff.mean = 1,
                    cross.eff.mean = -1,
                    cross.eff.sd =.25,
                    case.eff.sd =1,
                    noise.sd=1)

summary(lm(y~x + factor(case) + factor(time),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5458 -0.7073  0.0026  0.6446  3.1709 
## 
## Coefficients:
##                 Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## (Intercept)    -0.673576   0.264996   -2.542 0.011188 *  
## x              -0.812224   0.005523 -147.067  &amp;lt; 2e-16 ***
## factor(case)2   0.431969   0.201817    2.140 0.032582 *  
## factor(case)3   0.169660   0.201920    0.840 0.400994    
## factor(case)4   0.216513   0.201806    1.073 0.283604    
## factor(case)5   0.377437   0.201835    1.870 0.061794 .  
## factor(case)6   0.233856   0.201762    1.159 0.246726    
## factor(case)7   0.291743   0.202146    1.443 0.149292    
## factor(case)8   0.040054   0.201779    0.199 0.842696    
## factor(case)9   0.182815   0.201763    0.906 0.365123    
## factor(case)10  0.197116   0.201782    0.977 0.328886    
## factor(case)11  0.264956   0.201782    1.313 0.189480    
## factor(case)12  0.075685   0.201799    0.375 0.707708    
## factor(case)13  0.249261   0.201795    1.235 0.217062    
## factor(case)14  0.118726   0.201765    0.588 0.556382    
## factor(case)15  0.494571   0.201828    2.450 0.014451 *  
## factor(case)16 -0.085249   0.203634   -0.419 0.675579    
## factor(case)17  0.383878   0.201766    1.903 0.057403 .  
## factor(case)18  0.290350   0.201837    1.439 0.150619    
## factor(case)19  0.483656   0.201790    2.397 0.016734 *  
## factor(case)20  0.287368   0.201792    1.424 0.154759    
## factor(time)2   0.596679   0.319058    1.870 0.061780 .  
## factor(time)3   1.407443   0.319158    4.410 1.16e-05 ***
## factor(time)4  -0.328379   0.320361   -1.025 0.305616    
## factor(time)5  -0.119690   0.319047   -0.375 0.707634    
## factor(time)6   1.539948   0.319015    4.827 1.62e-06 ***
## factor(time)7   0.338443   0.319042    1.061 0.289052    
## factor(time)8   0.528072   0.319048    1.655 0.098232 .  
## factor(time)9  -0.032228   0.319030   -0.101 0.919556    
## factor(time)10  0.765270   0.319056    2.399 0.016657 *  
## factor(time)11  0.789097   0.319058    2.473 0.013568 *  
## factor(time)12  2.149822   0.319199    6.735 2.87e-11 ***
## factor(time)13  0.118301   0.319032    0.371 0.710862    
## factor(time)14 -0.734958   0.319015   -2.304 0.021452 *  
## factor(time)15  0.888259   0.319109    2.784 0.005486 ** 
## factor(time)16  0.553245   0.319027    1.734 0.083221 .  
## factor(time)17  0.652004   0.319030    2.044 0.041264 *  
## factor(time)18  1.351653   0.319104    4.236 2.50e-05 ***
## factor(time)19  0.260828   0.319061    0.817 0.413860    
## factor(time)20 -0.662932   0.319060   -2.078 0.038005 *  
## factor(time)21  1.509116   0.319095    4.729 2.60e-06 ***
## factor(time)22 -0.014879   0.319029   -0.047 0.962810    
## factor(time)23 -0.538377   0.319023   -1.688 0.091827 .  
## factor(time)24  0.493220   0.319046    1.546 0.122464    
## factor(time)25 -1.820174   0.319100   -5.704 1.57e-08 ***
## factor(time)26 -0.571426   0.319015   -1.791 0.073583 .  
## factor(time)27  0.093017   0.319026    0.292 0.770683    
## factor(time)28 -1.328269   0.319035   -4.163 3.43e-05 ***
## factor(time)29  0.342019   0.319038    1.072 0.283983    
## factor(time)30 -0.774338   0.319014   -2.427 0.015401 *  
## factor(time)31  0.461852   0.319026    1.448 0.148040    
## factor(time)32  0.308478   0.319046    0.967 0.333858    
## factor(time)33  1.343319   0.319094    4.210 2.80e-05 ***
## factor(time)34  1.989206   0.319258    6.231 7.03e-10 ***
## factor(time)35 -0.422407   0.319019   -1.324 0.185801    
## factor(time)36  0.614837   0.319085    1.927 0.054299 .  
## factor(time)37 -0.416240   0.319061   -1.305 0.192359    
## factor(time)38  0.779448   0.319089    2.443 0.014762 *  
## factor(time)39 -0.847622   0.319015   -2.657 0.008019 ** 
## factor(time)40  3.245566   0.319204   10.168  &amp;lt; 2e-16 ***
## factor(time)41 -0.589393   0.319031   -1.847 0.064999 .  
## factor(time)42  0.509922   0.319178    1.598 0.110469    
## factor(time)43 -0.815928   0.319029   -2.558 0.010699 *  
## factor(time)44  1.200743   0.320664    3.745 0.000192 ***
## factor(time)45 -0.677540   0.319015   -2.124 0.033946 *  
## factor(time)46  0.329598   0.319049    1.033 0.301842    
## factor(time)47  0.254068   0.319037    0.796 0.426027    
## factor(time)48  0.728453   0.319070    2.283 0.022652 *  
## factor(time)49  2.376336   0.319302    7.442 2.25e-13 ***
## factor(time)50  0.525069   0.319064    1.646 0.100172    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.009 on 930 degrees of freedom
## Multiple R-squared:  0.9625, Adjusted R-squared:  0.9598 
## F-statistic: 346.3 on 69 and 930 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bada bing bada boom. Now there are no missing coefficients. However, the coefficient on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; isn’t any easier to interpret, and in fact it is in even less transparent as it involves taking averages of averages of coefficients in the cross-section and within-case variation (say that five times fast). It is also peculiar that the model can only be fit with this kind of variation in the &lt;em&gt;slopes&lt;/em&gt;, as opposed to the intercepts, as might appear more logical. The models with fixed effects only for cases or time points do not have this problem at all.&lt;/p&gt;
&lt;p&gt;What makes this lack of identification particularly malicious is that it is virtually impossible to identify unless someone takes the trouble as we did to generate data from scratch. Visually, the dataset with single coefficients for cross-sectional/within-case variation appears pristine. It is only deep under the hood that the problem appears. The fact that R (and Stata has similar behavior) can magically make the problem go away by changing the data only makes it less likely that the problem will ever be identified. We simply do not know how many papers in the literature have been affected by this problem (or by similar situations where the regression slopes are almost fixed across cases or time points).&lt;/p&gt;
&lt;p&gt;There is one important caveat to this post. The two-way fixed effects model &lt;em&gt;can&lt;/em&gt; be a difference-in -differences model, but &lt;em&gt;only&lt;/em&gt; if there are exactly two (2) time points. It is not possible to run a two-way FE model with many time points and call it difference-in-difference as there are the same difficulties in interpretation. We discuss this more in the paper.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In summary, fixed effects models are very useful for panel data as they help isolate dimensions of variation that matter for research questions. They are not magical tools for causal inference, but rather frameworks for understanding data. It is important to think about which dimension (cross-section or within-case) is more relevant, and then go with that dimension. All other concerns, including modeling spatial or time autocorrelation, omitted variables and endogeneity are all secondary to this first and most important point, which is what comparisons can be drawn from the data?&lt;/p&gt;
&lt;p&gt;One important area that this can be applied to is allowing people to fit more models with fixed effects on &lt;em&gt;time points&lt;/em&gt; rather than cases. There are many questions that can be best understood by comparing cases to each other rather than cases to themselves over time. Studying long-run institutions like electoral systems, for example, can only be understood in terms of cross-sectional variation. There is nothing unique or special about the within-case model.&lt;/p&gt;
&lt;p&gt;Some claim that within-case variation is better because cases have less heterogeneity than the cross-section. However, there is no way this can be true a priori. If minimizing heterogeneity is the aim, then it is important to consider the time frame and how units change over time. For example, if we have a very long time series, say 200 years, and we are comparing the U.S. and Canada, then we might believe that the comparison of the U.S. of 1800 to the Canada of 1800 (the cross-section) has less noise than a comparison of the U.S. of 1800 to the U.S. of 2020 (within-case variation). We need to think more about our data and what it represents rather than taking the short-cut of employing the same model everyone else uses.&lt;/p&gt;
&lt;p&gt;While we did not discuss random effects/hierarchical models in this post, the same principles apply even if “partial pooling” is used rather than “no pooling”. Intercepts are designed to draw comparisons, and however the intercepts are modeled, it is important to think about what they are saying about the data, and whether that statement makes any sense.&lt;/p&gt;
&lt;p&gt;So if you don’t want to eat that cheeseburger… we release you. Go enjoy your tofu-based meat alternative with relish.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Proposed Model for Partial Identification of SARS-CoV2 Infection Rates Given Observed Tests and Cases</title>
      <link>/post/kubinec_model_draft/</link>
      <pubDate>Sat, 28 Mar 2020 15:00:00 +0000</pubDate>
      <guid>/post/kubinec_model_draft/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduction&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;In recent days as more and more data is available on observed case counts of the SARS-CoV2 coronavirus there are also increasing attempts to infer infection rates from these case counts. Furthermore, policy makers like Deborah Bix have begun to question whether the predictions of epidemiological models are far worse than the observed data.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; By contrast, in this paper I show that it is impossible to infer the true number of infected people from observed tests and cases alone, although it is possible to infer the effect of suppression policies on the &lt;em&gt;relative&lt;/em&gt; rates of infectious growth. As a result, the susceptible-infected-recovered (SIR)/susceptible-exposed-infected-recovered (SEIR) approach &lt;span class=&#34;citation&#34;&gt;(Peak et al. 2020; Riou et al. 2020; Robert Verity 2020; Perkins et al. 2020; Jose Lourenco 2020; Ruiyun Li 2020; Neil M Ferguson 2020)&lt;/span&gt; is the only approach that can provide bounds on the the true number of infected given reasonable assumptions. The intention of this paper is to offer a more limited approach based on modeling empirical data, particularly observed tests and cases, that allows researchers to test for the effect of suppression policies and other region-level factors that is partially identified, i.e. up to an unobserved scaling factor of the true infection rate. Furthermore, by using informed priors from SIR/SEIR models, the estimates presented in this paper can be transformed to reasonable estimates of the actual number of infected persons.&lt;/p&gt;
&lt;p&gt;The aim of the much simpler model presented in this paper is to show how inter-country comparisons based on observed case counts and tests can be rank-identified, i.e., we can know how countries’ infections compare relative to each other even if we do not know the true number of infected. Given the model’s estimates, we can then identify the effect of suppression policies even without knowing the total number of infected by comparing the &lt;em&gt;rates&lt;/em&gt; of disease progression across regions.&lt;/p&gt;
&lt;p&gt;Furthermore, by employing insights from SIR models and epidemiologists, we can further put bounds on the infection rates to convert them to actual numbers of infected individuals by allowing that the number of tests could be as low as ten percent of the total infected &lt;span class=&#34;citation&#34;&gt;(Ruiyun Li 2020; Perkins et al. 2020)&lt;/span&gt;. Employing this correction, I show that the model provides external validation of the SIR models’ assessment that the total number of infected is much higher than the reported number of cases given available empirical data &lt;span class=&#34;citation&#34;&gt;(Ruiyun Li 2020; Perkins et al. 2020)&lt;/span&gt;. In other words, policymakers’ skepticism is unfounded if it is based on how many cases have been reported once we condition on the number of tests. To show this, I fit the model to observed cases and test counts among US states and territories while incorporating lower bounds from SIR/SEIR models. This model estimates that the total number of infected people is at least six-seven times higher than the current observed case count of 80,000 individuals according to the New York Times, approaching approximately 700,000 infected individuals with COVID-19 in the United States as of March 27th. Furthermore, this estimate is conservative as it relies on reported data, which necessarily has a time delay as cases and tests are reported.&lt;/p&gt;
&lt;p&gt;In addition, the model shows that US states that declared states of emergencies earlier than other states have lower infection rates than if they had declared states of emergencies later. However, declaring an early state of emergency has not yet flattened the recent increase spike in infection rates (i.e. flattened the curve), only reduced the overall total of infected people. In other words, if these states had declared states of emergencies later than they did, they might be even worse off than they are now. With these results, the model reveals how we can model suppression policies aimed at the virus with observed data if we make important adjustments to the counts of observed tests and cases.&lt;/p&gt;
&lt;p&gt;The intention of posting this early draft is to share the model with the wider research community as well as to invite feedback on the model. I and several collaborators are collecting data on government responses to the COVID-19 pandemic via the &lt;a href=&#34;https://lumesserschmidt.github.io/CoronaNet/&#34;&gt;CoronaNet project&lt;/a&gt;, and we want to employ this model to see which government responses are most effective. In the absence of academic conferences, I am hoping that posting online will permit me to recieve necessary feedback on the model’s validity and how to improve it.&lt;/p&gt;
&lt;p&gt;To reproduce the model and to access the underlying Stan code, please see my &lt;a href=&#34;https://github.com/saudiwin/corona_tscs&#34;&gt;Github page&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;model-of-empirical-covid-19-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model of Empirical COVID-19 Data&lt;/h2&gt;
&lt;p&gt;In this document I put forward a model for the rate of infected people &lt;span class=&#34;math inline&#34;&gt;\(I_{ct} \in (0,1)\)&lt;/span&gt; in a given country/region &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; given the the number of cases &lt;span class=&#34;math inline&#34;&gt;\(a_{ct}\)&lt;/span&gt; and the number of tests &lt;span class=&#34;math inline&#34;&gt;\(q_{ct}\)&lt;/span&gt; for all outbreak time periods &lt;span class=&#34;math inline&#34;&gt;\(t \in T\)&lt;/span&gt;. The outcome is here defined as the tests and cases reported on a given day rather than the cumulative total for ease of modeling purposes (i.e., the lagged difference of the cumulative total). I assume that &lt;span class=&#34;math inline&#34;&gt;\(I_{ct}\)&lt;/span&gt; is an unobserved Beta-distributed variable with a 3-order polynomial time trend of the number of post-outbreak time periods &lt;span class=&#34;math inline&#34;&gt;\(T_O &amp;lt; T_A\)&lt;/span&gt;, where an outbreak begins at the first reported case in a given area. By using a single time function, the model assumes that the coronavirus follows a similar pattern of infection across countries, as appears to be the case (i.e., the virus has not mutated across countries).&lt;/p&gt;
&lt;p&gt;The unobserved infection rate &lt;span class=&#34;math inline&#34;&gt;\(I_{ct}\)&lt;/span&gt; is a function of the following parameters:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
Pr(I_{ct}|T=t) \sim Beta(&amp;amp;\alpha_1 + \alpha_c + \beta_{O1}\sum_{c=1}^{C} \textbf{1}(a_{ct&amp;#39;}&amp;gt;0) \forall t&amp;#39; \in t&amp;#39;&amp;lt;t + \textbf{1}\beta_{S1} + \\
                        &amp;amp;\beta_{I1}t_o + \beta_{I2}t_o^2 + \beta_{I3}t_o^3 + \textbf{1}\beta_{S2},\phi)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(g(\cdot)\)&lt;/span&gt; is the inverse logit function, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_c\)&lt;/span&gt; are country-level intercepts (less one for identification), &lt;span class=&#34;math inline&#34;&gt;\(\beta_{O1}\sum_{c=1}^{C} \textbf{1}(a_{ct_a-1}&amp;gt;0) \forall t&amp;#39; \in t&amp;#39;&amp;lt;t\)&lt;/span&gt; is the sum of countries with at least one case of infection in the world at any previous time point, and the three &lt;span class=&#34;math inline&#34;&gt;\(\beta_{Ii}\)&lt;/span&gt; are polynomial coefficients of post-outbreak time points &lt;span class=&#34;math inline&#34;&gt;\(t_o\)&lt;/span&gt;. The sum of countries parameter measures the spread of the virus due to travel across state borders, and as such will increase rapidly as more states are infected. By contrast, the polynomial time trends represent possible within-state transmission of the virus, which can cause exponentially growing case counts compared to the linear sum of infected countries parameter. Finally, the parameter &lt;span class=&#34;math inline&#34;&gt;\(\phi\)&lt;/span&gt; is a dispersion parameter governing how precise the infection rate estimate is.&lt;/p&gt;
&lt;p&gt;Given these two primary ways that the infection rate can increase, there are two ways that possible suppression measures targeted at the virus can enter the model. The first is a constant factor, &lt;span class=&#34;math inline&#34;&gt;\(\textbf{1}\beta_{S1}\)&lt;/span&gt;, which is an indicator function that is equal to &lt;strong&gt;1&lt;/strong&gt; if a country has taken suppression measures and &lt;strong&gt;0&lt;/strong&gt; otherwise. This constant parameter is meant to capture the ability of countries or regions to stop the spread of transmission due to foreign travelers arriving in the country/region, which occurs at a roughly constant level over time.&lt;/p&gt;
&lt;p&gt;The second way suppression measures enter the model is through &lt;span class=&#34;math inline&#34;&gt;\(\textbf{1}\beta_{S2}\)&lt;/span&gt;, which can increase over time as the virus increases. This parameter reflects possible social distancing measures which will grow more effective as domestic transmission of the virus increases (i.e. as the polynomial time trend takes off). As such, it is assumed that any deviation from the common domestic virus transmission pattern is due to these time-varying suppression measures.&lt;/p&gt;
&lt;p&gt;Once the outbreak has started in country &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;, which is indicated by a positive case count in time &lt;span class=&#34;math inline&#34;&gt;\(t-1\)&lt;/span&gt;, a time counter &lt;span class=&#34;math inline&#34;&gt;\(t_O=1\)&lt;/span&gt; starts for that country and increases by 1 for each following time point until &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given this specification of the infection rate process, we can then move to the generation of the observed data, tests &lt;span class=&#34;math inline&#34;&gt;\(q_{ct_a}\)&lt;/span&gt; and cases &lt;span class=&#34;math inline&#34;&gt;\(a_{ct_a}\)&lt;/span&gt;. The infection rate is assumed to influence both of these numbers. First, an increasing number of infections is associated with more tests as countries try to identify who may have the virus. Furthermore, a rising infection rate is associated with a higher ratio of positive results (reported cases) conditional on the number of tests. I model both of these observed indicators, tests and cases, jointly to simultaneously adjust for the infection rate’s influence on both factors.&lt;/p&gt;
&lt;p&gt;To model the number of tests, I assume that each country has an unobserved level of testing parameter, &lt;span class=&#34;math inline&#34;&gt;\(\beta_{cq}&amp;gt;0\)&lt;/span&gt;, indicating how strongly each country is willing and able to perform tests as a factor of the unobserved infection rate. The number of observed tests &lt;span class=&#34;math inline&#34;&gt;\(q_{ct}\)&lt;/span&gt; for a given time point &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and country &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt; is distributed as a Binomial proportion of the countries’ population &lt;span class=&#34;math inline&#34;&gt;\(c_{p}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
q_{ct} \sim B(c_{p},g(\alpha_2 + \beta_{cq}I_{ct}))
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_{cq}\)&lt;/span&gt; serves to scale the infection rate &lt;span class=&#34;math inline&#34;&gt;\(I_{ct}\)&lt;/span&gt; so that an increasing infection rate has heterogeneous effects on the number of tests by country. The intercept &lt;span class=&#34;math inline&#34;&gt;\(\alpha_2\)&lt;/span&gt; indicates how many tests would be performed in a country with an infection rate of zero. It is assumed that this number is quite low, though not necessarily zero.&lt;/p&gt;
&lt;p&gt;Given the parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_{cq}\)&lt;/span&gt;, a country could test almost no one or test far more than are actually infected depending on their willingness to impose tests. However, the number of tests is increasing in &lt;span class=&#34;math inline&#34;&gt;\(I_{ct}\)&lt;/span&gt; conditional on a country’s willingness to test people. That is, regardless of how much a country wants to test people, as the outbreak grows the number of tests will increase though at very different rates.&lt;/p&gt;
&lt;p&gt;Given the number of observed tests &lt;span class=&#34;math inline&#34;&gt;\(q_{ct}\)&lt;/span&gt;, I can then generate the number of observed cases &lt;span class=&#34;math inline&#34;&gt;\(a_{ct}\)&lt;/span&gt; as a binomial proportion of the number of tests &lt;span class=&#34;math inline&#34;&gt;\(q_{ct}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
a_{ct} \sim B(q_{ct},g(\alpha_3 + \beta_a I_{ct_a})))
\]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(g(\cdot)\)&lt;/span&gt; is again the inverse logit function, &lt;span class=&#34;math inline&#34;&gt;\(\alpha_3\)&lt;/span&gt; is an intercept that indicates how many cases would test positive with an infection rate of zero (equal to the false positive rate of the test), and &lt;span class=&#34;math inline&#34;&gt;\(\beta_a&amp;gt;0\)&lt;/span&gt; is a parameter that determines how hard it is to find the infected people and test them as opposed to people who are not actually infected. The multiplication of this parameter and the infection rate determines the observed number of cases &lt;span class=&#34;math inline&#34;&gt;\(a_{ct}\)&lt;/span&gt; as a proportion of the number of observed tests &lt;span class=&#34;math inline&#34;&gt;\(q_{ct}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To summarize the model, infection rates determine how many tests a country is likely to undertake and also the number of positive tests they receive conditional on a certain number of tests. This simultaneous adjustment helps takes care of mis-interpreting the observed data by not taking into account varying testing rates, which is likely why some policy makers argue that the epidemiological models are wrong.&lt;/p&gt;
&lt;p&gt;Because sampling from a model with a hierarchical Beta parameter can be difficult, we can simplify the final likelihood by combining the beta distribution and the binomial counts into a beta-binomial model for tests:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
q_{ct_a} \sim BB(c_p,g(\alpha_2 + \beta_qI_{ct_a}),\phi_q)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;and cases:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
a_{ct_a} \sim BB(q_{ct},g(\alpha_3 + \beta_a I_{ct_a})),\phi_a)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For estimation purposes, the infection rates &lt;span class=&#34;math inline&#34;&gt;\(I_{ct}\)&lt;/span&gt; are put in to the beta-binomial model on the logit scale instead of transforming them to (0,1), but they can be transformed to proportions post-estimation with the inverse logit function.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;identification&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Identification&lt;/h1&gt;
&lt;p&gt;This model contains an unobserved latent process &lt;span class=&#34;math inline&#34;&gt;\(I_{ct_a}\)&lt;/span&gt;, and as such there are further constraints necessary in order to have a unique scale and rotation of the latent variable. First, sign restrictions are put on the suppression measure parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_{S1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_{S2}\)&lt;/span&gt; so that they are strictly negative. Second, positivity constraints are put on the parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_a\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_q\)&lt;/span&gt; that govern the relationship between the infection rate and test and case counts. It is assumed that infection rates will increase both the number of tests and the number of cases relative to the number of tests, though at necessarily different rates.&lt;/p&gt;
&lt;p&gt;The other important restriction is to fix the intercept for the cases model &lt;span class=&#34;math inline&#34;&gt;\(\alpha_3\)&lt;/span&gt; to a fixed value of 0.1, or -2.2 on the logit scale. The reason for this restriction is because the intercept necessarily equals the false positive rate of a COVID-19 test. While there is still ongoing research as to the false positive rate, it is clear that it is likely in the area of 1 in 10 false positives for rapid tests.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; As such, by pinning this value, we can lower-bound the number of cases we are likely to see given an infection rate of zero, providing a helpful lower-bound for the estimate.&lt;/p&gt;
&lt;p&gt;As I will show, no other identification restrictions are necessary to estimate the model beyond weakly informative priors assigned to parameters. These are:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
\beta_a &amp;amp;\sim E(.1)\\
\beta_{qc} &amp;amp;\sim E(\sigma_q)\\
\sigma_q &amp;amp;\sim E(.1)\\
\beta_{Si} &amp;amp;\sim N(0,2)\\
\alpha_c &amp;amp;\sim N(0,3)\\
\beta_{Ii} &amp;amp;\sim N(0,10)\\
\alpha_1 &amp;amp;\sim N(0,10)\\
\alpha_2 &amp;amp;\sim N(0,10)
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The one prior to note is that a hierarchical regularizing prior is put on the varying testing adjustment parameters &lt;span class=&#34;math inline&#34;&gt;\(\beta_{qc}\)&lt;/span&gt; for regularization purposes due to the limited data available to inform the parameter.&lt;/p&gt;
&lt;p&gt;Other than these weakly informative priors, the model is identified, as I show in the next section. However, it is important to emphasize that there is no information in the model that identifies the &lt;em&gt;true&lt;/em&gt; number of infected people. Rather, the infection rate is a latent process, and as such it is not known exactly what scale to assign to it without further information. However, both the relative growth in infection rates are identified, along with the effect of suppression measures, so &lt;em&gt;the model is useful without being fully identified&lt;/em&gt;. Furthermore, by incorporating insights from SIR/SEIR models I can also identify the latent scale with reasonable informative priors, as I show in the data analysis section. The SIR/SEIR models are not “doomsday” predictions but rather rigorous models of the underlying infection process.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simulation&lt;/h1&gt;
&lt;p&gt;Because this model is fully generative, we can simulate it using Monte Carlo methods. The simulation is very important as it is the only way to demonstrate that the model is globally identified and can in fact capture unobserved parameters like suppression effects and relative infection rates. The following R code generates data from the model and plots the resulted unobserved infection rate and observed values for tests and cases:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulation parameters
num_country &amp;lt;- 100
# unobserved country-level heterogeneity for outbreak onset
country_int &amp;lt;- c(0,sort(rnorm(num_country-1,0,.25)))
time_points &amp;lt;- 100
# allows for linear growth that later becomes explosive
polynomials &amp;lt;- c(.03,0.007,-0.0001)

# factor that determines how many people a country is willing/able to test

country_test &amp;lt;- rnorm(num_country,5,0.25)

# size of countries

country_pop &amp;lt;- rpois(num_country,10000)

# assumt t=1 is unmodeled = exogenous start of the infection
  
t1 &amp;lt;- c(1,rep(0,num_country-1))

# create a suppression coefficient
# first is for preventing domestic transmission from occuring
# second is for preventing further domestic transmission once it starts

suppress1 &amp;lt;- -0.8
suppress2 &amp;lt;- -0.05

# high value of phi = high over-time stability
phi &amp;lt;- c(300,300)

# countries that suppress or don&amp;#39;t suppress

suppress_measures &amp;lt;- as.numeric(runif(num_country)&amp;gt;0.5)

# parameter governing how hard it is to find infected people and test them
# strictly positive

finding &amp;lt;- 1.5

# recursive function to generate time-series data by country

out_poly &amp;lt;- function(time_pt,end_pt,time_counts,tested,case_count,rate_infected,pr_domestic) {
  
  if(time_pt==1) {
    time_counts &amp;lt;- as.matrix(c(1,rep(0,num_country-1)))
    rate_infected &amp;lt;- as.matrix(c(.0001,rep(0,num_country-1)))
    tested &amp;lt;- as.matrix(rep(0,num_country))
    case_count &amp;lt;- as.matrix(c(1,rep(0,num_country-1)))
  }
  
  # if at time = t infected, start time tracker at t
  # need to know how many countries have reported at least one case = infection start
  
  world_count &amp;lt;- sum(case_count[,time_pt]&amp;gt;0)
  
  
  if(time_pt==1) {
    
    rate_infected_new &amp;lt;-  plogis(-5 + time_counts[,time_pt]*polynomials[1] + 
                                   suppress1*suppress_measures +
                    suppress2*suppress_measures*time_counts[,time_pt] +
                    .05*sum(world_count) +
      (time_counts[,time_pt]^2)*polynomials[2] + 
        (time_counts[,time_pt]^3)*polynomials[3])
    
    # conservative time counter that only starts when first case is recorded
    
    time_counts_new &amp;lt;- ifelse(time_counts[,time_pt]&amp;gt;0 | case_count[,time_pt]&amp;gt;0,time_counts[,time_pt]+1,time_counts[,time_pt])
    
  } else {
    
    rate_infected_new &amp;lt;- plogis(-5 + time_counts[,time_pt]*polynomials[1] + 
                    suppress1*suppress_measures +
                    suppress2*suppress_measures*time_counts[,time_pt] +
                    .05*sum(world_count) +
      (time_counts[,time_pt]^2)*polynomials[2] + 
        (time_counts[,time_pt]^3)*polynomials[3])
    
    # conservative time counter that only starts when first case is recorded
    
    time_counts_new &amp;lt;- ifelse(time_counts[,time_pt]&amp;gt;0 | case_count[,time_pt]&amp;gt;0,time_counts[,time_pt]+1,time_counts[,time_pt])
  }

  # of these, need to calculated a set number tested
  mu_test &amp;lt;- plogis(-7 + country_test*rate_infected_new)
  tested_new &amp;lt;- rbbinom(num_country,country_pop,mu_test*phi,(1-mu_test)*phi)
  
  # determine case count as percentage number tested
  # this is what we always observe
  mu_case &amp;lt;- plogis(-2.19 + finding*rate_infected_new)
  case_count_new &amp;lt;- rbbinom(num_country,tested_new,mu_case*phi,(1-mu_case)*phi)
  
  
  if(time_pt&amp;lt;end_pt) {
    out_poly(time_pt=time_pt+1,
             end_pt=end_pt,
             time_counts=cbind(time_counts,
                               time_counts_new),
             rate_infected=cbind(rate_infected,rate_infected_new),
             tested=cbind(tested,tested_new),
             case_count=cbind(case_count,case_count_new))
  } else {
    return(list(time_counts=time_counts,
                tested=tested,
                rate_infected=rate_infected,
                case_count=case_count))
  }
  
}

check1 &amp;lt;- out_poly(1,time_points)

check1 &amp;lt;- lapply(check1, function(c) {
  colnames(c) &amp;lt;- as.numeric(1:time_points)
  c
})

all_out &amp;lt;- bind_rows(list(time_counts=as_tibble(check1$time_counts),
                          `Proportion Population\nInfected`=as_tibble(check1$rate_infected),
                          `Number of Cases`=as_tibble(check1$case_count),
                          `Proportion of Cases from Domestic Transmission`=as_tibble(check1$pr_domestic),
                          `Number of Tests`=as_tibble(check1$tested)),.id=&amp;quot;Series&amp;quot;)

all_out$country &amp;lt;- rep(paste0(&amp;quot;country_&amp;quot;,1:num_country),times=length(check1))
all_out$suppress_measures &amp;lt;- factor(rep(suppress_measures,times=length(check1)),labels=c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;))

all_out %&amp;gt;% 
  gather(key = &amp;quot;time_id&amp;quot;,value=&amp;quot;indicator&amp;quot;,-Series,-country,-suppress_measures) %&amp;gt;% 
  mutate(time_id=as.numeric(time_id)) %&amp;gt;% 
  filter(!(Series %in% c(&amp;quot;time_counts&amp;quot;))) %&amp;gt;% 
  ggplot(aes(y=indicator,x=time_id)) +
  geom_line(aes(colour=suppress_measures,group=country),alpha=0.3) +
  xlab(&amp;quot;Days Since Outbreak&amp;quot;) +
  ylab(&amp;quot;&amp;quot;) +
  ggtitle(&amp;quot;Simulation of Observed Tests and Cases Given Unobserved Infectious Process&amp;quot;) +
  facet_wrap(~Series,scales=&amp;quot;free_y&amp;quot;) +
  theme(panel.background = element_blank(),
        panel.grid=element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(face=&amp;quot;bold&amp;quot;),
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/onset-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;sim_infect.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The plots above show one line for each country’s trajectory from a total of 100 countries and 100 time points. As can be seen, the green lines indicating suppression effects and the red lines indicating no suppression diverge substantially over time. However, the numbers of observed tests and cases show far more random noise due to the difficulty in inferring the true rate from the observed data. It is possible that some countries simply want to test more, and end up with more cases, or that the infection rate is in fact higher. As such, this model is able to incorporate that measurement uncertainty between the true (unobserved) rate and the observed indicators, tests and cases.&lt;/p&gt;
&lt;p&gt;The advantage of this model is that it allows for the testing of covariates that affect the true infection rate without requiring more heavy-duty approaches like SEIR/SIR. The intention is to have a more parsimonious model that can see how the effect of variables like suppression measures have on different countries/states infection numbers over time. I am currently collecting this data as part of the CoronaNet project.&lt;/p&gt;
&lt;p&gt;The model could be further extended with more complicated initial infection processes and inter-country transmission processes, such as spatial modeling, but for the purposes of this exposition I do not look further at such extensions.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation&lt;/h1&gt;
&lt;p&gt;I can then fit an empirical model using Stan, a Markov Chain Monte Carlo sampler, to model the unobserved infection rate given the simulated observed data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# all data from simulation
# primarily case and test counts

# need to make centered, ortho-normal polynomials

ortho_time &amp;lt;- poly(scale(1:time_points),degree=3)

init_vals &amp;lt;- function() {
  list(phi1=100,
       phi2=100)
}

sim_data &amp;lt;- list(time_all=time_points,
                 num_country=num_country,
                 country_pop=country_pop,
                 cases=check1$case_count,
                 phi_scale=1/300,
                 ortho_time=ortho_time,
                 tests=check1$tested,
                 count_outbreak=as.numeric(scale(apply(check1$time_counts,2,function(c) sum(c&amp;gt;0)))),
                 time_outbreak=check1$time_counts,
                 suppress=suppress_measures)

if(run_model) {
  
  pan_model &amp;lt;- stan_model(&amp;quot;corona_tscs_betab.stan&amp;quot;)

# run model

pan_model_est &amp;lt;- sampling(pan_model,data=sim_data,chains=2,cores=2,iter=1200,warmup=800,init=init_vals)

saveRDS(pan_model_est,&amp;quot;data/pan_model_est.rds&amp;quot;)
} else {
  pan_model_est &amp;lt;- readRDS(&amp;quot;data/pan_model_est.rds&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can access the estimated infection rates and plot them:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_est &amp;lt;- as.data.frame(pan_model_est,&amp;quot;num_infected_high&amp;quot;) %&amp;gt;% 
  mutate(iter=1:n()) %&amp;gt;% 
  gather(key=&amp;quot;variable&amp;quot;,value=&amp;quot;estimate&amp;quot;,-iter) %&amp;gt;% 
  group_by(variable) %&amp;gt;% 
  mutate(estimate=plogis(estimate)) %&amp;gt;% 
  summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %&amp;gt;% 
  mutate(country_num=as.numeric(str_extract(variable,&amp;quot;(?&amp;lt;=\\[)[1-9][0-9]?0?&amp;quot;)),
         time_point=as.numeric(str_extract(variable,&amp;quot;[1-9][0-9]?0?(?=\\])&amp;quot;)))

all_est &amp;lt;- left_join(all_est,tibble(country_num=1:num_country,
                                    suppress_measures=factor(suppress_measures,labels=c(&amp;quot;No&amp;quot;,&amp;quot;Yes&amp;quot;))),by=&amp;quot;country_num&amp;quot;)

all_est %&amp;gt;% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est,
  group=country_num,
  fill=suppress_measures),alpha=0.5) +
  theme_minimal() +
  scale_color_brewer(type=&amp;quot;div&amp;quot;) +
  ylab(&amp;quot;Proportion of Population Infected&amp;quot;) +
  xlab(&amp;quot;Days Since Outbreak Start&amp;quot;) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;top&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/plot_data-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see very good recovery of the estimates from the model given the observed data. The estimates disappear near the top of the plot as the true infected rate approaches 100% and there is little variability in the data. This plots shows the 5% - 95% high posterior density interval over time for each country. As can be seen, the suppression effect can be clearly distinguished even with residual uncertainty from not directly observing the infection rate, i.e., “flattening the curve.” We also can learn what the effect of the suppression measure is:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;require(bayesplot)

mcmc_hist(as.array(pan_model_est,c(&amp;quot;suppress_effect[1]&amp;quot;,&amp;quot;suppress_effect[2]&amp;quot;)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/suppress_effect-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that despite the uncertainty in not perfectly observing the infection rate, we can still get a precise credible interval on the suppression effect. The effect is not the same as the “true” value due to the use of orthonormal polynomials for estimation purposes, but the effect is clearly distinguishable.&lt;/p&gt;
&lt;p&gt;The advantage of this model, as can be seen, is that with testing numbers and case counts, we can model the effect of country-level (or region-level) variables on the unseen infection rate up to an unknown constant. It is far simpler than existing approaches while still permitting inference on these hard-to-quantify measures. It is no substitute for infectious disease modeling–for example, it produces no estimates of the susceptible versus recovered individuals in the population, nor death rates–but rather a way to measure the effect of different suppressive and social-distancing measures on disease outcomes as well as approximate disease trajectory.&lt;/p&gt;
&lt;p&gt;There is, however, an important caveat to be made. The infection rate here is modeled as a latent variable, and as such the scale is largely determined by the priors. In this simulation we were able to know a priori what the relationship between all the coefficients and the true infection rate is, but in an applied setting, as I discuss next, we will not be able to do so without making further assumptions based on SIR/SEIR models. However, what is important is that the sign and relative rank of suppression covariates is identified &lt;em&gt;even if the true scale of the latent infection rate is unknown&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimation-with-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimation with Data&lt;/h1&gt;
&lt;p&gt;I am currently in the process of collecting world-wide data to use with this model. To provide some initial empirical findings, in this section I model numbers of COVID-19 case counts on US states and territories provided by &lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34;&gt;The New York Times&lt;/a&gt;. I supplement these observed case counts with testing data by day from the &lt;a href=&#34;https://github.com/COVID19Tracking/covid-tracking-data&#34;&gt;COVID-19 Tracking Project&lt;/a&gt;. The testing data only goes back to March 4th, so we will impute the testing data back in time by assuming that the average case/tests ratio stays the same to the origin of the outbreak (an assumption, of course, but for the purposes of this analysis is acceptable). Furthermore, as there are discrepancies where the reported number of tests in some states like New Jersey is less than the total number of cases, I impute the number of tests via the case/test ratio for the sample as a whole. While more sophisticated imputation strategies may yield more realistic results, along with improved data collection, this minimalist imputation strategy will suffice for this presentation as it represents what data is currently available. If anyone reading this post has more information about available data, I would appreciate if you could let me know &lt;a href=&#34;mailto:rmk7@nyu.edu&#34;&gt;via email&lt;/a&gt; (or if you have any other thoughts on this draft).&lt;/p&gt;
&lt;p&gt;To analyze the effect of suppression policies, I use the list of the dates that states of emergencies were declared across U.S. states and territories from &lt;a href=&#34;https://en.wikipedia.org/wiki/U.S._state_and_local_government_response_to_the_2020_coronavirus_pandemic&#34;&gt;Wikipedia&lt;/a&gt;. The suppression covariate is then equal to a state X 1 vector of days which is mean centered and standardized.&lt;/p&gt;
&lt;p&gt;In this section I first fit a partially-identified model without any information about the true number of infected people to demonstrate that we can identify the sign and relative rank effect of suppression policies without this information. I then show how we can use insight from SIR/SEIR models to provide bounds on the total number of infected people.&lt;/p&gt;
&lt;p&gt;I first load and munge the data to create the necessary time trends and data format for the model, and then fit it using the same partially-identified model as in the previous section:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(us_state_populations)

state_pop &amp;lt;- filter(us_state_populations,year==2010) %&amp;gt;% 
  select(state,population)

merge_names &amp;lt;- tibble(state.abb,
                      state=state.name)

nyt_data &amp;lt;- read_csv(&amp;quot;~/covid-19-data/us-states.csv&amp;quot;) %&amp;gt;% 
  complete(date,state,fill=list(cases=0,deaths=0,fips=0)) %&amp;gt;% 
  mutate(month_day=ymd(date)) %&amp;gt;% 
  group_by(state) %&amp;gt;% 
    arrange(state,date) %&amp;gt;% 
  mutate(Difference=cases - dplyr::lag(cases),
         Difference=coalesce(Difference,0,0)) %&amp;gt;% 
  left_join(merge_names,by=&amp;quot;state&amp;quot;)

tests &amp;lt;- read_csv(&amp;quot;~/covid-tracking-data/data/states_daily_4pm_et.csv&amp;quot;) %&amp;gt;% 
  mutate(month_day=ymd(date)) %&amp;gt;% 
  arrange(state,month_day) %&amp;gt;% 
  group_by(state) %&amp;gt;% 
  mutate(tests_diff=total-dplyr::lag(total),
         cases_diff=positive-dplyr::lag(positive),
         cases_diff=coalesce(cases_diff,positive),
         cases_diff=ifelse(cases_diff&amp;lt;0,0,cases_diff),
         tests_diff=coalesce(tests_diff,total),
         tests_diff=ifelse(tests_diff&amp;lt;0,0,tests_diff)) %&amp;gt;% 
  select(month_day,tests=&amp;quot;tests_diff&amp;quot;,total,state.abb=&amp;quot;state&amp;quot;)

# merge cases and tests

combined &amp;lt;- left_join(nyt_data,tests,by=c(&amp;quot;state.abb&amp;quot;,&amp;quot;month_day&amp;quot;)) %&amp;gt;% 
  left_join(state_pop,by=&amp;quot;state&amp;quot;) %&amp;gt;% 
  filter(!is.na(population))

# add suppression data

emergency &amp;lt;- read_csv(&amp;quot;data/state_emergency_wikipedia.csv&amp;quot;) %&amp;gt;% 
  mutate(day_emergency=dmy(paste0(`State of emergency declared`,&amp;quot;-2020&amp;quot;)),
         mean_day=mean(as.numeric(day_emergency),na.rm=T),
         sd_day=sd(as.numeric(day_emergency),na.rm=T),
         day_emergency=((as.numeric(day_emergency) - mean_day)/sd_day)) %&amp;gt;% 
  select(state=&amp;quot;State/territory&amp;quot;,day_emergency,mean_day,sd_day) %&amp;gt;% 
  mutate(state=substr(state,2,nchar(state))) %&amp;gt;% 
  filter(!is.na(day_emergency))

combined &amp;lt;- left_join(combined,emergency,by=&amp;quot;state&amp;quot;)

# impute data

combined &amp;lt;- group_by(combined,state) %&amp;gt;% 
  mutate(test_case_ratio=sum(tests,na.rm=T)/sum(Difference,na.rm=T)) %&amp;gt;% 
  ungroup %&amp;gt;% 
  mutate(test_case_ratio=ifelse(test_case_ratio&amp;lt;1 | is.na(test_case_ratio),
                                mean(test_case_ratio[test_case_ratio&amp;gt;1],na.rm=T),test_case_ratio)) %&amp;gt;% 
  group_by(state) %&amp;gt;% 
    mutate(tests=case_when(Difference&amp;gt;0 &amp;amp; is.na(tests)~Difference*test_case_ratio,
                    Difference==0~0,
                    Difference&amp;gt;tests~Difference*test_case_ratio,
                    TRUE~tests)) %&amp;gt;% 
  arrange(state)

# create case dataset

cases_matrix &amp;lt;- select(combined,Difference,month_day,state) %&amp;gt;% 
  group_by(month_day,state) %&amp;gt;% 
  summarize(Difference=as.integer(mean(Difference))) %&amp;gt;% 
  spread(key = &amp;quot;month_day&amp;quot;,value=&amp;quot;Difference&amp;quot;)

cases_matrix_num &amp;lt;- as.matrix(select(cases_matrix,-state))

# create tests dataset

tests_matrix &amp;lt;- select(combined,tests,month_day,state) %&amp;gt;% 
  group_by(month_day,state) %&amp;gt;% 
  summarize(tests=as.integer(mean(tests))) %&amp;gt;% 
  spread(key = &amp;quot;month_day&amp;quot;,value=&amp;quot;tests&amp;quot;)

tests_matrix_num &amp;lt;- as.matrix(select(tests_matrix,-state))

# need the outbreak matrix

outbreak_matrix &amp;lt;- as.matrix(lapply(1:ncol(cases_matrix_num), function(c) {
  if(c==1) {
    outbreak &amp;lt;- as.numeric(cases_matrix_num[,c]&amp;gt;0)
  } else {
    outbreak &amp;lt;- as.numeric(apply(cases_matrix_num[,1:c],1,function(col) any(col&amp;gt;0)))
  }
  tibble(outbreak)
}) %&amp;gt;% bind_cols)

colnames(outbreak_matrix) &amp;lt;- colnames(cases_matrix_num)

time_outbreak_matrix &amp;lt;- t(apply(outbreak_matrix,1,cumsum))

just_data &amp;lt;- distinct(combined,state,day_emergency,population) %&amp;gt;% arrange(state)

# now give to Stan

ortho_time &amp;lt;- poly(scale(1:ncol(cases_matrix_num)),degree=3)

real_data &amp;lt;- list(time_all=ncol(cases_matrix_num),
                 num_country=nrow(cases_matrix_num),
                 country_pop=floor(just_data$population/100),
                 cases=cases_matrix_num,
                 ortho_time=ortho_time,
                 phi_scale=.1,
                 count_outbreak=as.numeric(scale(apply(outbreak_matrix,2,sum))),
                 tests=tests_matrix_num,
                 time_outbreak=time_outbreak_matrix,
                 suppress=just_data$day_emergency)

init_vals &amp;lt;- function() {
  list(phi_raw=c(10,10),
       world_infect=0,
       finding=1,
       alpha=c(0,0))
}


if(run_model) {
  us_fit &amp;lt;- sampling(pan_model,data=real_data,chains=3,cores=3,iter=1500,warmup=1000,control=list(adapt_delta=0.95),
                   init=init_vals)
  
  saveRDS(us_fit,&amp;quot;data/us_fit.rds&amp;quot;)
} else {
  us_fit &amp;lt;- readRDS(&amp;quot;data/us_fit.rds&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can then extract and plot the inferred infection rates by state. It is important to note that these infection rates are on the partially-identified latent scale, as such, they cannot be converted to total numbers of infected people. The only comparison can made between states (i.e., which states have the most infected people and by what proportion relative to other states).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_est_state &amp;lt;- as.data.frame(us_fit,&amp;quot;num_infected_high&amp;quot;) %&amp;gt;% 
  mutate(iter=1:n()) %&amp;gt;% 
  gather(key=&amp;quot;variable&amp;quot;,value=&amp;quot;estimate&amp;quot;,-iter) %&amp;gt;% 
  group_by(variable) %&amp;gt;% 
  mutate(state_num=as.numeric(str_extract(variable,&amp;quot;(?&amp;lt;=\\[)[1-9][0-9]?0?&amp;quot;)),
         time_point=as.numeric(str_extract(variable,&amp;quot;[1-9][0-9]?0?(?=\\])&amp;quot;)),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state &amp;lt;- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress,by=&amp;quot;state_num&amp;quot;))

# merge in total case count

case_count &amp;lt;- gather(cases_matrix,key=&amp;quot;time_point&amp;quot;,value=&amp;quot;cases&amp;quot;,-state) %&amp;gt;% 
  mutate(time_point=ymd(time_point)) %&amp;gt;% 
  group_by(state) %&amp;gt;% 
  arrange(state,time_point) %&amp;gt;% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count &amp;lt;- group_by(case_count,time_point) %&amp;gt;% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state &amp;lt;- left_join(all_est_state,us_case_count,by=&amp;quot;time_point&amp;quot;)

all_est_state %&amp;gt;% 
  mutate(estimate=estimate) %&amp;gt;% 
  group_by(state_num,time_point,suppress_measures) %&amp;gt;% 
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %&amp;gt;% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_line(aes(group=state_num,colour=suppress_measures),alpha=0.5) +
  stat_smooth() +
  theme_minimal() +
  scale_color_distiller(palette=&amp;quot;RdBu&amp;quot;,direction=-1) +
  ylab(&amp;quot;Latent Infection Scale&amp;quot;) +
  ggtitle(&amp;quot;Measuring Relative Infection Rates by U.S. State with Total Average&amp;quot;,
          subtitle=&amp;quot;Lines Colored by When State Declared State of Emergency&amp;quot;) +
  labs(caption=&amp;quot;As the total number of infected people is unknown, this chart measures the relative\ninfection rates between states.&amp;quot;) +
  xlab(&amp;quot;Days Since Outbreak Start&amp;quot;) +
  geom_hline(yintercept = 0,linetype=3) +
  guides(color=guide_colorbar(title=&amp;quot;Timing of State Emergency Declaration&amp;quot;)) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;top&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/infect_by_state-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_est_state %&amp;gt;% 
  mutate(estimate=estimate) %&amp;gt;% 
  group_by(state_num,time_point,suppress_measures) %&amp;gt;% 
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %&amp;gt;% 
  ggplot(aes(y=med_est,x=time_point)) +
  #geom_line(aes(group=state_num,colour=suppress_measures),alpha=0.5) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est,
  group=state_num,
  fill=suppress_measures),alpha=0.5) +
  stat_smooth() +
  theme_minimal() +
  scale_color_distiller(palette=&amp;quot;RdBu&amp;quot;,direction=-1) +
  ylab(&amp;quot;Latent Infection Scale&amp;quot;) +
  ggtitle(&amp;quot;Uncertainty in Relative Infection Rates by U.S. State with Total Average&amp;quot;,
          subtitle=&amp;quot;5% - 95% HPD Intervals Colored by When State Declared State of Emergency&amp;quot;) +
  labs(caption=&amp;quot;As the total number of infected people is unknown, this chart measures the relative\ninfection rates between states.&amp;quot;) +
  xlab(&amp;quot;Days Since Outbreak Start&amp;quot;) +
  geom_hline(yintercept = 0,linetype=3) +
  guides(fill=guide_colorbar(title=&amp;quot;Timing of State Emergency Declaration&amp;quot;)) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/infect_by_state-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;uncertain_state_rates.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see from this plot that the number of infected people started to increase dramatically about March 1st. It is also clear that the infection rate significantly increased before testing occurred, as the SIR models predicted.&lt;/p&gt;
&lt;p&gt;It would appear that the rate of increase has leveled off in the last week, though that is not a supported inference as the scale is the logit scale, so it is similar to the log scale in that higher numbers are farther away than they appear visually. Because the latent scale is not identified, the model is showing how the infection rates have evolved from zero to the true but unknown top infection rate. As such, it appears to be slowing as it reaches the top of the scale, but that is simply an illusion of logarithmic growth. The infection rate continues to increase in the United States, which will be clearer when I apply a transformation in the next section.&lt;/p&gt;
&lt;p&gt;As can be expected, on the whole states with earlier declarations of states of emergency have higher infection rates, which might at first lead us to believe that the declarations are associated with more, rather than fewer, infections. However, as I included a parameter for the declarations, we can estimate what independent effect the timing of the declaration had marginal of the rate of infectious growth. To really know what the association is between states of emergency and infection rates, we can examine the relevant parameters in the model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mcmc_hist(us_fit,regex_pars =&amp;quot;suppress&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/suppress-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that there is a statistically identifiable effect of early state of emergency declarations (&lt;code&gt;suppress_effect[1]&lt;/code&gt;) on reducing the infection rate with a 5% - 95% high posterior density (HPD) interval of (-3.57,-0.23). However, there does not appear to be a clear over-time effect on the curve of the epidemic (&lt;code&gt;suppress_effect[2]&lt;/code&gt;), suggesting that these early containment strategies have not yet stopped domestic transmission of the virus. In essence, what the model suggests is that early states of emergencies protected these states from later feedback effects as more neighboring states were also infected by the virus. As a result, even though these states have higher total infection rates, these rates would have been still higher if not for early state of emergency declarations.&lt;/p&gt;
&lt;p&gt;It is important to note that this association is just that, an association. It could be that states of emergency declarations are correlated with better public health systems, and that it is in fact the public health system that is doing the work, not the timing of the state of emergency declaration. However, the fine-grained distinctions in terms of the exact day that the state of emergency was announced suggests it is indicative that those states which made early preparations are starting to see some of the results already.&lt;/p&gt;
&lt;p&gt;As I have said, the model adjusts observed case counts for the number of test rates reported. We can see which states seem to do more tests per infected person by examining the estimated &lt;span class=&#34;math inline&#34;&gt;\(\beta_{cq}\)&lt;/span&gt; parameters in this plot:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;test_var &amp;lt;- as.data.frame(us_fit,&amp;quot;country_test_raw&amp;quot;) %&amp;gt;%
  mutate(iter=1:n()) %&amp;gt;%
  gather(key=&amp;quot;variable&amp;quot;,value=&amp;quot;estimate&amp;quot;,-iter) %&amp;gt;%
  mutate(state_num=as.numeric(str_extract(variable,&amp;quot;(?&amp;lt;=\\[)[1-9][0-9]?0?&amp;quot;)))

test_var &amp;lt;- left_join(test_var,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress))

test_var %&amp;gt;%
  group_by(state) %&amp;gt;%
    summarize(med_est=quantile(estimate,.5),
            high_est=quantile(estimate,.95),
            low_est=quantile(estimate,.05)) %&amp;gt;%
  ggplot(aes(y=med_est,x=reorder(state,med_est))) +
  geom_pointrange(aes(ymin=low_est,ymax=high_est)) +
  theme_minimal() +
  theme(panel.grid = element_blank()) +
  coord_flip() +
  xlab(&amp;quot;&amp;quot;) +
  ggtitle(&amp;quot;Comparison of States&amp;#39; Testing Rates Relative to Infection Rates&amp;quot;,
          subtitle=&amp;quot;Based on Model of Latent COVID-19 Infection Process&amp;quot;) +
  labs(caption = &amp;quot;Only relative differences between states are identified. The raw numbers do not have a
                  direct interpretation in terms of tests per infected individuals as the total number
                  of infected individuals is unknown.&amp;quot;) +
  ylab(&amp;quot;Proportion Tested Relative to Proportion Infected&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/tests_per_infected-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;testing.png&amp;quot;,scale=1.1)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This plot shows that the states that are testing way more than their infected populations are actually North Dakota, Alaska and Idaho, likely because these states have yet to see very large numbers of infections. While New York has been implementing many tests, the model does not downweight infection rates considerably even with the increase in testing, implying that the increase in cases in New York is a sign of a growing infection, not just increased tests. On the more worrying side, the model estimates that Maryland, Arizona, Georgia and Delaware may be testing far fewer than are actually infected.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;identifying-the-latent-scale&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Identifying the Latent Scale&lt;/h1&gt;
&lt;p&gt;To show how we can further attempt to identify the scale of the latent variable, instead of only relative differences between states, I show in this section how we can add in information from SEIR/SIR modeling to identify the total number of infected persons in the model. The crucial missing piece of information in the model is the ratio between the proportion of infected persons &lt;span class=&#34;math inline&#34;&gt;\(I_{ct}\)&lt;/span&gt; and the proportion of tests per state population &lt;span class=&#34;math inline&#34;&gt;\(q_{ct}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{q_{ct}}{I_{ct}}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Another way of framing the problem is to think of how much the number of tests should increase given a one-percentage increase in the infection rate. How many of these infected people will be tested? Without an idea of the true number of infected people, it is impossible to answer this question and thus identify the latent scale.&lt;/p&gt;
&lt;p&gt;However, an increasing number of SIR/SEIR papers show that it is likely that as few as 10% of the total number of infected persons are actually recorded as cases, including in the United States &lt;span class=&#34;citation&#34;&gt;(Ruiyun Li 2020; Perkins et al. 2020)&lt;/span&gt;. This number provides us with a conservative lower bound if we consider that the number of tests should be at least 10% of the total proportion of infected persons. In other words, we can consider adding the following information into the model:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\frac{q_{ct}}{I_{ct}} &amp;gt;0.1
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Every percentage increase in the infection rate should result in at least a 0.1% increase in the total number of people tested as a percentage of the population. It is difficult to impose this constraint directly in the model; however, we can consider adding it as prior information if we can define a prior density over the ratio. First, for computational simplicity, we can consider a distribution over the log differences of the parameters:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{log} q_{ct} - \text{log} I_{ct}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By simulating from a uniform distribution of possible rates for &lt;span class=&#34;math inline&#34;&gt;\(\text{log} q_{ct}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\text{log} I_{ct}\)&lt;/span&gt;, it is possible to tell that the a realistic distribution of log differences with 0.1 as a lower bound is in fact very close a standard normal distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(log(runif(1000,.01,.5)) - log(runif(1000,.01,.5)))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/std_normal-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can test whether it is a standard normal by comparing the log densities of different Normal distributions with different standard deviations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;log_diff &amp;lt;- log(runif(1000,.01,.5)) - log(runif(1000,.01,.5))

sum(dnorm(log_diff,0,1,log=T))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1546.175&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(dnorm(log_diff,0,2,log=T))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -1768.895&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(dnorm(log_diff,0,0.5,log=T))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2734.737&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sum(dnorm(log_diff,0,3,log=T))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] -2087.244&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see that the standard normal has the highest log density. As such, we can add the following prior to the model to bias the results to an interval between &lt;span class=&#34;math inline&#34;&gt;\(e^-2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(e^2\)&lt;/span&gt;, or 0.14 and 7.38. The prior density allows for the ratio of tests to infected individuals to be very large, suggesting that infection rates are not as high given increasing numbers of tests, but the rate is unlikely to be lower than 0.1, or 10 percent of the total number of infected. This conservative assumption allows us to use SEIR/SIR model conclusions while still permitting uncertainty over the relationship.&lt;/p&gt;
&lt;p&gt;Because the prior is essentially a way to weight the density, I do not need to do a Jacobian adjustment as I would need to if I wanted to back-transform the density.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; For these reasons, I simply add this term to the joint posterior for each time point &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and country &lt;span class=&#34;math inline&#34;&gt;\(c\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{log} q_{ct} - \text{log} I_{ct} \sim N(0,1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I then fit a model to the same United States data with the addition of the informative prior to scale the latent variable:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(run_model) {
  pan_model_scale &amp;lt;- stan_model(&amp;quot;corona_tscs_betab_scale.stan&amp;quot;)
  
  us_fit_scale &amp;lt;- sampling(pan_model_scale,data=real_data,chains=3,cores=3,iter=1500,warmup=1000,
                           control=list(adapt_delta=0.95),
                   init=init_vals)
  
  saveRDS(us_fit_scale,&amp;quot;data/us_fit_scale.rds&amp;quot;)
} else {
  us_fit_scale &amp;lt;- readRDS(&amp;quot;data/us_fit_scale.rds&amp;quot;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then calculate the total number of infected people in the United States by day given this re-scaled latent variable, and compare that to the observed number of cases from the New York Times data:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;all_est_state &amp;lt;- as.data.frame(us_fit_scale,&amp;quot;num_infected_high&amp;quot;) %&amp;gt;% 
  mutate(iter=1:n()) %&amp;gt;% 
  gather(key=&amp;quot;variable&amp;quot;,value=&amp;quot;estimate&amp;quot;,-iter) %&amp;gt;% 
  group_by(variable) %&amp;gt;% 
  mutate(state_num=as.numeric(str_extract(variable,&amp;quot;(?&amp;lt;=\\[)[1-9][0-9]?0?&amp;quot;)),
         time_point=as.numeric(str_extract(variable,&amp;quot;[1-9][0-9]?0?(?=\\])&amp;quot;)),
         time_point=ymd(min(combined$month_day)) + days(time_point-1))

all_est_state &amp;lt;- left_join(all_est_state,tibble(state_num=1:nrow(cases_matrix),
                                                state=cases_matrix$state,
                                                state_pop=real_data$country_pop,
                                    suppress_measures=real_data$suppress,by=&amp;quot;state_num&amp;quot;))

# merge in total case count

case_count &amp;lt;- gather(cases_matrix,key=&amp;quot;time_point&amp;quot;,value=&amp;quot;cases&amp;quot;,-state) %&amp;gt;% 
  mutate(time_point=ymd(time_point)) %&amp;gt;% 
  group_by(state) %&amp;gt;% 
  arrange(state,time_point) %&amp;gt;% 
  mutate(cum_sum_cases=cumsum(cases)) 

us_case_count &amp;lt;- group_by(case_count,time_point) %&amp;gt;% 
  summarize(all_cum_sum=sum(cum_sum_cases))

all_est_state &amp;lt;- left_join(all_est_state,us_case_count,by=&amp;quot;time_point&amp;quot;)

calc_sum &amp;lt;- all_est_state %&amp;gt;% 
  ungroup %&amp;gt;% 
  mutate(estimate=(plogis(estimate)/100)*(state_pop*100)) %&amp;gt;% 
  group_by(state_num,iter) %&amp;gt;% 
  arrange(state_num,time_point) %&amp;gt;% 
  mutate(cum_est=cumsum(estimate)) %&amp;gt;% 
  group_by(time_point,iter,all_cum_sum) %&amp;gt;% 
  summarize(us_total=sum(cum_est)) %&amp;gt;% 
  group_by(time_point,all_cum_sum) %&amp;gt;% 
  summarize(med_est=quantile(us_total,.5),
            high_est=quantile(us_total,.95),
            low_est=quantile(us_total,.05)) 

max_est &amp;lt;- as.integer(round(calc_sum$med_est[calc_sum$time_point==max(calc_sum$time_point)]))
high_max_est &amp;lt;- as.integer(round(calc_sum$high_est[calc_sum$time_point==max(calc_sum$time_point)]))
low_max_est &amp;lt;- as.integer(round(calc_sum$low_est[calc_sum$time_point==max(calc_sum$time_point)]))
max_obs &amp;lt;- calc_sum$all_cum_sum[calc_sum$time_point==max(calc_sum$time_point)]

options(scipen=999)

calc_sum %&amp;gt;% 
  ggplot(aes(y=med_est,x=time_point)) +
  geom_ribbon(aes(ymin=low_est,
  ymax=high_est),
  fill=&amp;quot;blue&amp;quot;,
  alpha=0.5) +
  geom_line(aes(y=all_cum_sum)) +
  theme_minimal() +
  ylab(&amp;quot;Total Number Infected/Reported&amp;quot;) +
  scale_y_continuous(labels=scales::comma) +
  ggtitle(&amp;quot;Approximate Total Number of COVID-19 Infected Individuals in the U.S.&amp;quot;,
          subtitle=&amp;quot;Blue 5% - 95% HPD Intervals Show Estimated Infected and Black Line Observed Cases&amp;quot;) +
  labs(caption=&amp;quot;These estimates are based on the assumption that as few as 10% of cases\nmay be reported based on SIR/SEIR models.&amp;quot;) +
  annotate(&amp;quot;text&amp;quot;,x=ymd(c(&amp;quot;2020-03-26&amp;quot;,&amp;quot;2020-03-26&amp;quot;)),
           y=c(max_est,max_obs),
           hjust=1,
           vjust=0,
           fontface=&amp;quot;bold&amp;quot;,
           size=3,
           label=c(paste0(&amp;quot;Estimated Infected:\n&amp;quot;,formatC(low_max_est,big.mark=&amp;quot;,&amp;quot;,format = &amp;quot;f&amp;quot;,digits=0),&amp;quot; - &amp;quot;,
                                                         formatC(high_max_est,big.mark=&amp;quot;,&amp;quot;,format = &amp;quot;f&amp;quot;,digits=0)),
                   paste0(&amp;quot;Total Reported Cases:\n&amp;quot;,formatC(max_obs,big.mark=&amp;quot;,&amp;quot;)))) +
  xlab(&amp;quot;Days Since Outbreak Start&amp;quot;) +
  theme(panel.grid = element_blank(),
        legend.position = &amp;quot;top&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/kubinec_model_draft_files/figure-html/infect_total_scaled-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;est_vs_obs.png&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This model was written to permit the identification of suppression measures targeted at the spread of COVID-19. It is not intended to replace the SIR/SEIR modeling literature, especially as this model relies on SIR/SEIR estimates for full identification. If anything, this modeling exercise shows why SIR/SEIR models are so important: without them it is literally impossible to know the total number of infected people on a given day. This model’s simplicity and ability to use empirical data are its main features, and the hope is that it can be used and extended by researchers looking at government policies and other tertiary factors on the spread of the disease.&lt;/p&gt;
&lt;p&gt;To fit the model, it is necessary to have at least an estimate of how many tests have been conducted. I am currently collecting this data and am open to any additional datasets that may be available. I am also involved in a collaboration known as &lt;a href=&#34;https://lumesserschmidt.github.io/CoronaNet/&#34;&gt;CoronaNet&lt;/a&gt; to collect full information on government responses to the pandemic with the intention of better understanding why governments impose certain policies and what the effect of them can be.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;bibliography&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1 class=&#34;unnumbered&#34;&gt;Bibliography&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references hanging-indent&#34;&gt;
&lt;div id=&#34;ref-lourenco2020&#34;&gt;
&lt;p&gt;Jose Lourenco, Mahan Ghafari, Robert Paton. 2020. “Fundamental Principles of Epidemic Spread Highlight the Immediate Need for Large-Scale Serological Surveys to Assess the Stage of the Sars-Cov-2 Epidemic.” &lt;em&gt;medRxiv&lt;/em&gt;. &lt;a href=&#34;https://doi.org/https://doi.org/10.1101/2020.03.24.20042291&#34;&gt;https://doi.org/https://doi.org/10.1101/2020.03.24.20042291&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ferguson2020&#34;&gt;
&lt;p&gt;Neil M Ferguson, Gemma Nedjati-Gilani, Daniel Laydon. 2020. “Impact of Non-Pharmaceutical Interventions (Npis) to Reduce Covid19 Mortality and Healthcare Demand.” &lt;em&gt;Imperial College of London Working Paper&lt;/em&gt;. &lt;a href=&#34;https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf&#34;&gt;https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-peak2020&#34;&gt;
&lt;p&gt;Peak, Corey M., Rebecca Kahn, Yonatan H. Grad, Lauren M. Childs, Ruoran Li, Marc Lipsitch, and Caroline O. Buckee. 2020. “Modeling the Comparative Impact of Individual Quarantine Vs. Active Monitoring of Contacts for the Mitigation of Covid-19.” &lt;em&gt;medRxiv&lt;/em&gt;. &lt;a href=&#34;https://doi.org/https://doi.org/10.1101/2020.03.05.20031088&#34;&gt;https://doi.org/https://doi.org/10.1101/2020.03.05.20031088&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-perkins2020&#34;&gt;
&lt;p&gt;Perkins, T. Alex, Sean M. Cavany, Sean M. Moore, Rachel J. Oidtman, Anita Lerch, and Marya Poterek. 2020. “Estimating Unobserved Sars-Cov-2 Infections in the United States.” &lt;em&gt;Working Paper&lt;/em&gt;. &lt;a href=&#34;http://perkinslab.weebly.com/uploads/2/5/6/2/25629832/perkins_etal_sarscov2.pdf&#34;&gt;http://perkinslab.weebly.com/uploads/2/5/6/2/25629832/perkins_etal_sarscov2.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-riou2020&#34;&gt;
&lt;p&gt;Riou, Julien, Anthony Hauser, Michel J. Counotte, and Christian L. Althaus. 2020. “Adjusted Age-Specific Case Fatality Ratio During the Covid-19 Epidemic in Hubei, China, January and February 2020.” &lt;em&gt;medRxiv&lt;/em&gt;. &lt;a href=&#34;https://doi.org/https://doi.org/10.1101/2020.03.04.20031104&#34;&gt;https://doi.org/https://doi.org/10.1101/2020.03.04.20031104&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-verity2020&#34;&gt;
&lt;p&gt;Robert Verity, Ilaria Dorigatti, Lucy C Okell. 2020. “Estimates of the Severity of Covid-19 Disease.” &lt;em&gt;medRxiv&lt;/em&gt;. &lt;a href=&#34;https://doi.org/https://doi.org/10.1101/2020.03.09.20033357&#34;&gt;https://doi.org/https://doi.org/10.1101/2020.03.09.20033357&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-li2020&#34;&gt;
&lt;p&gt;Ruiyun Li, Bin Chen, Sen Pei. 2020. “Substantial Undocumented Infection Facilitates the Rapid Dissemination of Novel Coronavirus (Sars-Cov2).” &lt;em&gt;Science&lt;/em&gt;. &lt;a href=&#34;https://doi.org/10.1126/science.abb3221&#34;&gt;https://doi.org/10.1126/science.abb3221&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;See article available at &lt;a href=&#34;https://www.realclearpolitics.com/video/2020/03/26/dr_birx_coronavirus_data_d&#34; class=&#34;uri&#34;&gt;https://www.realclearpolitics.com/video/2020/03/26/dr_birx_coronavirus_data_d&lt;/a&gt;&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;For a discussion, see &lt;a href=&#34;https://www.realclearpolitics.com/articles/2020/03/18/the_perils_of_mass_coronavirus_testing_142693.html&#34; class=&#34;uri&#34;&gt;https://www.realclearpolitics.com/articles/2020/03/18/the_perils_of_mass_coronavirus_testing_142693.html&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;For more information on this principle, see Daniel Lakeland’s discussion of “masking functions” &lt;a href=&#34;http://models.street-artists.org/2019/09/04/informative-priors-from-masking/&#34; class=&#34;uri&#34;&gt;http://models.street-artists.org/2019/09/04/informative-priors-from-masking/&lt;/a&gt;.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Conjoint Survey Experiments</title>
      <link>/post/conjoint_power_simulation/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      <guid>/post/conjoint_power_simulation/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Conjoint survey experiments have become more popular in political science since the publication of &lt;a href=&#34;http://hdl.handle.net/1721.1/84064&#34;&gt;Hainmueller, Hopkins and Yamamoto (2014)&lt;/a&gt;. However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see &lt;a href=&#34;https://osf.io/preprints/socarxiv/mrfcu/&#34;&gt;SocArchiv Draft&lt;/a&gt;). I employ both traditional power measures and newer statistics from &lt;a href=&#34;http://journals.sagepub.com/doi/abs/10.1177/1745691614551642&#34;&gt;Gelman and Carlin (2014)&lt;/a&gt; reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).&lt;/p&gt;
&lt;p&gt;The original Rmarkdown and saved simulation files can be downloaded from the &lt;a href=&#34;https://github.com/saudiwin/saudiwin.github.io/tree/sources/content&#34;&gt;site’s Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The packages required to run this simulation are listed in the code block below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Required packages
require(ggplot2)
require(dplyr)
require(tidyr)
require(multiwayvcov)
require(lmtest)
require(stringr)
require(kableExtra)

# package MASS also used but not loaded

# != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows.
# to install parallelsugar:
# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_github(&amp;#39;nathanvan/parallelsugar&amp;#39;)

# If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used

if(.Platform$OS.type==&amp;#39;windows&amp;#39;) {
  parallelfunc &amp;lt;- parallelsugar::mclapply_socket
} else {
  parallelfunc &amp;lt;- parallel::mclapply
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation Set-up&lt;/h2&gt;
&lt;p&gt;The following parameters control the range of coefficients tested and the number of simulations. The survey experiment design employs vignettes in which appeals and the actors making appeals are allowed to vary between respondents. Any one vignette has one actor and one appeal. The probability of assignment is assumed to be a simple random fraction of the number of appeal-actor combinations (14). If &lt;code&gt;run_sim&lt;/code&gt; is set to &lt;code&gt;TRUE&lt;/code&gt;, the simulation is run, otherwise the simulation results are loaded from an RDS file and plotted. Running the simulation will take approximately 6 to 12 hours depending on the number of cores and speed of the CPU.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Actually run the simulation or just load the data and look at it?
run_sim &amp;lt;- FALSE
# Max number of respondents fixed at 2700
num_resp &amp;lt;- 2700
# Number of iterations (breaks in sample size)
num_breaks &amp;lt;- 300
# Number of simulations to run per iteration
n_sims &amp;lt;- 1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then create a grid of all possible actor-appeal combinations as I am using simple randomization of profiles before presenting them to respondents. There are two vectors of treatments (actors and appeals) that each have 7 separate treatments for a total of 14 separate possible treatments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Two treatment variables producing a cross-product of 7x7
treatments1 &amp;lt;- c(&amp;#39;military&amp;#39;,&amp;#39;MOI&amp;#39;,&amp;#39;president&amp;#39;,&amp;#39;MOJ&amp;#39;,&amp;#39;parliament&amp;#39;,&amp;#39;municipality&amp;#39;,&amp;#39;government&amp;#39;)
treatments2 &amp;lt;- c(&amp;#39;exprop.firm&amp;#39;,&amp;#39;exprop.income&amp;#39;,&amp;#39;permit.reg&amp;#39;,&amp;#39;contracts.supply&amp;#39;,&amp;#39;permit.export&amp;#39;,&amp;#39;permit.import&amp;#39;,&amp;#39;reforms&amp;#39;)
total_treat &amp;lt;- length(c(treatments1,treatments2))
grid_pair &amp;lt;- as.matrix(expand.grid(treatments1,treatments2))
print(head(grid_pair))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Var1           Var2         
## [1,] &amp;quot;military&amp;quot;     &amp;quot;exprop.firm&amp;quot;
## [2,] &amp;quot;MOI&amp;quot;          &amp;quot;exprop.firm&amp;quot;
## [3,] &amp;quot;president&amp;quot;    &amp;quot;exprop.firm&amp;quot;
## [4,] &amp;quot;MOJ&amp;quot;          &amp;quot;exprop.firm&amp;quot;
## [5,] &amp;quot;parliament&amp;quot;   &amp;quot;exprop.firm&amp;quot;
## [6,] &amp;quot;municipality&amp;quot; &amp;quot;exprop.firm&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;To simulate the data, I first sample 14 coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; (one for each treatment &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;) from a normal distribution with mean zero and standard deviation one. I then randomly sample from two profile combinations for each of the &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; respondents in accordance with simple random sampling. Two profile combinations, for a total of four tasks &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, are selected to reflect the fact that paired vignettes will be shown to each respondent as in the study design. I also sample a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Z_I\)&lt;/span&gt; that is a random binomial vector with probability of 0.2 (thus 20% of respondents will fall into this cell). A treatment interaction effect &lt;span class=&#34;math inline&#34;&gt;\(\beta_z\)&lt;/span&gt; is sampled from a normal distribution with mean 0.5 and standard deviation of 0.3 to provide a sampling distribution for the true effect, instead of assuming that the true effect is a fixed population value. Adding a distribution for &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; reflects additional uncertainty beyond standard sampling distribution uncertainty. In this case, it represents additional measurement error between the true concept and the indicators used in the survey design.&lt;/p&gt;
&lt;p&gt;I also post-stratify some estimates with a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_I\)&lt;/span&gt; from a binomial distribution of probability .5 that has a constant effect on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(+1\)&lt;/span&gt; (representing a fixed effect).&lt;/p&gt;
&lt;p&gt;I then randomly sample a pair of outcomes, for a total of four tasks &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; in the range of &lt;span class=&#34;math inline&#34;&gt;\([1,10]\)&lt;/span&gt; by drawing a number from a multivariate normal distribution. The mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_{it}\)&lt;/span&gt; of this normal distribution is equal to a linear model with an intercept of 5, the 14 dummy variables for treatment indicators &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; with associated coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt;, the interaction &lt;span class=&#34;math inline&#34;&gt;\(\beta_z\)&lt;/span&gt; between the pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{ij}\)&lt;/span&gt;, and a post-stratification covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_i\)&lt;/span&gt;. To simplify matters, &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; is not given its own constituent term as I am not interested in the unconditional effect of &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt;, only the effect of &lt;span class=&#34;math inline&#34;&gt;\(X_{ijt}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Z_{i}\)&lt;/span&gt;. Finally, I draw correlated errors from a multivariate normal distribution with mean of zero and length of 4 (equal to the number of tasks per respondent) to produce a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; variance matrix &lt;span class=&#34;math inline&#34;&gt;\(\varSigma_i\)&lt;/span&gt; with a diagonal of 4 and intra-respondent covariation of 1 (correlation of 0.5).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X_{ITJ} &amp;amp;\sim  \mathrm{B} \Big( \frac{1}{J \times 2} \Big)\\
B_{J} &amp;amp;\sim  \mathrm{N}(0,2)\\
\beta_z &amp;amp;\sim \mathrm{N}(0.5,0.3)\\
Z_I &amp;amp;\sim  \mathrm{B}(0.2)\\
Q_I &amp;amp;\sim  \mathrm{B}(0.5)\\
\mu_{it} &amp;amp;=  5 + \sum_{j=1}^{J} \sum_{t=1}^{T} \beta_j * X_{itj} + \beta_z * X_{it1} *Z_i + Q_i\\
Y_{it} &amp;amp;\sim  \mathrm{N}(\mu_{it},\varSigma_i)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This process will produce some numbers outside the &lt;span class=&#34;math inline&#34;&gt;\([1,10]\)&lt;/span&gt; range; however, it is better to leave these values in as explicit truncation will violate the assumptions of the underlying causal model.&lt;/p&gt;
&lt;p&gt;I run 1000 simulations for each of 300 sequential sample sizes ranging from 100 to 2700. I then take the mean significant effect and report that as the likely significant effect size for that sample size. I also record the ratio of draws for which the effect is significant (the power). However, given that the true effect is not fixed, I interpret power as the ability detect a true effect greater than zero. I record both unadjusted p-values and p-values adjusted using the &lt;code&gt;cluster.vcov&lt;/code&gt; function from the multiwayvcov package by clustering around respondent ID to reflect the pairing of vignettes. I also use separate results when post-stratifying on a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In addition, I included M-errors (error of absolute magnitude of significant coefficients) and S-errors (incorrect sign of significant coefficients). M-errors provide an estimate of publication bias given that the &lt;span class=&#34;math inline&#34;&gt;\(p=0.05\)&lt;/span&gt; threshold is a hard boundary and will necessarily result in smaller effects being reported as statistically insignificant when in fact they are greater than zero. S-errors help determine the probability that an estimated effect is the correct sign even if it is significant. S-errors are particularly problematic in small samples when sampling error can produce large negative deviations that may be statistically significant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(run_sim==TRUE) {
  file.create(&amp;#39;output_log.txt&amp;#39;,showWarnings = FALSE)
    
    # Need to randomize over the simulations so that parallelization works correctly on windows
    
    sampled_seq &amp;lt;- sample(seq(100,num_resp,length.out = num_breaks))
    
  all_sims &amp;lt;- parallelfunc(sampled_seq,function(x) {
  
    out_probs &amp;lt;- 1:n_sims
    cat(paste0(&amp;quot;Now running simulation on data sample size &amp;quot;,x),file=&amp;#39;output_log.txt&amp;#39;,sep=&amp;#39;\n&amp;#39;,append=TRUE)
  
    out_data &amp;lt;- lapply(1:n_sims, function(j) {
      
      total_probs &amp;lt;- sapply(1:x,function(x) {
        treat_rows &amp;lt;- sample(1:nrow(grid_pair),4)
        treatments_indiv &amp;lt;- c(grid_pair[treat_rows,])
        return(treatments_indiv)
      })
      
      by_resp &amp;lt;- t(total_probs)
      by_resp &amp;lt;- as_data_frame(by_resp)
      names(by_resp) &amp;lt;- c(paste0(&amp;#39;actor.&amp;#39;,1:4,&amp;quot;_cluster&amp;quot;,c(1,1,2,2)),paste0(&amp;#39;gift.&amp;#39;,1:4,&amp;quot;_cluster&amp;quot;,c(1,1,2,2)))
      by_resp$respondent &amp;lt;- paste0(&amp;#39;Respondent_&amp;#39;,1:nrow(by_resp))
      by_resp &amp;lt;- gather(by_resp,attribute,indicator,-respondent) %&amp;gt;% separate(attribute,into=c(&amp;#39;attribute&amp;#39;,&amp;#39;cluster&amp;#39;),sep=&amp;#39;_&amp;#39;) %&amp;gt;% 
        separate(attribute,into=c(&amp;#39;attribute&amp;#39;,&amp;#39;task&amp;#39;)) %&amp;gt;% spread(attribute,indicator)
      
      
      # Assign true coefficients for treatments
  
      #Beta_js
      
      coefs &amp;lt;- data_frame(coef_val=rnorm(n=length(c(treatments1,treatments2)),mean=0,sd=1),
                          treat_label=c(treatments1,treatments2))
      
      # Create cluster covariance in the errors
      
      sigma_matrix &amp;lt;- matrix(2,nrow=4,ncol=4)
      diag(sigma_matrix) &amp;lt;- 4
  
      # Add on the outcome as a normal draw, treatment coefficients, interaction coefficient, group errors/interaction by respondent
      
      by_resp &amp;lt;- gather(by_resp,treatment,appeal_type,actor,gift) %&amp;gt;% 
        left_join(coefs,by=c(&amp;#39;appeal_type&amp;#39;=&amp;#39;treat_label&amp;#39;))
      
      # Record interaction coefficient (true estimate of interest)
      
      true_effect &amp;lt;- rnorm(n=1,mean=0.5,sd=0.3)
      
      by_resp &amp;lt;- select(by_resp,-treatment) %&amp;gt;% spread(appeal_type,coef_val) %&amp;gt;%  group_by(respondent) %&amp;gt;% mutate(error=MASS::mvrnorm(1,mu=rep(0,4),Sigma=sigma_matrix)) %&amp;gt;% ungroup
      
      # interaction coefficient only in function if military==TRUE
      
      by_resp &amp;lt;- mutate(by_resp,int_coef=true_effect*rbinom(n = n(),prob = 0.2,size=1),
                        int_coef=if_else(military!=0,int_coef,0))
      by_resp &amp;lt;- lapply(by_resp, function(x) {
        if(is.double(x)) {
          x[is.na(x)] &amp;lt;- 0
        }
        return(x)
      }) %&amp;gt;% as_data_frame
      
      # To make the outcome, need to turn the dataset long
      # However, we now need to drop the reference categories
      # Drop one dummy from actor/gift to prevent multicollinearity = reforms + government combination
      
      out_var &amp;lt;- gather(by_resp,var_name,var_value,-respondent,-task,-cluster) %&amp;gt;% 
         filter(!(var_name %in% c(&amp;#39;reforms&amp;#39;,&amp;#39;government&amp;#39;))) %&amp;gt;% 
        group_by(respondent,task) %&amp;gt;% summarize(outcome=sum(var_value)+5)
      
      combined_data &amp;lt;- left_join(out_var,by_resp,by=c(&amp;#39;respondent&amp;#39;,&amp;#39;task&amp;#39;))
      
      
      # Re-estimate with a blocking variable
      
  
      combined_data$Q &amp;lt;- c(rep(1,floor(nrow(combined_data)/2)),
                           rep(0,ceiling(nrow(combined_data)/2)))
      
      combined_data$outcome &amp;lt;- if_else(combined_data$Q==1,combined_data$outcome+1,
                                       combined_data$outcome)
      
      # # Create data predictor matrix and estimate coefficients from the simulated dataset
      # 
      to_lm &amp;lt;- ungroup(combined_data) %&amp;gt;% select(contracts.supply:reforms,int_coef,Q)
      to_lm &amp;lt;- mutate_all(to_lm,funs(if_else(.!=0,1,.))) %&amp;gt;% mutate(outcome=combined_data$outcome)
      
      #No post-stratification
      # I don&amp;#39;t estimate a constituent term for int_coef because it is assumed to be zero
      
      results &amp;lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +
                      parliament + permit.export + permit.import + permit.reg + president + 
                      int_coef:military,data=to_lm)
      
      results_clust &amp;lt;- cluster.vcov(results,cluster = combined_data$respondent)
      pvals_adj &amp;lt;- coeftest(results,vcov.=results_clust)[-1,4]&amp;lt;0.05
      pvals_orig &amp;lt;- coeftest(results)[-1,4]&amp;lt;0.05
      
      total_sig_orig &amp;lt;- mean(pvals_orig)
      total_sig_adj &amp;lt;- mean(pvals_adj)
      
      int_sig_orig &amp;lt;- pvals_orig[&amp;#39;military:int_coef&amp;#39;]
      int_sig_adj &amp;lt;- pvals_adj[&amp;#39;military:int_coef&amp;#39;]
      
      
      # Now run the poststratification model
      
      results_ps &amp;lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +
                      parliament + permit.export + permit.import + permit.reg + president + 
                      int_coef:military + Q,data=to_lm)
      
      results_clust &amp;lt;- cluster.vcov(results,cluster = combined_data$respondent)
      pvals_adj &amp;lt;- coeftest(results_ps,vcov.=results_clust)[-1,4]&amp;lt;0.05
      pvals_orig &amp;lt;- coeftest(results_ps)[-1,4]&amp;lt;0.05
      
      total_sig_orig_blocker &amp;lt;- mean(pvals_orig)
      total_sig_adj_blocker &amp;lt;- mean(pvals_adj)
      
      int_sig_orig_blocker &amp;lt;- pvals_orig[&amp;#39;military:int_coef&amp;#39;]
      int_sig_adj_blocker &amp;lt;- pvals_adj[&amp;#39;military:int_coef&amp;#39;]
      
      out_results &amp;lt;- data_frame(int_sig_adj,int_sig_orig,int_sig_adj_blocker,int_sig_orig_blocker,
                                total_sig_adj,total_sig_orig,total_sig_adj_blocker,
                                total_sig_orig_blocker,abs_true_effect=abs(true_effect),
                                true_effect=true_effect,
                                est_effect=coef(results)[&amp;#39;military:int_coef&amp;#39;],
                                est_effect_ps=coef(results)[&amp;#39;military:int_coef&amp;#39;])
    })
    out_data &amp;lt;- bind_rows(out_data)
    
    return(out_data)
  },mc.cores=parallel::detectCores(),mc.preschedule=FALSE)
  #save the data for inspection
  
  all_sims_data &amp;lt;- bind_rows(all_sims) %&amp;gt;% mutate(sample_size=rep(sampled_seq,each=n_sims),
                                                  iter=rep(1:n_sims,times=num_breaks))

}

if(run_sim==TRUE) {
saveRDS(object = all_sims_data,file=&amp;#39;all_sims_data.rds&amp;#39;)
} else {
  all_sims_data &amp;lt;- readRDS(&amp;#39;all_sims_data.rds&amp;#39;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simulation yields a row with the significant effect of the interaction term for that simulation for a total of &lt;code&gt;n_sims&lt;/code&gt; draws. From this raw data I am able to calculate all of the necessary statistics mentioned above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add in different calculations

all_sims_data &amp;lt;- group_by(all_sims_data,sample_size)  %&amp;gt;% mutate(sigeffVorig=ifelse(int_sig_orig,
                                                                                     est_effect,
                                                                                     NA),
sigeffVadj=ifelse(int_sig_adj,est_effect,NA),
sigeffVps_orig=ifelse(int_sig_orig_blocker,est_effect_ps,NA),
sigeffVps_adj=ifelse(int_sig_adj_blocker,est_effect_ps,NA),
powerVorig=int_sig_orig &amp;amp; (true_effect&amp;gt;0),
powerVadj=int_sig_adj &amp;amp; (true_effect&amp;gt;0),
powerVps_orig=int_sig_orig_blocker &amp;amp; (true_effect &amp;gt; 0),
powerVps_adj=int_sig_adj_blocker &amp;amp; (true_effect &amp;gt; 0),
SerrVorig=ifelse(int_sig_orig,1-(sign(est_effect)==sign(true_effect)),NA),
SerrVadj=ifelse(int_sig_adj,1-(sign(est_effect)==sign(true_effect)),NA),
SerrVps_orig=ifelse(int_sig_orig_blocker,
                    1-(sign(est_effect_ps)==sign(true_effect)),NA),
SerrVps_adj=ifelse(int_sig_adj_blocker,
                   1-(sign(est_effect_ps)==sign(true_effect)),NA),
MerrVorig=ifelse(int_sig_orig,abs(est_effect)/abs_true_effect,NA),
MerrVadj=ifelse(int_sig_adj,abs(est_effect)/abs_true_effect,NA),
MerrVps_orig=ifelse(int_sig_orig_blocker,abs(est_effect_ps)/abs_true_effect,NA),
MerrVps_adj=ifelse(int_sig_adj_blocker,abs(est_effect_ps)/abs_true_effect,NA))

long_data &amp;lt;- select(all_sims_data,matches(&amp;#39;V|sample|iter&amp;#39;)) %&amp;gt;% gather(effect_type,result,-sample_size,-iter) %&amp;gt;% separate(effect_type,into=c(&amp;#39;estimate&amp;#39;,&amp;#39;estimation&amp;#39;),sep=&amp;#39;V&amp;#39;) %&amp;gt;% 
  mutate(estimate=factor(estimate,levels=c(&amp;#39;sigeff&amp;#39;,&amp;#39;power&amp;#39;,&amp;#39;Serr&amp;#39;,&amp;#39;Merr&amp;#39;),
                         labels=c(&amp;#39;Mean\nSignificant\nEffect&amp;#39;,
                                  &amp;#39;Mean\nPower&amp;#39;,
                                  &amp;#39;S-Error\nRate&amp;#39;,
                                  &amp;#39;M-Error\nRate&amp;#39;)),
         estimation=factor(estimation,levels=c(&amp;#39;adj&amp;#39;,&amp;#39;orig&amp;#39;,&amp;#39;ps_adj&amp;#39;,&amp;#39;ps_orig&amp;#39;),
                           labels=c(&amp;#39;No Post-Stratification\nClustered Errors\n&amp;#39;,
                                    &amp;#39;No Post-Stratification\nUn-clustered Errors\n&amp;#39;,
                                    &amp;#39;Post-Stratification\nClustered Errors\n&amp;#39;,
                                    &amp;#39;Post-Stratification\nUn-clustered Errors\n&amp;#39;)))

long_data_treatment &amp;lt;- select(all_sims_data,matches(&amp;#39;total|iter|sample&amp;#39;)) %&amp;gt;% gather(effect_type,result,-sample_size,-iter) %&amp;gt;%
mutate(effect_type=factor(effect_type,levels=c(&amp;#39;total_sig_adj&amp;#39;,
                                               &amp;#39;total_sig_orig&amp;#39;,
                                               &amp;#39;total_sig_adj_blocker&amp;#39;,
                                               &amp;#39;total_sig_orig_blocker&amp;#39;),
                          labels=c(&amp;#39;No Post-Stratification\nClustered Errors\n&amp;#39;,
                                   &amp;#39;No Post-Stratification\nUn-clustered Errors\n&amp;#39;,
                                   &amp;#39;Post-Stratification\nClustered Errors\n&amp;#39;,
                                   &amp;#39;Post-Stratification\nUn-clustered Errors\n&amp;#39;)))



# Plot a sample of the data (too big to display all of it)

long_data %&amp;gt;% ungroup %&amp;gt;% 
  slice(1:10) %&amp;gt;% 
  select(-estimation) %&amp;gt;% 
  mutate(estimate=str_replace(estimate,&amp;quot;\\n&amp;quot;,&amp;quot; &amp;quot;)) %&amp;gt;% 
  knitr::kable(.) %&amp;gt;% 
  kable_styling(font_size = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 8px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sample_size
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
iter
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
estimate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
result
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6298429
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3874088
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1438379
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5653086
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2689594
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting&lt;/h2&gt;
&lt;p&gt;I use the &lt;code&gt;gam&lt;/code&gt; function in the ggplot2 package to plot a smoothed regression line of the simulation draws for each sample size.&lt;/p&gt;
&lt;p&gt;First we can look at the difference that clustered errors makes across the different statistics. The only noticeable differences are at sample sizes smaller than 500. Clustering on respondents tends to result in smaller average significant effects, but it also results in increases in sign errors. This finding differs from the literature that considers clustering important to control for intra-respondent correlation, which in this simulation was fixed at 0.5. At sample sizes larger than 500, there does not appear to be any difference between clustered and un-clustered estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,grepl(&amp;#39;No Post&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;clust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I look at post-stratification as an option to improve the precision of estimates. For unclustered errors reported below, post-stratified estimates do have higher power and slightly lower average significant effects, and importantly, the post-stratified estimates worsen neither type S nor type M errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,grepl(&amp;#39;Un-clustered&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;post_unclust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Post-stratification appears to have a similar effect on clustered error estimations, although the differences are smaller. In smaller samples, post-stratified estimates do have smaller M-errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,!grepl(&amp;#39;Un-clustered&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Conjoint_Power_Simulation_files/figure-html/unclustered_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;post_clust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I also report average numbers of significant coefficients for the 14 treatments. Given that the 14 treatments were sampled from a normal distribution with prior density in the positive values with a meaan of 0.5, in expectation 95% of estimates should be statisticall significant. While that upper limit is reached only in high sample numbers, it looks like the ratio for treatment effects reaches an acceptable level of 70 percent at about 500 sample respondents. Also, post-stratifying un-clustered models results in effects that are reported as significant at much higher rates, as would follow from the previous results about post-stratification.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
ggplot(long_data_treatment,aes(y=result,x=sample_size,linetype=effect_type,colour=effect_type)) +
  theme_minimal() + stat_smooth() +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Dark2&amp;#39;) + guides(linetype=g_title,colour=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;all_treat_rate.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This simulation study shows that a sample size of approximately 1,000 respondents is enough to obtain high power while also lowering both the S and M-error rates for treatment interaction effects in this conjoint experiment. The treatment effects themselves are generally of high quality once the sample size reaches 500 because the total number of respondents in each treatment cell is considerably higher than in an interaction. Post-stratification appears to be a useful strategy to increase precision without inducing S or M errors; at the very least, post-stratification does not appear to have any adverse effects on the estimation.&lt;/p&gt;
&lt;p&gt;On the other hand, it appears that clustering errors increases the S-error rate at small sample sizes, a surprising finding considering that clustering methods are designed to inflate, not deflate, standard errors. Given that the S-error rate reveals the likelihood of making an error about the sign of the treatment effect, this is a potentially serious problem. For that reason I intend to report both clustered and un-clustered estimates in my analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>idealstan: an R Package for Ideal Point Modeling with Stan</title>
      <link>/post/stancon2018_paper/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      <guid>/post/stancon2018_paper/</guid>
      <description>
&lt;script src=&#34;/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;/rmarkdown-libs/datatables-binding/datatables.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;This is a paper that was presented at the &lt;a href=&#34;http://mc-stan.org/events/stancon2018/&#34;&gt;StanCon2018 conference&lt;/a&gt; on Bayesian inference with the Stan Hamiltonian Markov Chain Monte Carlo (MCMC) method. Video of my talk is available &lt;a href=&#34;https://www.youtube.com/watch?v=0ZjrLOosXwk&amp;amp;t=4s&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This notebook introduces &lt;a href=&#34;https://cran.r-project.org/package=idealstan&#34;&gt;&lt;code&gt;idealstan&lt;/code&gt;&lt;/a&gt;, a new R package front-end to &lt;a href=&#34;http://www.mc-stan.org&#34;&gt;&lt;code&gt;Stan&lt;/code&gt;&lt;/a&gt; that allows for flexible modeling of a class of latent variable models known as ideal point models. Ideal point modeling is a form of dimension reduction, and shares similarities with multi-dimensional scaling, factor analysis and item-response theory. While the parameterization employed by &lt;code&gt;idealstan&lt;/code&gt; is a derivation based on item-response theory, it is important to note that several other parameterizations are possible &lt;a name=cite-carroll2013&gt;&lt;/a&gt;&lt;a name=cite-armstrong2014&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-carroll2013&#34;&gt;Carroll, Lewis, Lo, Poole, and Rosenthal, 2013&lt;/a&gt;; &lt;a href=&#34;#bib-armstrong2014&#34;&gt;Armstrong, Bakker, Carroll, Hare, Poole, and Rosenthal, 2014&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;What distinguishes ideal point modeling from other latent space models is that the latent factor is bi-directional: in other words, the unobserved latent construct that underlies the data could increase or decrease as the observed stimuli either increase or decrease. This type of latent variable is very useful when the aim is to cluster units around a polarizing dimension, such as the left-right axis in politics or the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value debate in statistics. While many of the applications of ideal point models are in political science and economics, it certainly can be applied more broadly, especially as the approach is related to latent space models more generally, such as in &lt;a name=cite-hoff2002&gt;&lt;/a&gt;&lt;a href=&#34;#bib-hoff2002&#34;&gt;Hoff, Raftery, and Hancock (2002)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are well-established frequentist (&lt;a href=&#34;https://cran.r-project.org/web/packages/emIRT/index.html&#34;&gt;&lt;code&gt;emIRT&lt;/code&gt;&lt;/a&gt;) and Bayesian (&lt;a href=&#34;https://cran.r-project.org/web/packages/pscl/pscl.pdf&#34;&gt;&lt;code&gt;pscl&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/MCMCpack/index.html&#34;&gt;&lt;code&gt;MCMCpack&lt;/code&gt;&lt;/a&gt;) packages for ideal point inference with R. However, these packages offer standard models that are limited in scope to particular problems, especially when it comes to including predictors in the latent space. The R package &lt;code&gt;idealstan&lt;/code&gt; has three features that set it apart from existing approaches:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Stan offers significant flexibility compared to existing approaches in using non-conjugate priors and a clearer programming interface. This flexibility allows for more options for end users in ideal point modeling that can be used to test new theories and hypotheses instead of being limited in the range and type of models available. In particular, Stan makes it easier to add hierarchical and time-series priors on to parameters, which opens the door to further analysis of dynamic and time-varying social phenomena.&lt;/li&gt;
&lt;li&gt;Stan scales well with the number of parameters, which is an attribute of most dimension reduction methods. Advances in Hamiltonian Monte Carlo estimation, including the use of variational inference and soon parallel computing within chains, promise to make full Bayesian inference of these models practical even with very large datasets.&lt;/li&gt;
&lt;li&gt;Increasingly, Stan is being married to helpful and powerful diagnostic packages, including &lt;a href=&#34;https://cran.r-project.org/web/packages/bayesplot/index.html&#34;&gt;&lt;code&gt;bayesplot&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/shinystan/index.html&#34;&gt;&lt;code&gt;shinystan&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/loo/index.html&#34;&gt;&lt;code&gt;loo&lt;/code&gt;&lt;/a&gt;, which extends the ability of &lt;code&gt;idealstan&lt;/code&gt; to provide not only cutting-edge Bayesian inference but also increasingly sophisticated tools for graphical analysis of resulting estimates. Given the complex nature of latent variable models, diagonistics are extremely important for determining when the model is behaving as it should (and what the model should be doing in the first place).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/saudiwin/idealstan&#34;&gt;&lt;code&gt;idealstan&lt;/code&gt;&lt;/a&gt; is an effort to build on prior efforts but also to offer new models that satisfy the increasing variety of applications to which ideal point models are being put. Both &lt;code&gt;pscl&lt;/code&gt; and &lt;code&gt;MCMCpack&lt;/code&gt; were designed for ideal point modeling of binary (logit) data from legislatures in which the outcome is made up of yes and no votes cast by legislators &lt;a name=cite-jackman2004&gt;&lt;/a&gt;&lt;a name=cite-quinn2002&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers, 2004&lt;/a&gt;; &lt;a href=&#34;#bib-quinn2002&#34;&gt;Martin and Quinn, 2002&lt;/a&gt;). More recently scholars have begun applying ideal point models to Twitter data &lt;a name=cite-barbera2015&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-barbera2015&#34;&gt;Barberá, 2015&lt;/a&gt;), massive campaign finance datasets &lt;a name=cite-bonica2014&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-bonica2014&#34;&gt;Bonica, 2014&lt;/a&gt;) and party campaign manifestos &lt;a name=cite-slapin2008&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-slapin2008&#34;&gt;Slapin and Proksch, 2008&lt;/a&gt;). This package offers both the traditional forms of ideal point models along with new extensions, including a version of ideal points that can take into account certain forms of missing data.&lt;/p&gt;
&lt;p&gt;In this notebook I first introduce ideal-point models and contrast them with “traditional” item response theory (IRT), and then I demonstrate the &lt;code&gt;idealstan&lt;/code&gt; package through simulations. I then perform two empirical analyses, the first of coffee product ratings from Amazon and the second with voting data taken from the 114th Senate. In the second example, I also show how &lt;code&gt;idealstan&lt;/code&gt; enables new estimation of strategic legislator absence, a type of data that is usually coded as missing in vote datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ideal-point-models-as-a-subset-of-statistical-measurement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ideal Point Models as a Subset of Statistical Measurement&lt;/h2&gt;
&lt;p&gt;Measurement error models have a long history in applied statistics and are increasingly in demand as the amount of noisy observational data grows with the digital revolution. Canonical statistical models, particularly linear regression, assume that the predictors are measured without error, but in many situations in social science, the variable of interest cannot in fact be measured. Rather, proxies or indicators are used to stand in for the latent construct. Measurement models offer a way to match the indicators with the latent construct while making appropriate assumptions about the relationship.&lt;/p&gt;
&lt;p&gt;In other words, we suppose that the regression predictor matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is itself a function of certain indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c \in \{1 ... C\}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
X = f(\forall I\_c \in \{1 ... C\})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Different latent variable, latent space and measurement models can be distinguished in terms of the function &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt;. Fundamentally, &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; has to stipulate how &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; will change as the indicators change. It is possible to make very precise conditions for this relationship, which is the method applied in confirmatory factor analysis and structural equation modeling. It is also possible to let &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; make only minimal assumptions about this relationship, which is the method adopted by exploratory factor analysis and item-response theory (IRT), in addition to the other latent space models in the literature. It is also possible, of course, to have models that fall somewhere in between truly exploratory and truly confirmatory models.&lt;/p&gt;
&lt;p&gt;While much has been written about the different kids of &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; that can be used in measurement models, in this paper I focus on a metric relevant to ideal point models: as the values of &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt; increase, does &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; also increase so that &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; must be always non-decreasing? If that is the case, then the model can be thought of as falling into the domain of item-response theory and traditional factor analysis (in fact, IRT is itself a non-linear version of factor analysis per &lt;a name=cite-takane1986&gt;&lt;/a&gt;&lt;a href=&#34;#bib-takane1986&#34;&gt;Takane and de Leeuw (1986)&lt;/a&gt;). By contrast, ideal point models are based on a different set of assumptions in which &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; could increase or decrease as the indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt; increase or decrease; in other words, the relationship is bi-polar instead of uni-polar.&lt;/p&gt;
&lt;p&gt;Ideal point modeling originated from analysis of a common situation in policy research in which different legislators select from among competing policy alternatives based on the utility offered by each policy &lt;a name=cite-enelow198&gt;&lt;/a&gt;&lt;a name=cite-poole2008&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-enelow198&#34;&gt;Enelow and Hinich, 1984&lt;/a&gt;; &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&#34;&gt;Poole, Lewis, Lo, and Carroll, 2008&lt;/a&gt;). Supposing that the policies are evaluated on a uni-dimensional space, if the utility of the “Yes” position on the policy is greater than the utility of the “No” position to a particular legislator, then that legislator will choose that policy. While very simple, this model has a requirement that is different from much of traditional IRT estimation: the policy outcomes can have different meanings based on their position relative to the legislator in the policy space. For example, if a legislator is very liberal (has a high value on the latent construct), then that legislator is likely to vote yes on bills that are also liberal, such as single-payer health care. But if the bill is conservative, such as national defense, then that legislator is more likely to vote no. In other words, the meaning of the positions of the bills in the latent space depends on the ideal point of the legislator, and thus the mapping to the latent space &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; cannot be always non-decreasing. For some votes, a yes position may mean a high value of the latent space, while for others it may mean a lower value.&lt;/p&gt;
&lt;p&gt;By comparison, in traditional IRT, responses to stimuli (the indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt;) reflect correct or incorrect answers, such as a student taking a test. Correct answers, which in the legislative context mean yes votes, always equal greater ability, while incorrect answers, which would be no votes, are always a sign of less ability. In this situation, &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; is always non-decreasing. Thus while ideal point models share much in common with IRT, they require different assumptions.&lt;/p&gt;
&lt;p&gt;A brief review of the mathematical notation will make the difference clear. The 2-PL IRT model takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y\_{ij} = \alpha\_j x\_i - \beta\_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; can be an outcome of any type (binary, ordinal, Poisson, etc.) that includes the correct or incorrect responses of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; test takers to &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; test items, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; are the discrimination parameters that control the narrowness, hence discrimination, of each item &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; on the test in the latent space, &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; are the test-taker ability parameters that reflect the ability of a person to answer an item correctly, and &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt; are the difficulty or average probability/value of a correct response (the intercept). In the traditional IRT format, the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; discrimination parameters are always positive because a higher value (a correct answer) is always associated with higher ability &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;, while a lower value (an incorrect answer) is associated with less ability.&lt;/p&gt;
&lt;p&gt;However, if the requirement that the discrimination parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; are positive is removed, then this model can also be used for ideal point modeling. For most applications, ideal-point modeling using IRT is built on the standard 2-PL model &lt;a name=cite-gelman2005&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers, 2004&lt;/a&gt;; &lt;a href=&#34;#bib-gelman2005&#34;&gt;Bafumi, Gelman, Park, and Kaplan, 2005&lt;/a&gt;), with one notable exception: the discrimination parameters must be un-constrained, while in traditional IRT the discrimination parameters are constrained to all be either positive or negative. Constraining discrimination parameters to be positive is also the approach taken in the &lt;a href=&#34;https://cran.r-project.org/web/packages/edstan/vignettes/briefmanual.html&#34;&gt;&lt;code&gt;edstan&lt;/code&gt;&lt;/a&gt; package, which offers the full range of standard IRT models using Stan.&lt;/p&gt;
&lt;p&gt;Leaving the discrimination parameters un-consrained ensures that a higher or lower value of &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; could be associated with either a “correct” or “incorrect” answer as is necessary in an ideal point model. This happens because the latent space in an ideal point model is fundamentally a Euclidean distance that is rotation-invariant. The following figure makes this clear by showing a version of an ideal point model in which the ideal point (ability parameter) &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; is represented by a Normal distribution while the Yes and No points of a proposed policy are vertical lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/stancon2018_paper_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; is to the right of the Indifference line, the legislator will vote Yes on the proposal, whereas if she is to the left, she will vote No. However, a different policy (item) could have the Yes and No positions switched so that a Yes vote would correspond to -2.5 and vice versa for the No vote. Thus the binary outcomes do not have the same interpretation as the standard IRT model because the relationship between the outcomes and the ability points is contingent.&lt;/p&gt;
&lt;p&gt;If the discrimination parameters are unconstrained, &lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers (2004)&lt;/a&gt; showed that the midpoint of the policy/item parameter, which is labeled on the plot as the indifference line, is identified as &lt;span class=&#34;math inline&#34;&gt;\(-\frac{\alpha\_j}{\beta\_j}\)&lt;/span&gt; in the original IRT model. This midpoint is important because it represents the point in the latent space in which an actor is indifferent, or has a 50% chance of voting yes or no. Unfortunately, the IRT paramerization of the ideal point model does not return the actual Yes or No positions of a bill/item, but these midpoints are sufficient to characterize the position of the items in the latent space.&lt;/p&gt;
&lt;p&gt;While this difference between ideal point models and standard IRT may seem trivial, it has serious consequences for modeling. The basic issue is the non-identifiability of the IRT model, which the switching polarity of the discrimination parameters only makes more difficult. Ultimately, it becomes necessary to constrain the polarity of some of the ability or discrimination parameters to identify the model. However, it is necessary to choose parameters which would avoid “splitting the likelihood”, such as constraining a legislator whose true ideal point is near zero (&lt;a href=&#34;#bib-gelman2005&#34;&gt;Bafumi, Gelman, Park, et al., 2005&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;idealstan&lt;/code&gt; exploits variational Bayesian (VB) inference &lt;a name=cite-NIPS2015_5758&gt;&lt;/a&gt;(&lt;a href=&#34;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&#34;&gt;Kucukelbir, Ranganath, Gelman, and Blei, 2015&lt;/a&gt;) to assist with finding parameters for identification. &lt;code&gt;idealstan&lt;/code&gt; first estimates an IRT model with rotation-unidentified parameters, which is not a problem for a single VB run. Then &lt;code&gt;idealstan&lt;/code&gt; can select those parameters which show the highest or lowest values, and these parameters are then constrained in terms of their polarity. This approach can quickly speed up the sometimes painful process of identifying a particular IRT ideal point model, although it is also possible to constrain parameters before estimation based on prior information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data-in-ideal-point-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing Data in Ideal Point Models&lt;/h2&gt;
&lt;p&gt;Because of the flexibility of Stan,&lt;code&gt;idealstan&lt;/code&gt; is not limited to implementing the standard IRT ideal point model. One of the modifications it offers is a model that can account for one-way censoring in the data, or what is called an absence-inflated ideal point model &lt;a name=cite-kubinec2017&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-kubinec2017&#34;&gt;Kubinec, 2017a&lt;/a&gt;). This model was motivated by the application of ideal point models to legislatures in which legislator actions are more diverse than simply Yes or No votes. Legislators can also be absent on votes and in parliamentary systems as in Europe, abstentions are also common. Abstentions occur when a legislator votes on a policy but refuses to either vote Yes or No, resulting in a vote record of abstention that conventional models will treat as missing data. However, if these two additional vote outcomes–absent and abstention–are removed from the estimation by encoding them as missing data, then the additional information about legislator behavior present in this data is lost.&lt;/p&gt;
&lt;p&gt;As a solution for abstentions, I proposed in &lt;a href=&#34;#bib-kubinec2017&#34;&gt;Kubinec (2017a)&lt;/a&gt; to model abstentions as a middle category between yes and no votes by treating &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; as an ordered logistic outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ijk}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; in three possible vote choices: &lt;span class=&#34;math inline&#34;&gt;\(\{1,2,3\}\)&lt;/span&gt; (no, abstain, yes).&lt;/p&gt;
&lt;p&gt;While switching from a logit to an ordered logistic model is straightforward, modeling the censoring implied by legislator absences requires the estimation of additional parameters. To do so, I model absence as a binary outcome in a separate equation as a hurdle model. Intuitively, legislators only show up to vote if they are able to overcome the hurdle of being present. Because in an ideal point model we only want to estimate a single set of person parameters (ideal points), I include an additional set of item (bill) parameters in the hurdle component that signify the way in which being absent on a policy proposal is related to the ideological value of the legislation.&lt;/p&gt;
&lt;p&gt;The result is a two-stage model that inflates the ideal points by the probability that a legislator is absent on a particular bill. To create this model, I first start again with the essential 2-PL IRT model except that it now allows for cutpoints &lt;span class=&#34;math inline&#34;&gt;\(c\_k\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; vote outcomes (yes, abstain, no) with &lt;span class=&#34;math inline&#34;&gt;\(\zeta(\cdot)\)&lt;/span&gt; representing the logit function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
    L(\beta,\alpha,x|Y\_{ijk}) = \prod\_{i-1}^{I} \prod\_{j=1}^{J}
    \begin{cases} 
    1 -  \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_1) &amp;amp; \text{if } K = 0 \\
    \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k-1}) - \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k})       &amp;amp; \text{if } 0 &amp;lt; k &amp;lt; K, \text{ and} \\
    \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k-1}) - 0 &amp;amp; \text{if } k=K
    \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Except for the addition of the vote cutpoints, this model is identical to the standard 2-PL IRT form, with ability parameters &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;, discriminations &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt;, and difficulties &lt;span class=&#34;math inline&#34;&gt;\(a\_j\)&lt;/span&gt;. The cutpoints carve up the latent space so that as ability rises or falls, the legislator becomes more likely to vote yes or no with abstention falling in between these two categories.&lt;/p&gt;
&lt;p&gt;This ordered logit has to be embedded in the hurdle model to account for one-way censoring. Suppose that each legislator has a choice &lt;span class=&#34;math inline&#34;&gt;\(r \in \{0,1\}\)&lt;/span&gt; in which &lt;span class=&#34;math inline&#34;&gt;\(r=1\)&lt;/span&gt; if a legislator is absent and &lt;span class=&#34;math inline&#34;&gt;\(r=0\)&lt;/span&gt; otherwise. With this notation, the likelihood of the hurdle model takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
    L(\beta,\alpha,X,Q,\gamma,\omega|Y\_{k},Y\_{r}) = 
    \prod\_{I}^{i=1} \prod\_{J}^{j=1}
    \begin{cases}
    \zeta(x\_{i}&amp;#39;\gamma\_j - \omega\_j ) &amp;amp; \text{if } r=0, \text{ and} \\
    (1-\zeta({x\_{i}&amp;#39;\gamma\_j - \omega\_j}))L(\beta,\alpha,X|Y\_{k1}) &amp;amp; \text{if } r=1
    \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the hurdle model to be able to affect the legislator ideal points, a separate IRT 2-PL model is included to predict the binary outcome of absence or presence. In this secondary model, the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\_j\)&lt;/span&gt; discrimination and &lt;span class=&#34;math inline&#34;&gt;\(\omega\_j\)&lt;/span&gt; difficulty parameters represent the salience of a particular piece of legislation to an individual legislator &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;. Only if a bill clears the hurdle of salience will the legislator choose to vote on the legislation, i.e., reach the vote outcome model &lt;span class=&#34;math inline&#34;&gt;\(L(\beta,\alpha,X)\)&lt;/span&gt;. This device allows for the ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; to adjust for the fact that the decision of a legislator to show up to vote on a particular bill may be strategic instead of random. In addition, this model is more widely applicable to situations in which an ideal point model is employed and missing data may be a function of the persons’ ideal points.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Simulation&lt;/h2&gt;
&lt;p&gt;To identify the model, I placed &lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt; parameters on the &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; ideal points and additional polarity constraints on either the ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; or the discrimination parameters &lt;span class=&#34;math inline&#34;&gt;\(\gamma\_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt;. As a further restriction, I constrain one of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt; to equal zero. These are the standard set of restrictions included in &lt;code&gt;idealstan&lt;/code&gt;, although the scales of parameters can be modified. The full set of priors is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
        c\_k - c\_{k-1} \sim N(0,5)\\
        \gamma\_j \sim N(0,2)\\
        \omega\_j \sim N(0,5)\\
        \beta\_j \sim N(0,2)\\
        \alpha\_j \sim N(0,5)\\
        x\_i \sim N(0,1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(c\_k\)&lt;/span&gt; parameters are the cutpoints for the &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; ordinal outcomes. They are given a weakly informative prior on the differences between the cutpoints (see the prior help file in the Stan Development Github site). The rest of the priors are arbitrary as the scale of the latent variables is fixed only in the priors themselves.&lt;/p&gt;
&lt;p&gt;To demonstrate the package, I first generate data from this data-generating process using a function &lt;code&gt;id_sim_gen()&lt;/code&gt; built into &lt;code&gt;idealstan&lt;/code&gt;:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;],[1,3,4,4,4,3],[4,1,1,1,4,4],[4,2,3,1,3,4],[1,4,4,4,4,1],[1,4,3,4,4,4],[4,2,3,2,3,2],[1,4,4,4,4,2],[2,4,4,4,4,4],[4,1,1,1,3,4],[4,3,3,3,4,3]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;1&lt;\/th&gt;\n      &lt;th&gt;2&lt;\/th&gt;\n      &lt;th&gt;3&lt;\/th&gt;\n      &lt;th&gt;4&lt;\/th&gt;\n      &lt;th&gt;5&lt;\/th&gt;\n      &lt;th&gt;6&lt;\/th&gt;\n      &lt;th&gt;7&lt;\/th&gt;\n      &lt;th&gt;8&lt;\/th&gt;\n      &lt;th&gt;9&lt;\/th&gt;\n      &lt;th&gt;10&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:[1,2,3,4,5,6,7,8,9,10]},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;In this matrix, the legislators (persons) are represented by the rows and the bills (items) by the columns. The ordinal vote outcomes are numbered 1 to 3 (1=No,2=Abstain,3=Yes) and 4 represents absence.&lt;/p&gt;
&lt;p&gt;I can then take this simulated data and put it into the &lt;code&gt;id_estimate&lt;/code&gt; function. I also use the true values of the legislator ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; for polarity constraints in order to be able to return the “true” latent variables. The &lt;code&gt;id_estimate&lt;/code&gt; function loads pre-compiled stan code a la &lt;a href=&#34;https://cran.r-project.org/web/packages/rstantools/index.html&#34;&gt;&lt;code&gt;rstantools&lt;/code&gt;&lt;/a&gt; and then returns an R object containing a compiled stan model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;true_legis &amp;lt;- ord_ideal_sim@simul_data$true_person
high_leg &amp;lt;- sort(true_legis,decreasing = T,index.return=T)
low_leg &amp;lt;- sort(true_legis,index.return=T)

ord_ideal_est &amp;lt;- id_estimate(idealdata=ord_ideal_sim,
                             model_type=4,
                             fixtype=&amp;#39;constrained&amp;#39;,
                             restrict_type=&amp;#39;constrain_twoway&amp;#39;,
                             restrict_ind_high = high_leg$ix[1:2],
                             restrict_ind_low=low_leg$ix[1:2],
                             refresh=500)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For testing simulation results, the package contains a residual plot for looking at differences between true and estimated values which are stored in the R object:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/stancon2018_paper_files/figure-html/check_true-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The recovery of the true parameters is not perfect, but generally the errors sum to zero across the parameters. Recovering the true parameters is not usually of great interest in a latent variable model as the scale and rotation of the true values is rather arbitrary. However, this exercise is a basic test of the Stan model’s correspondence with the simulated data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-the-polarization-of-coffee&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical Example: The Polarization of Coffee&lt;/h2&gt;
&lt;p&gt;To demonstrate the model’s functionality, I first use a dataset of ordinal rankings draw from Amazon food product reviews. &lt;a name=cite-mcauley2013&gt;&lt;/a&gt;&lt;a href=&#34;#bib-mcauley2013&#34;&gt;McAuley and Leskovec (2013)&lt;/a&gt; collected over 500,000 Amazon food reviews from 1999 to 2012 which are all coded on the 1 to 5 ranking scale that users can post on each product. I focus on a subset of this data by collecting all the reviews that mention the word “coffee” at least a handful of times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;just_coffee &amp;lt;- readRDS(&amp;#39;just_coffee.rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ideal point models use variation between products and users in reviews to identify the latent space. For these reasons, little information is contributed by products and/or users that have very few reviews, and we can remove them from the data first to reduce the dimensionality of the data. Because &lt;code&gt;idealstan&lt;/code&gt; is a full Bayesian model, these users and products do not cause any statistical problems, but they will slow estimation considerably as a large number of users only review one or two products.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coffee_count_prod &amp;lt;- group_by(just_coffee,ProductId) %&amp;gt;% summarize(tot_unique=length(unique(UserId))) %&amp;gt;% 
  filter(tot_unique&amp;lt;30)
coffee_count_user &amp;lt;- group_by(just_coffee,UserId) %&amp;gt;% summarize(tot_unique=length(unique(ProductId))) %&amp;gt;% 
  filter(tot_unique&amp;lt;3)
just_coffee &amp;lt;- anti_join(just_coffee,coffee_count_prod,by=&amp;#39;ProductId&amp;#39;) %&amp;gt;% 
  anti_join(coffee_count_user,by=&amp;#39;UserId&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is currently in long format. To bring the data into &lt;code&gt;idealstan&lt;/code&gt;, it first must be translated to wide format in which each item (product) is a column and each person (user) is a row. For this model, we will drop missing data as we will focus solely on the model’s ordinal rankings without accounting for the fact that users do not have reviews for all products.&lt;/p&gt;
&lt;p&gt;Even with inactive users removed from the data, this model is quite large with 11269 users and 429 products. However, we can take advantage of variational inference in Stan to obtain approximate posterior estimates in a reasonable amount of time. We will also take advantage of &lt;code&gt;idealstan&lt;/code&gt;’s use of variational inference for automatic model identification by running a non-identified (i.e., no constraints) model first, determining which of the users has a very high ideal point, and then constraining that ideal point in the final fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coffee_model &amp;lt;- id_estimate(idealdata=coffee_data,
                            model_type=3,
                            fixtype=&amp;#39;vb&amp;#39;,
                            restrict_ind_high = 1,
                            restrict_params=&amp;#39;person&amp;#39;,
                            auto_id = F,
                            use_vb=T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.042 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 420 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -6e+004             1.000            1.000
##    200    -5e+004             0.642            1.000
##    300    -5e+004             0.441            0.284
##    400    -5e+004             0.346            0.284
##    500    -5e+004             0.277            0.059
##    600    -5e+004             0.234            0.059
##    700    -5e+004             0.200            0.040
##    800    -5e+004             0.176            0.040
##    900    -5e+004             0.157            0.014
##   1000    -5e+004             0.142            0.014
##   1100    -5e+004             0.042            0.006   MEDIAN ELBO CONVERGED
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.
## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.035 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 350 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -5e+009             1.000            1.000
##    200    -1e+008            22.582           44.164
##    300    -9e+010            15.388            1.000
##    400    -4e+009            17.796           25.020
##    500    -2e+007            58.526           25.020
##    600    -2e+006            49.788           25.020
##    700    -1e+006            42.834            6.097
##    800    -5e+004            40.303           22.581
##    900    -5e+004            35.826            6.097
##   1000    -5e+004            32.244            6.097
##   1100    -5e+004            32.146            6.097   MAY BE DIVERGING... INSPECT ELBO
##   1200    -5e+004            27.730            1.114   MAY BE DIVERGING... INSPECT ELBO
##   1300    -5e+004            27.630            1.114   MAY BE DIVERGING... INSPECT ELBO
##   1400    -5e+004            25.128            0.016   MAY BE DIVERGING... INSPECT ELBO
##   1500    -5e+004             2.984            0.015   MAY BE DIVERGING... INSPECT ELBO
##   1600    -5e+004             2.374            0.005   MEDIAN ELBO CONVERGED   MAY BE DIVERGING... INSPECT ELBO
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is of primary interest in this model are the discrimination parameters for the products. The discriminations will tell us which coffee products tend to divide the users most strongly into two poles as the model is one-dimensional. We can first look at the distribution of discriminations via a histogram:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_all_hist(coffee_model,params = &amp;#39;regular_discrim&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stancon2018_paper_files/figure-html/plot_discrim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To gain a sense of how polarizing these products, we can plot the product mid-point, or the point at which a user is indifferent between one rating score and a higher rating score, over the ideal point distribution. I color the user points by their actual ratings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis(coffee_model,group_color=F,group_overlap = T,
              person_labels=F,person_ci_alpha = .05,
              item_plot=429,
              group_labels=F,
              point_size=2,
              show_score = c(&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;,&amp;#39;4&amp;#39;,&amp;#39;5&amp;#39;),
              text_size_group = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stancon2018_paper_files/figure-html/high_pos_discrim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, the product midpoints nearly perfectly segment the user base. We also a large number of 1s and 5s in the observed ratings. This distribution shows that users tend to be very divided on this product, and also that their disagreement over the product is itself a reflection of an underlying dimension, their ideal points.&lt;/p&gt;
&lt;p&gt;Finally, I include here the top 10 most polarizing (highly discriminative) products from each end of the ideal point spectrum.&lt;/p&gt;
&lt;p&gt;First, the top 10 most positive discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;Kicking Horse Coffee 454 Horse Power Dark, Whole Bean&#34;,&#34;Amazing Grass Green Superfood Organic Powder&#34;,&#34;Starbucks Via Ready Brew Instant Coffee&#34;,&#34;Stephen&#39;s Gourmet Hot Cocoa, Candycane Cocoa&#34;,&#34;Prince of Peace Organic Tea, Oolong&#34;,&#34;NOW Foods Erythritol Natural Sweetener&#34;,&#34;illy Cappucino, 12 pack&#34;,&#34;Nestle Hot Cocoa Mix&#34;,&#34;Better than Milk Vegan Soy Powder&#34;,&#34;Ghirardelli Chocolate Sweet Ground Chocolate &amp;amp; Cocoa&#34;],[7.87259,7.864585,7.80567,7.805575,7.770375,7.770345,7.727985,7.33822,7.33797,7.28447]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Product Description&lt;\/th&gt;\n      &lt;th&gt;Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Second, the top 10 least positive negative discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;Melitta Cafe de Europa Gourmet Coffee&#34;,&#34;McCann&#39;s Steel Cut Irish Oatmeal&#34;,&#34;Stash Tea Gunpowder Green Loose Leaf Tea&#34;,&#34;Nutiva Organic Coconut Oil&#34;,&#34;Stash Tea Company English Breakfast&#34;,&#34;Equal Exchange Organic Coffee, Mind Body Soul, Whole Bean&#34;,&#34;Ghirardelli Premium Hot Beverage Mix, White Mocha, 19-Ounce Cans&#34;,&#34;Victorian Inn Instant Hot Cappucino, French Vanilla&#34;,&#34;Medaglia Doro Instant Espresso&#34;,&#34;Yogi Tea, Green Tea Super Antioxidant&#34;],[-9.79306,-9.785615,-9.70132,-9.616965,-7.77101,-7.75944,-7.74106,-7.73298,-7.17082,-7.13721]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Product Description&lt;\/th&gt;\n      &lt;th&gt;Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;While a full interpretation of these results would require significant work at examining the full range of products and their relevant discrimination scores, what is clear is that positive discrimination tends to have more large brands of coffee, including Starbucks and Illy, while the negative discrimination products tend to b esmaller brands, such as Melitta Cafe and Equal Exchange Organic Coffee. The most interesting finding in this analysis is that Ghirardelli products are at both ends of the scale: one type of Ghirardelli powdered drink, Ghirardelli Chocolate, has high positive discrimination, while Ghirardelli White Mocha has high negative discrimination. Intuitivelly, these products are appealing to different reviewers with very diverging preferences.&lt;/p&gt;
&lt;p&gt;In the next section, I apply the ideal point model to a more traditional domain–the U.S. Congress–and also examine the role of including missing data via inflation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-114th-senate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical Example: 114th Senate&lt;/h2&gt;
&lt;p&gt;As an empirical example of the model, I use data from the &lt;a href=&#34;http://www.voteview.com&#34; class=&#34;uri&#34;&gt;http://www.voteview.com&lt;/a&gt; website on the voting record of the 114th US Senate. This dataset comes with the &lt;code&gt;idealstan&lt;/code&gt; package, and I then recode the data to correspond to a standard R matrix. However, because abstentions rarely happen in the US Congress, I do not include abstentions in this model and instead estimate a standard binary IRT 2-PL with inflation for missing data (absences). I then pass the matrix of vote records to the &lt;code&gt;id_make&lt;/code&gt; function to create an object suitable for running the model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/stancon2018_paper_files/figure-html/use_senate-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given a suitable object from &lt;code&gt;id_make&lt;/code&gt;, the &lt;code&gt;id_estimate&lt;/code&gt; function can estimate a variety of IRT ideal point models, including 2-PL, ordinal, hierarchical and dynamic IRT (using random-walk priors). For this run we set the &lt;code&gt;model_type&lt;/code&gt; parameter to &lt;code&gt;2&lt;/code&gt;, which represents a 2-PL model with absence inflation. Because this dataset is of a significant size, I use variational inference instead of the full sampler to demonstrate its use. I do not, however, use the automatic identification option &lt;code&gt;auto_id&lt;/code&gt; because it is relatively easy to constrain particular legislator ideal points for the Senate. In this case, I constrain Ben Sasse, Marco Rubio, Berni Sanders, Ted Cruz, Harry Reid and Elizabeth Warren, all senators with pronounced ideological views.&lt;/p&gt;
&lt;p&gt;After estimating the model, I can look at a graphical display of the ideal points with the &lt;code&gt;id_plot&lt;/code&gt; function. The &lt;code&gt;id_plot&lt;/code&gt; function produces a &lt;code&gt;ggplot2&lt;/code&gt; object which can be further modified. The legislators are given colors based on their party affiliation. The general shape of the ideal point distribution reflects the nature of polarization in the US Congress, with relatively few moderates near the center of the distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.026 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 260 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -2e+004             1.000            1.000
##    200    -2e+004             0.509            1.000
##    300    -2e+004             0.341            0.018
##    400    -2e+004             0.256            0.018
##    500    -2e+004             0.205            0.003   MEDIAN ELBO CONVERGED
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stancon2018_paper_files/figure-html/run_114_model-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also plot the bill (item) midpoints as a line of indifference that we overlay on top of the ideal points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot(sen_est,person_ci_alpha=.1,item_plot=205,
        group_labels=T,
        abs_and_reg=&amp;#39;Vote Points&amp;#39;) + scale_colour_brewer(type=&amp;#39;qual&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/stancon2018_paper_files/figure-html/bill_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this particular piece of legislation, the midpoint is right in the middle of the ideal point distribution, showing that the bill has very high discrimination. Also, the rug lines at the bottom of the plot, which show the HPD for the midpoint, indicate that the model is uncertain about the votes of only a handful of Senators on this bill.&lt;/p&gt;
&lt;p&gt;We can similarly examine the absence midpoint for the same bill, which signifies the place on the ideal point spectrum at which a legislator is indifferent from showing up to vote. In this case, only very conservative Republicans chose not to show up to vote.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/stancon2018_paper_files/figure-html/abs_bill_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use the bill absence parameters to also see which bills showed the highest discrimination in terms of absences, or in other words, for which bills did the absence of legislators signify that they intentionally did not show up? To do this, I sort the discrimination parameters from the underlying Stan object and then merge them with the bill labels from voteview.org.&lt;/p&gt;
&lt;p&gt;First we can look at the top 10 bills with liberal/Democrat absence discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-4&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-4&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;To ensure that the storage and transportation of petroleum coke is regulated in a manner that ensures the protection of public and ecological health.&#34;,&#34;To continue cleaning up fields and streams while protecting neighborhoods, generating affordable energy, and creating jobs.&#34;,&#34;To express the sense of the Senate that climate change is real and not a hoax.&#34;,&#34;To require the use of iron, steel, and manufactured goods produced in the United States in the construction of the Keystone XL Pipeline and facilities.&#34;,&#34;To express the sense of Congress regarding federally protected land.&#34;,&#34;To promote energy efficiency.&#34;,&#34;To provide limits on the designation of new federally protected land.&#34;,&#34;To express the sense of Congress regarding climate change.&#34;,&#34;To conform citizen suits under the Endangered Species Act of 1973.&#34;,&#34;To ensure that oil transported through the Keystone XL pipeline into the United States is used to reduce United States dependence on Middle Eastern oil.&#34;],[1.819605,1.688775,1.67506,1.66709,1.643,1.549565,1.52296,1.499735,1.483755,1.453105]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Bill Description&lt;\/th&gt;\n      &lt;th&gt;Absence Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Interestingly, Democrats appeared to be strategically absent most often on bills about climate change and the Keystone XL pipeline.&lt;/p&gt;
&lt;p&gt;For conseratives the bills are as follows:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-5&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-5&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;To limit the availability of amounts authorized to be appropriated for overseas contingency operations pending relief from the spending limits under the Budget Control Act of 2011.&#34;,&#34;To strike the FOIA exemption.&#34;,&#34;To improve the bill.&#34;,&#34;To improve the requirements relating to removal of personal information from cyber threat indicators before sharing.&#34;,&#34;To strengthen the Justice for Victims of Trafficking Act by incorporating additional bipartisan amendments.&#34;,&#34;To protect information that is reasonably believed to be personal information or information that identifies a specific person.&#34;,&#34;To improve the definitions of cybersecurity threat and cyber threat indicator.&#34;,&#34;To exempt from the capability and process within the Department of Homeland Security communication between a private entity and the Federal Bureau of Investigation or the United States Secret Service regarding cybersecurity threats.&#34;,&#34;An original bill to improve cybersecurity in the United States through enhanced sharing of information about cybersecurity threats, and for other purposes.&#34;,&#34;To modify section105 to require DHS to review all cyber threat indicators and countermeasures in order to remove certain personal information.&#34;],[-3.956115,-3.639605,-3.63636,-3.57792,-3.516255,-3.42238,-3.15867,-3.12428,-3.075425,-2.992295]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Bill Description&lt;\/th&gt;\n      &lt;th&gt;Absence Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;For Republicans, on the other hand, it appears that cybersecurity and inter-departmental information sharing were the laws in which they chose not to show up for ideological (or political) reasons. This could be due to concerns among the conservative base regarding government over-reach and collecting information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;idealstan&lt;/code&gt; is an effort to put state-of-the-art ideal point models along with the power of full Bayesian analysis in the hands of applied researchers in the social sciences. While the empirical examples presented were from the US Congress, the model can be used in any situation in which the assumptions of the ideal point model (unconstrained ideal point space) apply, i.e., situations in which the latent space is fundamentally polarizing between people. While similar in function and form to the &lt;code&gt;edstan&lt;/code&gt; package, the &lt;code&gt;idealstan&lt;/code&gt; package has to use more complicated forms of identification because of the difficulty of leaving discrimination parameters unconstrained.&lt;/p&gt;
&lt;p&gt;Moving forward, I intend to add in more functionality to smoothly operate with &lt;code&gt;shinystan&lt;/code&gt; and &lt;code&gt;bayesplot&lt;/code&gt;, as well as add further types of explanatory IRT models. The intention is to have a package on which applied researchers can run different ideal point models and then examine how different modeling choices affect the results. In addition, I will continue to build more visualization options so that the results are easily digestable and publishable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;’ &lt;a name=bib-enelow198&gt;&lt;/a&gt;&lt;a href=&#34;#cite-enelow198&#34;&gt;[1]&lt;/a&gt; J. M. Enelow and M. J. Hinich. &lt;em&gt;The Spatial Theory of Voting: An Introduction&lt;/em&gt;. Cambridge University Press, 1984.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-takane1986&gt;&lt;/a&gt;&lt;a href=&#34;#cite-takane1986&#34;&gt;[2]&lt;/a&gt; Y. Takane and J. de Leeuw. “On the Relationship Between Item Response Theory and Factor Analysis of Discretized Variables”. In: &lt;em&gt;Psychometrika&lt;/em&gt; 52.3 (1986), pp. 393-408.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-hoff2002&gt;&lt;/a&gt;&lt;a href=&#34;#cite-hoff2002&#34;&gt;[3]&lt;/a&gt; P. D. Hoff, A. E. Raftery and M. S. Hancock. “Latent Space Approaches to Social Network Analysis”. In: &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 97.460 (2002), pp. 1090-1098.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-quinn2002&gt;&lt;/a&gt;&lt;a href=&#34;#cite-quinn2002&#34;&gt;[4]&lt;/a&gt; A. D. Martin and K. M. Quinn. “Dynamic Ideal Point Estimation via Markov Chain Monte Carlo for the U.S. Supreme Court, 1953-1999”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 10.2 (2002), pp. 134-153.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-jackman2004&gt;&lt;/a&gt;&lt;a href=&#34;#cite-jackman2004&#34;&gt;[5]&lt;/a&gt; J. Clinton, S. Jackman and D. Rivers. “The Statistical Analysis of Rollcall Data”. In: &lt;em&gt;American Political Science Review&lt;/em&gt; 98.2 (2004), pp. 355-370.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-gelman2005&gt;&lt;/a&gt;&lt;a href=&#34;#cite-gelman2005&#34;&gt;[6]&lt;/a&gt; J. Bafumi, A. Gelman, D. K. Park, et al. “Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 13.2 (2005), pp. 171-187.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-poole2008&gt;&lt;/a&gt;&lt;a href=&#34;#cite-poole2008&#34;&gt;[7]&lt;/a&gt; K. T. Poole, J. B. Lewis, J. Lo, et al. &lt;em&gt;Scaling Roll Call Votes with W-NOMINATE in R&lt;/em&gt;. Working Paper. Social Science Research Network, Oct. 06, 2008. URL: &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&#34; class=&#34;uri&#34;&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-slapin2008&gt;&lt;/a&gt;&lt;a href=&#34;#cite-slapin2008&#34;&gt;[8]&lt;/a&gt; J. B. Slapin and S. Proksch. “A Scaling Model for Estimating Time-Series Party Positions from Texts”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 52.3 (2008), pp. 705-722.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-carroll2013&gt;&lt;/a&gt;&lt;a href=&#34;#cite-carroll2013&#34;&gt;[9]&lt;/a&gt; R. Carroll, J. B. Lewis, J. Lo, et al. “The Structure of Utility in Spatial Models of Voting”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 57.4 (2013), pp. 1008-1028.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-mcauley2013&gt;&lt;/a&gt;&lt;a href=&#34;#cite-mcauley2013&#34;&gt;[10]&lt;/a&gt; J. McAuley and J. Leskovec. “From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews”. In: &lt;em&gt;WWW&lt;/em&gt; (2013).&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-armstrong2014&gt;&lt;/a&gt;&lt;a href=&#34;#cite-armstrong2014&#34;&gt;[11]&lt;/a&gt; D. A. Armstrong, R. Bakker, R. Carroll, et al. &lt;em&gt;Analyzing Spatial Models of Choice and Judgment with R&lt;/em&gt;. CRC Press, 2014.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-bonica2014&gt;&lt;/a&gt;&lt;a href=&#34;#cite-bonica2014&#34;&gt;[12]&lt;/a&gt; A. Bonica. “Mapping the Ideological Marketplace”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 58.2 (2014), pp. 367-386.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-barbera2015&gt;&lt;/a&gt;&lt;a href=&#34;#cite-barbera2015&#34;&gt;[13]&lt;/a&gt; P. BarberÃ¡. “Birds of the Same Feather Tweet Together: Bayesian Ideal Point Estimation Using Twitter Data”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 23 (2015), pp. 76-91.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-NIPS2015_5758&gt;&lt;/a&gt;&lt;a href=&#34;#cite-NIPS2015_5758&#34;&gt;[14]&lt;/a&gt; A. Kucukelbir, R. Ranganath, A. Gelman, et al. “Automatic Variational Inference in Stan”. In: &lt;em&gt;Advances in Neural Information Processing Systems 28&lt;/em&gt;. Ed. by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama and R. Garnett. Curran Associates, Inc., 2015, pp. 568-576. URL: &lt;a href=&#34;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&#34; class=&#34;uri&#34;&gt;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-kubinec2017&gt;&lt;/a&gt;&lt;a href=&#34;#cite-kubinec2017&#34;&gt;[15]&lt;/a&gt; R. Kubinec. &lt;em&gt;Absence Makes the Ideal Points Sharper&lt;/em&gt;. Poster. Political Methodology Society, 2017.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
