<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Academic</title>
    <link>http://www.robertkubinec.com/post/</link>
      <atom:link href="http://www.robertkubinec.com/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© 2020 Robert Kubinec</copyright><lastBuildDate>Fri, 12 Mar 2021 15:00:00 +0000</lastBuildDate>
    <image>
      <url>http://www.robertkubinec.com/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>http://www.robertkubinec.com/post/</link>
    </image>
    
    <item>
      <title>The Good, the Bad and the Ugly in the CDC&#39;s Face Mask Study</title>
      <link>http://www.robertkubinec.com/post/cdc_fe/</link>
      <pubDate>Fri, 12 Mar 2021 15:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/cdc_fe/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;p&gt;While perusing the news, I read eagerly about the &lt;a href=&#34;https://www.cdc.gov/mmwr/volumes/70/wr/mm7010e3.htm&#34;&gt;CDC’s recent study examining associations between county-level mask mandates and COVID-19 growth rates&lt;/a&gt;. This study has already been the subject of &lt;a href=&#34;https://www.nytimes.com/2021/03/05/health/coronavirus-restaurant-dining-masks.html&#34;&gt;angry retorts from the restaurant industry&lt;/a&gt; due to the CDC’s claim that restaurant closures reduced COVID-19 spread. Looking at the underlying details in the study, though, revealed some concerns about the particular model they used, known as a two-way fixed effects model, and also an important factor they left out of the model entirely: political partisanship.&lt;/p&gt;
&lt;p&gt;To examine these deficiencies, I replicated their model from scratch and also employed different panel data models to provide clearer answers. In brief, the negative association between mask mandates and the spread of COVID-19 does hold up, but it appears to be highly conditional on partisanship. In addition, there are a lot of complexities when it comes to comparing counties, and I do not believe the CDC or anyone else has yet resolved all of them. So while I applaud the CDC for doing a lot of hard work on data collection, we still have a ways to go before we can fully understand how effective mask mandates were against COVID-19.&lt;/p&gt;
&lt;p&gt;In this post I first discuss my challenges replicating their analysis, and then I critique the choices they made in their paper. Later I put forward my own analysis to probe in more detail the role of mask mandates. I include the code I use to estimate models in this blog post to be transparent about how I did the re-analysis. If you are not familiar with R code, feel free to skip those sections.&lt;/p&gt;
&lt;p&gt;If you would like access to the code and data I used to write this blog post, please use this &lt;a href=&#34;https://github.com/saudiwin/cdcmask&#34;&gt;Github repository&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;collecting-the-cdc-papers-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Collecting the CDC Paper’s Data&lt;/h2&gt;
&lt;p&gt;The first and most necessary step to replicate the CDC’s findings is to obtain data to fit their model. This exercise is also important because it shows if 1) their article is clear enough to successfully re-trace their steps and if 2) all the data is available to do so. The authors of the study have posted their mask and other county policy data online, which was easy to download and analyze. Unfortunately, some of their data, particularly testing and case data collected by Health and Human Services (HHS), is not available to the wider public. Given the new administration’s commitment to transparency, it really should be available publicly. Because I am writing a blog post and wanted data I could share, I chose instead to use open source repositories of case and test data, specifically the &lt;a href=&#34;https://github.com/nytimes/covid-19-data&#34;&gt;New York Times&lt;/a&gt; for county-level cases and the &lt;a href=&#34;https://github.com/COVID19Tracking/website&#34;&gt;COVID-19 Tracking Project&lt;/a&gt; for test data by state. Because I could not find testing at the county level, I approximated tests by county by multiplying that county’s share of state COVID-19 tests with that state’s daily testing total.&lt;/p&gt;
&lt;p&gt;There were some ambiguities in interpreting the article’s instructions about the CDC’s county policy data. The policy restrictions, including stay-at-home orders, mask mandates, and restaurant and bar closures, have multiple categories, such as partial enforcement, and it was not clear if I should use all the categories or collapse them to binary indicators. This became an issue later in the model because some of the categories were collinear and the model dropped them, particularly bar closures. I am not sure for that reason if my use of the CDC’s data exactly matches theirs.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;replicating-the-cdcs-results&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Replicating the CDC’s Results&lt;/h2&gt;
&lt;p&gt;Given these caveats about data collection, the model estimates I obtained using the same specification–two-way fixed effects for county and day–are broadly similar to their estimates, although the associations are weaker on average. I also used the same comparison periods they describe in the paper. For each county, I selected the first twenty days preceding the mask mandate implementation, and then fit separate models to five different post-mandate periods: 1-20 days following the mandate, 21-40 days, 41-60 days, 61-80 days, and 81-100 days. The ambiguity that surfaced while doing this occurred in counties with multiple periods of mask mandates: what if there weren’t twenty days prior to the second mask mandate? I included as many as were available, but its still important to note this ambiguity as it is not clear exactly what data coding the CDC used.&lt;/p&gt;
&lt;p&gt;The plot below compares my estimates of the effect of mask mandates to theirs by the five different post-mandate time periods. The weaker associations in my estimates could simply be due to measurement error from using unofficial sources, so the smaller effects are not necessarily a huge concern (though it would be better if the CDC released all of the data they used). On the whole, I think this replication was at least partially successful as the confidence intervals of the estimates overlap.&lt;/p&gt;
&lt;p&gt;The following code replicating the CDC’s paper is quite long but is included for transparency’s sake. I used the &lt;code&gt;fixest&lt;/code&gt; package with R to fit linear models due to the scale of the data and the number of fixed effects (one for each county plus one for each day).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# estimate CDC model using lm
# baseline = 1-20 days before implementation
# models differentiated by number of days included post-implementation
# note: no mention of whether this makes the panel imbalanced
# a bit odd too if there are multiple times that county switches in and out of mask mandates
d1 &amp;lt;-
  filter(combined_data,
         (n_day_before &amp;gt; 0 &amp;amp;
            n_day_before &amp;lt; 21) |
           (n_day_after &amp;gt; 0 &amp;amp;
              n_day_after &amp;lt; 21)) %&amp;gt;% fixest::feols(
                cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather |
                  fips + date,
                data =
                  .,
                cluster =
                  &amp;quot;FIPS_State&amp;quot;,
                weights =
                  ~ pop
              )

d2 &amp;lt;-
  filter(combined_data,
         (n_day_before &amp;gt; 0 &amp;amp;
            n_day_before &amp;lt; 21) |
           (n_day_after &amp;gt; 20 &amp;amp;
              n_day_after &amp;lt; 41)) %&amp;gt;% fixest::feols(
                cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather |
                  fips + date,
                data =
                  .,
                cluster =
                  &amp;quot;FIPS_State&amp;quot;,
                weights =
                  ~ pop
              )

d3 &amp;lt;-
  filter(combined_data,
         (n_day_before &amp;gt; 0 &amp;amp;
            n_day_before &amp;lt; 21) |
           (n_day_after &amp;gt; 40 &amp;amp;
              n_day_after &amp;lt; 61)) %&amp;gt;% fixest::feols(
                cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather |
                  fips + date,
                data =
                  .,
                cluster =
                  &amp;quot;FIPS_State&amp;quot;,
                weights =
                  ~ pop
              )

d4 &amp;lt;-
  filter(combined_data,
         (n_day_before &amp;gt; 0 &amp;amp;
            n_day_before &amp;lt; 21) |
           (n_day_after &amp;gt; 60 &amp;amp;
              n_day_after &amp;lt; 81)) %&amp;gt;% fixest::feols(
                cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather |
                  fips + date,
                data =
                  .,
                cluster =
                  &amp;quot;FIPS_State&amp;quot;,
                weights =
                  ~ pop
              )

d5 &amp;lt;-
  filter(
    combined_data,
    (n_day_before &amp;gt; 0 &amp;amp;
       n_day_before &amp;lt; 21) | (n_day_after &amp;gt; 80 &amp;amp; n_day_after &amp;lt; 101)
  ) %&amp;gt;%
  fixest::feols(
    cases_diff_log ~ mask_status + test_prop + stay_status + rest_action + bar_order + ban_gather |
      fips + date,
    data = .,
    cluster = &amp;quot;FIPS_State&amp;quot;,
    weights =  ~ pop
  )

# combine data from all models together into a tibble

all_coefs &amp;lt;- lapply(list(d1,d2,d3,d4,d5),function(d) slice(summary(d)$coeftable,1)) %&amp;gt;% 
  bind_rows %&amp;gt;% 
  mutate(Model=c(&amp;quot;1-20 Days&amp;quot;,
                 &amp;quot;21-40 Days&amp;quot;,
                 &amp;quot;41-60 Days&amp;quot;,
                 &amp;quot;61-80 Days&amp;quot;,
                 &amp;quot;81-100 Days&amp;quot;),
         upb=Estimate + 1.96*`Std. Error`,
         lb=Estimate - 1.96*`Std. Error`,
         Type=&amp;quot;Replication&amp;quot;)

# combine with original estimates

all_coefs &amp;lt;- bind_rows(all_coefs,
                       tibble(Estimate=c(-0.5,-1.1,-1.5,-1.7,-1.8),
                              lb=c(-0.1,-0.6,-0.8,-0.9,-0.7),
                              upb=c(-0.8,-1.6,-2.1,-2.6,-2.8),
                              Model=c(&amp;quot;1-20 Days&amp;quot;,
                 &amp;quot;21-40 Days&amp;quot;,
                 &amp;quot;41-60 Days&amp;quot;,
                 &amp;quot;61-80 Days&amp;quot;,
                 &amp;quot;81-100 Days&amp;quot;),Type=&amp;quot;CDC&amp;quot;))

all_coefs %&amp;gt;% 
  ggplot(aes(y=Estimate,x=Model)) +
  geom_pointrange(aes(ymin=lb,ymax=upb,colour=Type),
                  position=position_dodge(width=0.5),
                  fatten=2) +
  geom_text(aes(label=round(Estimate,digits=2),group=Type),
            position=position_dodge(width=0.5),
            size=3,vjust=-.75) +
  geom_hline(yintercept=0,linetype=2) +
  theme_tufte() +
  geom_vline(xintercept=0,linetype=2) +
  scale_colour_viridis_d() +
  coord_flip() +
  scale_y_continuous(labels=scales::percent_format(scale=1)) +
  labs(y=&amp;quot;Comparison Period&amp;quot;,x=&amp;quot;Percentage Change in Daily COVID-19 Cases&amp;quot;) +
  ggtitle(&amp;quot;Replication of CDC Mask Results with Open Source Data&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/cdc_fe_files/figure-html/cdc_model-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;issues-with-the-cdcs-analysis&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Issues with the CDC’s Analysis&lt;/h2&gt;
&lt;p&gt;Given the reasonably successful replication, I now turn to some of the issues with the analysis. One peculiar choice they made was to subdivide their data into the different days following a mask mandate as the plot above shows. The reason for this choice, though they did not discuss it in their paper, is probably due to the difficulty in comparing COVID-19 case rates across counties. It is quite likely that counties will wait to adopt mask mandates until COVID-19 rates are very high, so the unconditional association between mask mandates and COVID-19 could end up being strongly positive. This issue can be seen in the plot below, where I show each county’s COVID-19 growth rate by day, and colour the lines by whether a mask mandate was enforced. As can be seen, the imposition of mask mandates begins later in the course of the pandemic around March and April as case rates pick up and the CDC issues mask-wearing guidance.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# plot the orders as line plots to show when the tend to be implemented

combined_data %&amp;gt;% 
  ggplot(aes(y=cases/pop,x=date)) +
  geom_line(aes(colour=mask_status,group=fips),alpha=0.5) +
  theme_tufte() +
  scale_color_viridis_d() +
  labs(x=&amp;quot;&amp;quot;,y=&amp;quot;Percent of County Infected with COVID-19&amp;quot;,
       caption=&amp;quot;COVID-10 cases data from the New York Times and mask mandate data from the CDC.&amp;quot;) +
  scale_y_continuous(labels=scales::percent) +
  ggtitle(&amp;quot;County COVID-19 Infection Rates Over Time&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/cdc_fe_files/figure-html/plotorders-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The CDC tried to resolve this by restricting their sample to the twenty days preceding a mask mandate and one of the five categories mentioned before for twenty-day periods following the mask mandate. This is one approach, but not a particularly satisfying one. The day cutoffs are very arbitrary (why twenty-day intervals? Why only the previous twenty days before a mask mandate?). I think a much better approach, and one I show later, is to try to control for some of the differences that may lead to mask adoption, and to use all the sample data unless there is a very clear reason not to.&lt;/p&gt;
&lt;p&gt;However, there is an even more significant concern, and that is the modeling approach itself. The CDC used a linear model with fixed effects for both days and counties, or what is commonly known as a “two-way” fixed effects model. This would be the equivalent of having a dummy/binary variable in the model for each county and day separately, or 3477 additional variables. This stunning number should give us pause: what is the point of adding thousands of variables to the model? The resulting linear model is so big that R’s default linear model function &lt;code&gt;lm&lt;/code&gt; could not estimate it, requiring me to use the &lt;code&gt;fixest&lt;/code&gt; package which is more memory efficient. What is happening to our interpretation of mask mandates by including so many extra variables?&lt;/p&gt;
&lt;p&gt;I’ve written a previous blog post about the &lt;a href=&#34;http://www.robertkubinec.com/post/fixed_effects/&#34;&gt;two-way fixed effect model and panel data models in general&lt;/a&gt;, following on a &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231349&#34;&gt;paper I wrote with Jon Kropko on the topic&lt;/a&gt;, which I refer to the reader to a fuller answer to these questions. Our research and an increasing number of papers question whether the two-way fixed effects model does what people say it does, which is control for a wide array of possible omitted variables.&lt;/p&gt;
&lt;p&gt;The problem is that fixed effects are &lt;em&gt;not&lt;/em&gt;, on their own, some magic formula to get rid of inferential problems. Including fixed effects can help isolate one of two dimensions in the data: 1) variation in COVID-19 growth rates within counties over time (what the plot above shows) &lt;strong&gt;or&lt;/strong&gt; 2) a cross-section of county growth rates for a given day. The first corresponds to including a dummy variable for each county, and the second including a dummy variable for each day.&lt;/p&gt;
&lt;p&gt;In general, both of these comparisons are potentially interesting. Looking at growth rates within counties implies that we think COVID-19 growth rates following the introduction of mask mandates will fall. However, this comparison can be difficult because we know there are a lot of other factors that change over time within counties, such as people’s increasing awareness of the effectiveness of masks. Similarly, we would think that if mask mandates are effective, counties with mandates on a given day should have lower growth rates than counties without mandates (the cross-section). Of course, differences across counties can affect these comparisons, especially if there are reasons why some counties may want to adopt and enforce mask mandates more than others.&lt;/p&gt;
&lt;p&gt;The CDC, though, included both kind of fixed effects, which makes it very hard to know what comparison they are trying to draw. In the blog post mentioned above, I show why this particular estimate is nearly impossible to understand. It would seem the CDC is to trying to address all possible inferential issues by including a cornucopia of fixed effects, but fixed effects only enable the researcher to draw comparisons, not remove all sources of bias. To deal with the possible factors that might affect our estimates, we need to think about including data that measures potential confounding variables, not just including as many fixed effects as possible and hoping for the best.&lt;/p&gt;
&lt;p&gt;To illustrate this, I replicate the CDC’s analysis except that I compare a model with fixed effects for counties (within-country variation), a model with fixed effects for days (cross-sectional variation), and plot them next to the two-way fixed effects results:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bind_rows(all_coefs_within,
          all_coefs_between,filter(all_coefs,Type==&amp;quot;Replication&amp;quot;)) %&amp;gt;% 
  ggplot(aes(y=Estimate,x=Model)) +
  geom_pointrange(aes(ymin=lb,ymax=upb,colour=Type),
                  position=position_dodge(width=.8),
                  fatten=2) +
  geom_text(aes(label=round(Estimate,digits=2),group=Type),
            position=position_dodge(width=.8),
            size=2.5,vjust=-.75,fontface=&amp;quot;bold&amp;quot;) +
  theme_tufte() +
  geom_vline(xintercept=0,linetype=2) +
  scale_colour_viridis_d() +
  geom_hline(yintercept=0,linetype=2) +
  scale_y_continuous(labels=scales::percent_format(scale=1)) +
  coord_flip() +
  labs(y=&amp;quot;Comparison Period&amp;quot;,x=&amp;quot;Percentage Change in Daily COVID-19 Cases&amp;quot;) +
  ggtitle(stringr::str_wrap(&amp;quot;Comparison of Mask Mandates Effects both Within-County Over-time and Daily Cross-sections&amp;quot;,width = 60))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/cdc_fe_files/figure-html/crosswithin-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, the models with either fixed effects for days or counties tend to have larger and more negative estimates of the effect of mask mandates than do the two-way fixed effects models. This is of course puzzling, as if the two-way fixed effects models controls accounts for both sets of fixed effects, shouldn’t the estimate be somewhere between the two one-way models? That intuition is misleading and deeply flawed. The two-way fixed effects model combines variation in both dimensions in a way that is complex and highly non-linear, which means it could produce an estimate completely independent of both dimensions in the data. That is the reason why that estimate is hard to interpret, and frankly not the best choice for the CDC. I have no idea what the two-way estimates substantively mean–whether and to what extent they refer to variation within counties or variation across counties–and as a result I can’t also explain why they are lower than either considered separately.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-do-it-better&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to Do It Better&lt;/h2&gt;
&lt;p&gt;To summarize the blog post so far, the CDC is trying to take seriously the fact that mask mandates were not randomly assigned to counties, and that the course of the pandemic is affecting counties in a variety of ways over time. However, their approach tries to use brute force modeling and arbitrary data subsetting. There must be a better way, you ask. Well, there is, if we are comfortable with trying first to draw relevant comparisons while acknowledging that in this situation, there probably is no way to easily remove all concerns about other factors that could affect or explain the results. However, we can still do better by addressing these inferential threats head on rather than try to sneak around them with hordes of dummy variables.&lt;/p&gt;
&lt;p&gt;As mentioned earlier, there are two basic and much clearer options for panel data models: within-case county models and cross-section by day models. (Later on I will show some new work that offers other options, but we’ll stick with these for now). I chose to go with the cross-sectional models, which may be a bit surprising, but the reason has to do with available data. For the within-county over-time model, we should be concerned about rising awareness of masks and beliefs in their effectiveness. Unfortunately, there is very little over-time data on counties we can use. Most polling is only available at the state level at best. We have Google mobility data by day, but that is about it. If we are concerned that people become more compliant with mask mandates over time because they are becoming better informed about how masks help, there is no way to include data about those changing perceptions in the model.&lt;/p&gt;
&lt;p&gt;By contrast, in the cross-sectional model we would need to be worried about a different issues: are there different reasons why some counties would adopt mask mandates in the first place? That is, when we compare counties with and without mask mandates, are there reasons why they adopt mask mandates &lt;em&gt;and&lt;/em&gt; see fewer infections? The answer is, of course there are. However, we &lt;em&gt;can&lt;/em&gt; measure these factors because they are constant over time. For my purposes, I collected three important control variables: county median income, the proportion foreign-born from the Census, and crucially, 2016 presidential vote totals for Donald J. Trump.&lt;/p&gt;
&lt;p&gt;In my own work on the pandemic, which you can &lt;a href=&#34;https://osf.io/preprints/socarxiv/jp4wk/&#34;&gt;read more here&lt;/a&gt;, I show that Trump partisanship has a very strong relationshp to spread of the pandemic. In that paper I examine states, so I can look at within-state variation because I can measure changes in Trump approval ratings over time. Lacking that data for counties, though, we can still examine whether partisanship affects mask mandates–which we should expect. Counties that are more conservative are probably less likely to adopt mask mandates, and if they do adopt them, to comply with them, while the opposite probably holds true for more liberal areas. Adding in county-level income helps account for the resources a county might have to address the pandemic and also how many workers are exposed to the virus through their daily jobs as we know that higher-paid workers tended to work at home. The proportion of foreign-born is a proxy for travel patterns as foreign residents tend to live closer to areas with access to international travel, which was a cause of the early spread of COVID-19. By accounting for these factors, we can make more compelling comparisons between counties by controlling for the factors that might prompt some counties to be more likely to adopt mask mandates and to see greater levels of compliance.&lt;/p&gt;
&lt;p&gt;In the code below, I include the CDC’s data with the additional control variables. I also interact mask mandate status with vote share to see if mask mandates do seem to be more effective in more liberal areas. While it would be nice to include a two-way fixed effects for comparison’s sake, the model could not estimate due to some collinearities between varibles (not clear why). Oddly enough, I could estimate such a model when using the smaller subsets of the data that the CDC did. It is really an odd duck.&lt;/p&gt;
&lt;p&gt;Following is the code I used to estimate a model with additional controls and fixed effects for days:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d1_part_between &amp;lt;- fixest::feols(
                cases_diff_log ~ mask_status*vote_share + scale(test_prop) +   vote_share +prop_foreign + scale(median_income) + stay_status + rest_action + bar_order + ban_gather  |
                  date,
                data =
                  combined_data,
                cluster =
                  &amp;quot;FIPS_State&amp;quot;,
                weights =
                  ~ pop
              )&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In the plot below I show all of the results of the model ordered by how negative or positive their association is with COVID-19 spread. What is immediately apparent is a strong and very positive association between Trump vote share and COVID-19: more conservative areas tend to see more infections after controlling for other factors. Furthermore, removing restrictions on bars is strongly associated with more infections, though we don’t see similar associations for restaurants, which may be a comfort to non-bar restaurant owners and the precautions they’ve taken. Importantly, the interaction effect on Trump vote share and mask mandates is strongly positive, suggesting that mask mandates are less effective in conservative areas.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;d1_part_between$coeftable %&amp;gt;% 
  mutate(Variables=row.names(d1_part_between$coeftable),
         Variables=forcats::fct_recode(Variables,
  &amp;quot;Ban on &amp;gt;10 Gatherings&amp;quot; = &amp;quot;ban_gather&amp;quot;,
  &amp;quot;Bars Curbside Only&amp;quot; = &amp;quot;bar_orderCurbside/delivery only&amp;quot;,
  &amp;quot;No Bar Restrictions&amp;quot; = &amp;quot;bar_orderNo restriction found&amp;quot;,
  &amp;quot;Bars Open with Limitations&amp;quot; = &amp;quot;bar_orderOpen with limitations&amp;quot;,
  &amp;quot;Mask Mandate&amp;quot; = &amp;quot;mask_statusPublic Mask Mandate&amp;quot;,
  &amp;quot;Mask Mandate X Trump Vote&amp;quot; = &amp;quot;mask_statusPublic Mask Mandate:vote_share&amp;quot;,
  &amp;quot;Median Income&amp;quot; = &amp;quot;scale(median_income)&amp;quot;,
  &amp;quot;% Foreign&amp;quot; = &amp;quot;prop_foreign&amp;quot;,
  &amp;quot;Restaurant Curbside Only&amp;quot; = &amp;quot;rest_actionCurbside/carryout/delivery only&amp;quot;,
  &amp;quot;Restaurant Open with Precautions&amp;quot; = &amp;quot;rest_actionOpen with social distancing/reduced seating/enhanced sanitation&amp;quot;,
  &amp;quot;Stay at Home Advisory&amp;quot; = &amp;quot;stay_statusAdvisory/Recommendation&amp;quot;,
  &amp;quot;Stay at Home Mandatory&amp;quot; = &amp;quot;stay_statusMandatory - all people&amp;quot;,
  &amp;quot;Stay at Home Certain Areas&amp;quot; = &amp;quot;stay_statusMandatory - all people in certain areas of state&amp;quot;,
  &amp;quot;Stay at Home At-risk Certain Areas&amp;quot; = &amp;quot;stay_statusMandatory - at-risk in certain areas of state&amp;quot;,
  &amp;quot;Stay at Home At Risk Only&amp;quot; = &amp;quot;stay_statusMandatory - at-risk people only&amp;quot;,
  &amp;quot;Tests&amp;quot; = &amp;quot;scale(test_prop)&amp;quot;,
  &amp;quot;Trump Vote&amp;quot; = &amp;quot;vote_share&amp;quot;
)) %&amp;gt;% 
  ggplot(aes(y=Estimate,x=reorder(stringr::str_wrap(Variables,width = 40),Estimate))) +
  geom_pointrange(aes(ymin=Estimate - 1.96*`Std. Error`,
                     ymax=Estimate + 1.96*`Std. Error`),fatten=2,alpha=0.8) +
  geom_text(aes(label=round(Estimate,digits=2)),
            size=2.5,vjust=-.75) +
  theme_tufte() +
  geom_vline(xintercept=0,linetype=2) +
  scale_colour_viridis_d() +
  scale_y_continuous(labels=scales::percent_format(scale=1)) +
  coord_flip() +
  labs(y=&amp;quot;Comparison Period&amp;quot;,x=&amp;quot;Percentage Change in Daily COVID-19 Cases&amp;quot;) +
  ggtitle(stringr::str_wrap(&amp;quot;Trump Vote Share Interaction with Mask Mandates&amp;quot;,width = 60))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/cdc_fe_files/figure-html/plotpartisan-1.png&#34; width=&#34;100%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To better understand the relationship between vote share and mask mandates, I plot the sample-average marginal effects for mask mandates below. These estimates show what effect the mask mandate seemed to have across counties when we vary vote share for Donald Trump from the lowest observed value of 4.1% voting for Trump to the maximum value of 96% voting for Trump:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# use Leeper&amp;#39;s method for calculating marginal effects
# ignore uncertainty as that would require bootstrapping/jackknifing

eps &amp;lt;- 1e-7
setstep &amp;lt;- function(x) {
  x + (max(abs(x), 1, na.rm = TRUE) * sqrt(eps)) - x
}

# loop over full range of vote shares

combined_filter &amp;lt;- ungroup(combined_data) %&amp;gt;%
  select(
    date,
    mask_status,
    test_prop,
    vote_share,
    prop_foreign,
    median_income,
    stay_status,
    rest_action,
    bar_order,
    ban_gather
  ) %&amp;gt;%
  mutate(test_prop = scale(test_prop),
         median_income = scale(median_income)) %&amp;gt;%
  filter(complete.cases(.))

over_vote &amp;lt;- parallel::mclapply(seq(
  min(combined_filter$vote_share),
  max(combined_filter$vote_share),
  length.out = 100
),
function(v) {
  this_data &amp;lt;- rbind(
    mutate(
      combined_filter,
      vote_share = v,
      mask_status = &amp;quot;No Public Mask Mandate&amp;quot;
    ),
    mutate(
      combined_filter,
      vote_share = v,
      mask_status = &amp;quot;Public Mask Mandate&amp;quot;
    )
  )
  
  # get predictions and take average difference
  
  y_hat &amp;lt;-
    predict(d1_part_between, newdata = this_data)
  
  ld &amp;lt;- nrow(combined_filter)
  
  # marginal effect = E[y_hat under treatment - y_hat under
  # control] conditional on vote_share = v
  
  marg_eff &amp;lt;-
    mean(y_hat[(ld + 1):(2 * ld)] - y_hat[1:ld])
         
         tibble(`Marginal Effect` = marg_eff,
                `Vote Share` = v)
         
         
}, mc.cores = parallel::detectCores()) %&amp;gt;% bind_rows
  
over_vote %&amp;gt;% 
  ggplot(aes(y=`Marginal Effect`,
             x=`Vote Share`)) +
  geom_line(linetype=2) +
  scale_y_continuous(labels=scales::percent_format(scale=1)) +
  scale_x_continuous(labels=scales::percent) + 
  theme_tufte() +
  labs(y=&amp;quot;Change in Growth Rates&amp;quot;,
       x=&amp;quot;Trump 2016 Vote Share&amp;quot;) +
  ggtitle(&amp;quot;Marginal Effect of Mask Mandates on COVID-19 Growth Rates&amp;quot;,
          subtitle = &amp;quot;Conditional on 2016 County-level Trump Vote Share&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/cdc_fe_files/figure-html/predictmask-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Unfortunately, I don’t have uncertainty for the plot as that would require a lot more work given the fixed effects. However, the relationship is quite strong and clear: mask mandates are most effective in counties that did not vote for Trump and least effective in counties that voted the most for Trump. In those counties, mask mandates are almost ineffective at preventing COVID-19. As a result, it’s clear that the CDC &lt;em&gt;should&lt;/em&gt; have included partisanship in their model in some way.&lt;/p&gt;
&lt;p&gt;Again, this model is imperfect: there are certainly other factors we might want to include in the model, and there is always the chance that there is some other part of the picture we are missing. Such, however, is life with observational data. We can still learn from these comparisons &lt;a href=&#34;https://osf.io/preprints/socarxiv/a492b/&#34;&gt;provided we are appropriately skeptical&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In addition, we could consider using some newer, more nuanced methods. I turn to these next.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;another-way-multiple-period-diff-in-diff&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Another Way: Multiple Period Diff-in-Diff&lt;/h2&gt;
&lt;p&gt;Recent research on panel data models has been promoting the use of what is called multi-period difference-in-difference. Difference-in-difference modeling has a big fan base because it seems to draw a compelling and possibly causally identified comparison between units observed at two time periods, pre and post-treatment. Curiously, though few notice it, the comparisons are actually of two &lt;em&gt;cross-sections&lt;/em&gt; over time, and so is substantively similar to the cross-sectional model I ran earlier.&lt;/p&gt;
&lt;p&gt;However, when there are more than two time periods, or when units are treated at different times, then the simple difference-in-difference model doesn’t apply. There issues are present in spades in the CDC’s analysis: many counties adopted mask mandates at different times and for different lengths of time. The CDC tried to restrict their sample to specific days, to avoid this, but because counties adopted mask mandates on different dates, they inevitably end up comparing different periods as well.&lt;/p&gt;
&lt;p&gt;There is increasing research on how best to make a difference-in-difference like comparison in more complicated situations. In this section I use the work of &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0304407620303948&#34;&gt;Callaway and Sant’Anna&lt;/a&gt;, which has a compelling take on this question. Using their approach, we first assign counties to the same group for comparison if they adopted a mask mandate on the same day. In that way, we might think that some of the over-time issues I mentioned earlier will not matter as much. Second, we can be more up front about what we use as the reference group, either counties that on that day did not have a mask mandate or counties which never imposed a mask mandate. For our purposes, I’ll adopt the former approach given that many counties had a mask mandate at some point. As if that weren’t enough, we can even include what they call anticipation effects, which would occur if people started to change their behavior &lt;em&gt;before&lt;/em&gt; the mask mandate was implemented.&lt;/p&gt;
&lt;p&gt;In the code below I fit their model to the CDC data with an allowance for anticipation of up to 3 days. The resulting model ends up being gigantic as the 3,000 plus counties are assigned to one of a possible 200+ groups given the number of days. The model also complained that some of our groups were consequently too small, i.e., only a few counties adopted a mask mandate on that particular day. Furthermore, I only include the additional control variables like vote share, not the CDC’s policy data as it is likely to be problematic to include it when the groups are so small. As is always the case, this model is not a panacea for our concerns due to these issues, but we can still learn quite a bit from it.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# takes about 30 minutes to estimate

if(run_did) {
  
  did_est &amp;lt;- combined_data %&amp;gt;%  
  ungroup %&amp;gt;% 
  select(cases_diff_log,first_treat,fips_num,date_num,
         vote_share,test_prop,prop_foreign,median_income,
         stay_status,rest_action,bar_order,ban_gather) %&amp;gt;% 
  filter(complete.cases(.)) %&amp;gt;% 
  att_gt(yname=&amp;quot;cases_diff_log&amp;quot;,
                          gname=&amp;quot;first_treat&amp;quot;,
                          idname=&amp;quot;fips_num&amp;quot;,
                          tname=&amp;quot;date_num&amp;quot;,
                          xformla=~vote_share + test_prop + prop_foreign + median_income,
                          # xformla=~vote_share + test_prop  +prop_foreign + median_income + stay_status + rest_action + bar_order + ban_gather,
                          control_group=&amp;quot;notyettreated&amp;quot;,
                          anticipation=3,panel=TRUE,allow_unbalanced_panel = F,
                          cores=4,weightsname = NULL,
                          est_method=&amp;quot;reg&amp;quot;,
                          data=.)
  
  saveRDS(did_est,&amp;quot;data/did_est.rds&amp;quot;)
  
} else {
  
  # File is too big to store on github, to download use this link:
  # https://drive.google.com/file/d/18GhUFbcLbx4a27Y5LJ2Ov_Iexn0ws6mb/view?usp=sharing
  
  did_est &amp;lt;- readRDS(&amp;quot;data/did_est.rds&amp;quot;)
  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then end up with separate estimates for mask mandate &lt;em&gt;for each of the 3,000+ groups&lt;/em&gt;. This is cool, though not particularly useful for us. What I like about Callaway and Sant’Anna’s work though is that they are more transparent about the different dimensions of panel data (even if they tend not use that terminology). Given the group-level estimates, they have a way of aggregating these to an effect of the mask mandate for each day of the sample. I show these in the plot below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we need to aggregate and plot these

# some bug in the package requires this to be loaded
library(DRDID)
library(matrixStats)
library(MatrixModels)

ag_did &amp;lt;- aggte(did_est,type=&amp;quot;dynamic&amp;quot;)
ag_overall &amp;lt;- aggte(did_est,type=&amp;quot;group&amp;quot;)

ggdid(ag_did) + 
  theme_tufte() +
  theme(axis.ticks.x = element_blank()) +
  scale_x_continuous(breaks=c(-200,-100,-50,-25,0,25,50,100,200)) +
  geom_hline(yintercept=0,linetype=2) +
  xlab(&amp;quot;Days Before/After Mask Mandate&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/cdc_fe_files/figure-html/didplot-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot encapsulates a much more nuanced set of comparisons than those considered previously. At the same time, the uncertainty in the estimates means there is also only so much we can learn. It would appear that at earlier and later time periods, mask mandates were associated with more COVID-19, but its impossible to say for sure. One approach to deal with this could be to collapse the data to weeks and then estimate a model, which might pool the data somewhat to get more precise answers.&lt;/p&gt;
&lt;p&gt;What we can also do with this model is combine the estimates into a single, overall effect. I show that in the table below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(`Average Treatment Effect for Treated`=ag_overall$overall.att,
       `5% CI Lower`=ag_overall$overall.att - 1.96*ag_overall$overall.se,
       `95% CI Upper`=ag_overall$overall.att + 1.96*ag_overall$overall.se) %&amp;gt;% 
  knitr::kable(digits=2) %&amp;gt;% 
  kableExtra::kable_styling()&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
Average Treatment Effect for Treated
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
5% CI Lower
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
95% CI Upper
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.33
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
-0.94
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.28
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;This table shows that it is most likely there is a negative association for mask mandates with COVID-19: hurray! Unfortunately, the estimate is fairly imprecise and it could also be associated with increased disease spread. As is so often the case, data analysis does not always produce definitive answers. However, I believe it is most important to be open about what we know and what we don’t know, which is why I spent the time to write this post. On the whole, the evidence suggests a negative association between mask mandates and COVID-19 spread, but we need to do a better job collecting additional data to allow us to do informed comparisons at the county level.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Linking to RBloggers</title>
      <link>http://www.robertkubinec.com/post/rbloggers/</link>
      <pubDate>Fri, 27 Nov 2020 15:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/rbloggers/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;You can access RBloggers, an R consortium of bloggers, &lt;a href=&#34;https://www.r-bloggers.com/&#34;&gt;at this link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why People Are Doubting the AstraZeneca Vaccine Report</title>
      <link>http://www.robertkubinec.com/post/astrazeneca/</link>
      <pubDate>Fri, 27 Nov 2020 15:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/astrazeneca/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In this blog post, I use Gelman and Loken’s &lt;a href=&#34;http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf&#34;&gt;garden of forking paths&lt;/a&gt; analysis to construct a simulation showing why skepticism of &lt;a href=&#34;https://www.astrazeneca.com/media-centre/press-releases/2020/azd1222hlr.html&#34;&gt;AstraZeneca’s vaccine results&lt;/a&gt; is warranted at this early stage. This simulation uses the numbers from their press release to illustrate how noise in treatment regimes can undermine serendipitous results. As Gelman and Loken describe, researchers can stumble on conclusions which they are then very able to provide post hoc justifications for. Unfortunately, when incentives to get results are strong, human beings are also very likely to focus on the positive at the expense of obtaining the full picture.&lt;/p&gt;
&lt;p&gt;Similar to my &lt;a href=&#34;http://www.robertkubinec.com/post/vaccinepval/&#34;&gt;last post on Pfizer’s vaccine&lt;/a&gt;, I start with &lt;a href=&#34;https://rpubs.com/ericnovik/692460&#34;&gt;Eric Novik’s excellent blog post&lt;/a&gt; on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
VE = 1 - \frac{p_t}{p_c}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(p_t\)&lt;/span&gt; is the proportion of cases in the treatment (vaccinated) group with COVID-19 and &lt;span class=&#34;math inline&#34;&gt;\(p_c\)&lt;/span&gt; is the proportion of cases in the control (un-vaccinated) group). In essence, if we assume the vaccinated group will have no more cases than the control group, this statistic will converge to 1 as &lt;span class=&#34;math inline&#34;&gt;\(p_t\)&lt;/span&gt; goes to zero, so VE of 100% would be a case in which there are no cases in the treatment group.&lt;/p&gt;
&lt;p&gt;They don’t give us all the information to figure out how many people were infected with COVID-19 in treatment versus control, but we can infer &lt;span class=&#34;math inline&#34;&gt;\(p_t\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_c\)&lt;/span&gt; by solving two equations given the fact that we know that the proportion infected in the whole trial was equal to &lt;span class=&#34;math inline&#34;&gt;\(\frac{131}{11636}\)&lt;/span&gt; and the overall VE was 0.7:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[\begin{align}
1 - \frac{p_t}{p_c} &amp;amp;= 0.7\\
p_t + p_c &amp;amp;= \frac{131}{11636}
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Thankfully, this system has a single solution where &lt;span class=&#34;math inline&#34;&gt;\(p_t\)&lt;/span&gt; is equal to  (~0.0026) and &lt;span class=&#34;math inline&#34;&gt;\(p_c\)&lt;/span&gt; is equal to  (~0.0087). In other words, the proportion infected in the treatment group was about four times less than the control group. We’ll assume that these distributions are the correct ones, and then sample subgroup-varying &lt;span class=&#34;math inline&#34;&gt;\(p_{ti}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(p_{ci}\)&lt;/span&gt; from Beta distributions that are fairly tight around these values. This will allow us to model treatment heterogeneity within the sample. The distribution of possible VEs given subgroup heterogeneity can be seen in the plot below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# use the mean/variance parameterization of the beta distribution

pt &amp;lt;- 393/151268
pc &amp;lt;- 655/75634

# Generate values for VE given beta distributions for pt/pc

VE &amp;lt;-  1 - rbeta(10000,pt*5000,(1-pt)*5000)/rbeta(10000,pc*5000,(1-pc)*5000)

hist(VE)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/astrazeneca_files/figure-html/veheg-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot shows that the true VE in the population is on average 0.7 but could vary substantially. It could be below 0.5 for a small subset of the population and above 0.9 for a small subset of the population. To simulate our data, we will draw VEs from this beta distribution separately for each of four possible subgroups &lt;span class=&#34;math inline&#34;&gt;\(i \in \{1, 2, 3, 4\}\)&lt;/span&gt; in a hypothetical study:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
VE_i \sim 1 - \frac{Beta(5000p_{ti},5000(1-p_{ti}))}{Beta(5000p_{ci},5000(1-p_{ci}))}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Four subgroups are chosen to be roughly equal to the size of the half-dose group in the AstraZeneca study given a sample size of 11,636. I add a further step in that I assume that the probability of VE being reported for a subgroup &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, which I term &lt;span class=&#34;math inline&#34;&gt;\(Pr(r=1)\)&lt;/span&gt;, is increasing in the rate of VE. That is, as VE rises, the subgroup analysis is more likely to be reported. For these reasons, we can distinguish between the full range of VE estimates for a subgroup &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(VE_i\)&lt;/span&gt;, and the particular subgroup &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; that is reported, &lt;span class=&#34;math inline&#34;&gt;\(VE_{ir=1}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Pr(VE_{ir}) = Pr(r|VE_i)VE_{ir=1} + (1 - Pr(r|VE_i))VE_{ir=0}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This equation shows that the extent of the bias in using only reported results is equal to &lt;span class=&#34;math inline&#34;&gt;\((1 - Pr(r|VE_i))VE_{ir=0}\)&lt;/span&gt;, or the probability that a result won’t be reported times the level of VE in the non-reported subgroups.&lt;/p&gt;
&lt;p&gt;The R code to generate this model in terms of treatment/control COVID cases and whether or not subgroup analyses are reported is as follows:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# number of samples

N &amp;lt;- 1000

sim_data &amp;lt;- lapply(1:N,function(n) {
  
  # generate subgroup treatment/control COVID proportions
  
  sub_pt &amp;lt;- rbeta(4,pt*5000,(1-pt)*5000)
  sub_pc &amp;lt;- rbeta(4,pc*5000,(1-pc)*5000)
  
  
  # generate subgroup VEs
  
  VE &amp;lt;-  1 - sub_pt/sub_pc
  
  # generate COVID case data with binomial distribution
  
  covid_t &amp;lt;- sapply(sub_pt, function(pt) rbinom(n=1,size = floor(11636/4),prob=pt))
  covid_c &amp;lt;- sapply(sub_pc, function(pc) rbinom(n=1,size = floor(11636/4),prob=pc))
  
  # output data
  
  tibble(draw=n,
         groups=1:4,
         sub_pt=sub_pt,
         sub_pc=sub_pc,
         VE=VE,
         VE_r=as.numeric(runif(n=4)&amp;lt;plogis(-250 + 300*VE)),
         covid_t = covid_t,
         covid_c=covid_c)
  
}) %&amp;gt;% bind_rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Given this simulation, only a minority (4%) of subgroup analyses are reported. The average VE for the reported subgroups is 77% as opposed to a VE of 69% across all simulations. As such, the simulation assumes that it is unlikely that subgroups with low vaccine efficacy will end up being reported, while subgroups with stronger efficacy are more likely to be reported.&lt;/p&gt;
&lt;p&gt;For each random draw from the simulation, I can then fit a model that analyses only those analyses that are reported:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# modified code from Eric Novik

vac_model &amp;lt;- &amp;quot;data {
  int g; // number of sub-groups
  int&amp;lt;lower=0&amp;gt; r_c[g]; // num events, control
  int&amp;lt;lower=0&amp;gt; r_t[g]; // num events, treatment
  int&amp;lt;lower=1&amp;gt; n_c[g]; // num cases, control
  int&amp;lt;lower=1&amp;gt; n_t[g]; // num cases, treatment
}
parameters {
  vector&amp;lt;lower=0, upper=1&amp;gt;[g] p_c; // binomial p for control
  vector&amp;lt;lower=0, upper=1&amp;gt;[g] p_t; // binomial p for treatment 
}
transformed parameters {
  real VE       = mean(1 - p_t ./ p_c);  // average vaccine effectiveness across groups
}
model {
  p_t ~ beta(2,2); // weakly informative,
  p_c ~ beta(2,2); // centered around no effect
  r_c ~ binomial(n_c, p_c); // likelihood for control
  r_t ~ binomial(n_t, p_t); // likelihood for treatment
}
generated quantities {
  vector[g] effect   = p_t - p_c;      // treatment effect
  vector[g] log_odds = log(p_t ./ (1 - p_t)) -
                  log(p_c ./ (1 - p_c));
}&amp;quot;

to_stan &amp;lt;- cmdstan_model(write_stan_file(vac_model))



est_bias &amp;lt;- lapply(unique(sim_data$draw), function(i) {
  
  sink(&amp;quot;output.txt&amp;quot;)
  
  this_data &amp;lt;- filter(sim_data,draw==i,VE_r==1)
  
  stan_data &amp;lt;- list(g=length(unique(this_data$groups)),
                    r_c=as.array(this_data$covid_c),
                    r_t=as.array(this_data$covid_t),
                  n_c=as.array(round(rep(floor(11636/4),
                                length(unique(this_data$groups)))/2)),
                  n_t=as.array(round(rep(floor(11636/4),
                                length(unique(this_data$groups)))/2)))
  
  if(stan_data$g&amp;lt;1) {
    tibble(draw=i,
           VE=NA)
     
  } else {
    est_mod &amp;lt;- to_stan$sample(data=stan_data,
                              seed=624018,
                              chains=1,iter_warmup=500,iter_sampling=1000,refresh=500)
  
    draws &amp;lt;- est_mod$draws() %&amp;gt;% as_draws_df
  
    tibble(draw=i,
         VE=draws$VE)
  }
  
  sink()
  
}) %&amp;gt;% bind_rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I can then plot the density of the estimated reported VE from this simulation along with a line indicating the true average VE in the population:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;est_bias %&amp;gt;% 
  ggplot(aes(x=VE)) +
  geom_density(fill=&amp;quot;blue&amp;quot;,alpha=0.5) +
  geom_vline(xintercept=mean(sim_data$VE),linetype=2) +
  theme_tufte() +
  xlim(c(0,1)) +
  xlab(&amp;quot;Reported VE&amp;quot;) +
  ylab(&amp;quot;&amp;quot;) +
  annotate(&amp;quot;text&amp;quot;,x=mean(sim_data$VE),y=3,label=paste0(&amp;quot;True VE = &amp;quot;,round(mean(sim_data$VE)*100,1),&amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/astrazeneca_files/figure-html/sample-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, it is much more likely that VEs higher than the true average VE will be reported. The extent of the bias is driven by the simulation and how much weight the simulation puts on discovering high VE values. Given the profit that AstraZeneca stands to gain, and the fame and prestige for the involved academics, it would seem logical that reported analyses from subgroups will tend to be subgroups that out-perform the average.&lt;/p&gt;
&lt;p&gt;Note that this simulation shows how AstraZeneca could be reporting valid statistical results, yet these results can still be a biased estimate of what we want to know, which is how the vaccine works for the population as a whole. The possibility of random noise in subgroups and the fact that subgroups are likely to respond differently means that we should be skeptical of analyses that only report certain subgroups instead of all subgroups. Only when we understand the variability in subgroups can we say whether the reported finding in AstraZeneca’s press release represents a real break-through or simply luck. Of course, they can test it directly by doing another trial, which it seems to be is their intention. Serendipity, though, isn’t enough of a reason to trust these results unless we can examine all of their data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A More Realistic P-Value for the Pfizer Vaccine Report</title>
      <link>http://www.robertkubinec.com/post/vaccinepval/</link>
      <pubDate>Thu, 19 Nov 2020 15:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/vaccinepval/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I start with &lt;a href=&#34;https://rpubs.com/ericnovik/692460&#34;&gt;Eric Novik’s excellent blog post&lt;/a&gt; on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
VE = 1 - \frac{p_t}{p_c}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&#34;math inline&#34;&gt;\(p_t\)&lt;/span&gt; is the proportion of cases in the treatment (vaccinated) group with COVID-19 and &lt;span class=&#34;math inline&#34;&gt;\(p_c\)&lt;/span&gt; is the proportion of cases in the control (un-vaccinated) group). This is kind of an odd statistic as it would seem to make more sense to simply report the ratio rather than one minus the ratio, although subtracting one means a null ratio (difference of 1) equals zero. I am not sure a general audience is very aware of the distinction and what VE in fact means. In essence, if we assume the vaccinated group will have no more cases than the control group, this statistic will converge to 1 as &lt;span class=&#34;math inline&#34;&gt;\(p_t\)&lt;/span&gt; goes to zero, so VE of 100% would be a case in which there are no cases in the treatment group.&lt;/p&gt;
&lt;p&gt;As Eric Novik shows, the VE is actually a calculated quantity and isn’t modeled directly. The &lt;a href=&#34;https://pfe-pfizercom-d8-prod.s3.amazonaws.com/2020-09/C4591001_Clinical_Protocol.pdf&#34;&gt;clinical pre-registration&lt;/a&gt; says they will use a beta-binomial model. The put a prior on a transformation of the VE (vaccine efficacy) of &lt;span class=&#34;math inline&#34;&gt;\(Beta(0.700102, 1)\)&lt;/span&gt;. We can get an idea of what that looks like by examining the distribution of these values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prior_VE &amp;lt;- rbeta(10000,.700102,1)

round(quantile((2*prior_VE - 1)/(prior_VE - 1),probs=seq(0,1,by=.05)),5)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##          0%          5%         10%         15%         20%         25% 
## -4994.45563   -12.93514    -5.62962    -3.11045    -1.86012    -1.15668 
##         30%         35%         40%         45%         50%         55% 
##    -0.63120    -0.28035    -0.00591     0.19966     0.36281     0.50447 
##         60%         65%         70%         75%         80%         85% 
##     0.61128     0.69638     0.76767     0.83150     0.88354     0.92454 
##         90%         95%        100% 
##     0.95870     0.98584     1.00000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This prior puts significant mass at negative VEs which would suggest that the vaccine actually makes things worse for the treatment group than for the control group. While this would seem unrealistic, it also suggests that very high VEs, such as above 90%, are relatively unlikely. We can see that the prior suggests it is about as likely that the VE is negative (the vaccine causes the virus) as it is likely that the VE is greater than 80%.&lt;/p&gt;
&lt;p&gt;Given this conservative but weakly informative prior, we can then calculate a p-value for what they pre-register in their study, which is the probability that the VE (vaccine efficacy) is greater than 30%. We can then fit a Bayesian beta-binomial model with these priors by modifying Eric Novik’s original code:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# from Eric Novik, changed the prior per pre-reg

vac_model &amp;lt;- &amp;quot;data {
  int&amp;lt;lower=1&amp;gt; r_c; // num events, control
  int&amp;lt;lower=1&amp;gt; r_t; // num events, treatment
  int&amp;lt;lower=1&amp;gt; n_c; // num cases, control
  int&amp;lt;lower=1&amp;gt; n_t; // num cases, treatment
  real a[2];   // prior values for treatment effect
  
}
parameters {
  real&amp;lt;lower=0, upper=1&amp;gt; p_c; // binomial p for control
  real&amp;lt;lower=0, upper=1&amp;gt; p_t; // binomial p for treatment 
}
transformed parameters {
  real VE       = 1 - p_t / p_c;  // vaccine effectiveness
}
model {
  (VE - 1)/(VE - 2) ~ beta(a[1], a[2]); // prior for treatment effect
  r_c ~ binomial(n_c, p_c); // likelihood for control
  r_t ~ binomial(n_t, p_t); // likelihood for treatment
}
generated quantities {
  real effect   = p_t - p_c;      // treatment effect
  real log_odds = log(p_t / (1 - p_t)) -
                  log(p_c / (1 - p_c));
}&amp;quot;

to_stan &amp;lt;- cmdstan_model(write_stan_file(vac_model))

n &amp;lt;- 4.4e4  # number of volunteers
r_c &amp;lt;- 162  # number of events in control
r_t &amp;lt;- 8    # number of events in vaccine group

stan_data &amp;lt;- list(n=n,r_c=r_c,r_t=r_t,
                  n_c=n/2,n_t=n/2,
                  a=c(.700102,1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we can sample and plot the results. We’ll draw a lot of samples so we can get good estimates in the tails (i.e. very low p-values).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pfizer_est &amp;lt;- to_stan$sample(data=stan_data,chains=1,iter_warmup=500,iter_sampling=100000,refresh=50000,show_messages=F)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Running MCMC with 1 chain...
## 
## Chain 1 Iteration:      1 / 100500 [  0%]  (Warmup) 
## Chain 1 Iteration:    501 / 100500 [  0%]  (Sampling) 
## Chain 1 Iteration:  50500 / 100500 [ 50%]  (Sampling) 
## Chain 1 Iteration: 100500 / 100500 [100%]  (Sampling) 
## Chain 1 finished in 2.1 seconds.&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;draws &amp;lt;- pfizer_est$draws() %&amp;gt;% as_draws_df&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here’s a plot of the VE as reported in the press release with the dotted line as the average estimate:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean_ve &amp;lt;- mean(draws$VE)

draws %&amp;gt;% 
  ggplot(aes(x=VE)) +
  geom_density(fill=&amp;quot;blue&amp;quot;,alpha=0.5)+
  theme_tufte() +
  ylab(&amp;quot;&amp;quot;) +
  scale_x_continuous(labels=scales::percent_format(accuracy = 1)) +
  geom_vline(aes(xintercept=mean(VE)),linetype=2) +
  annotate(&amp;quot;text&amp;quot;,x=mean_ve,y=12,label=paste0(&amp;quot;  VE = &amp;quot;,round(mean_ve*100,1),&amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/vaccinepval_files/figure-html/veplot-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This estimate matches the press release. So let’s calculate the p-value:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(1 - as.numeric(draws$VE&amp;gt;.3))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Virtually nil.&lt;/p&gt;
&lt;p&gt;However, we can also test some other thresholds, such as the FDA value of 50% VE:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(1 - as.numeric(draws$VE&amp;gt;.5))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Still vanishingly small. We can try something a bit closer, such as whether VE exceeds 90%:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mean(1 - as.numeric(draws$VE&amp;gt;.9))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01913&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This p-value is at least reportable at 0.02. As this is a Bayesian model, this can be interpreted directly that the probability that the vaccine efficacy is less than 90% is quite small, less than a 2% chance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why You Should Be Careful About the MaterniT 21 Test</title>
      <link>http://www.robertkubinec.com/post/maternity21/</link>
      <pubDate>Sun, 13 Sep 2020 15:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/maternity21/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;My wife and I have been faced with a decision in our pregnancies that has always caused me some consternation: should we take the MaterniT 21 test to see if our baby might have Down’s syndrome (trisomy 21) or other genetic abnormalities? This test, marketed by the company LabCorp, offers a way to analyze the blood of a pregnant woman for DNA markers that could indicate genetic problems. While non-invasive, it isn’t cheap (at least in the United States), and if it returns a positive result, it could be used for more invasive procedures like amniocentesis that could be harmful to the fetus.&lt;/p&gt;
&lt;p&gt;So while it might seem like a good idea (let’s learn as much as we can), the test can only guide decisions if it’s reasonably accurate. However, due to corporate greed and manipulated statistics, people put way too much trust in the test when they probably shouldn’t. As a result, women and their partners may be led to make unwise decisions based on misplaced confidence in the test.&lt;/p&gt;
&lt;p&gt;In this post I explain why you shouldn’t buy into the company’s marketing about the test’s accuracy–in fact, LabCorp is patently misleading consumers about how useful its test is for diagnosing Down’s syndrome in utero. It will take a bit of time to read through this post, so I’ll give the highlights first:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For women under 30, even if they get a &lt;strong&gt;positive&lt;/strong&gt; result from the MaterniT 21 test, it’s still more likely that the fetus &lt;strong&gt;does not&lt;/strong&gt; have Down’s syndrome.&lt;/li&gt;
&lt;li&gt;Only for women over 40 does the test provide reasonably conclusive results.&lt;/li&gt;
&lt;li&gt;The misleading statistics put out by LabCorp might convince some women to do dangerous, invasive follow-on testing (or even an abortion) based on misplaced confidence in the test.&lt;/li&gt;
&lt;/ul&gt;
&lt;div id=&#34;investigating-labcorp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Investigating LabCorp&lt;/h2&gt;
&lt;p&gt;LabCorp (under its subsidiary Integrated Genetics) offers a &lt;a href=&#34;https://www.integratedgenetics.com/sites/default/files/2020-04/rep-1039-v6-1119.pdf&#34;&gt;brochure for the test on its website&lt;/a&gt;. This brochure mentions how accurate the test is in only one place:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/img/brochure.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;
While they don’t define what the technical terms are in the parentheses, 97.9% does sound pretty good. In another part of the brochure, the company says that “the test delivers clear positive or negative results.” I take this as enough evidence to say that LabCorp wants women to believe that the test will determine whether the fetus has Down’s syndrome with reasonable confidence, such as a greater than 90% probability.&lt;/p&gt;
&lt;p&gt;Now for the fun part–I’m going to break down what is meant by the number cited in the brochure above and trace it back to the original research to see what it actually means. You may have noticed the footnote in the image above–that is a reference to &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/24657131/&#34;&gt;an article published in 2014 that evaluated tests like MaterniT 21, known as cell-free DNA tests&lt;/a&gt;. A scan of the article’s results shows that LabCorp did indeed get the number right–97.9% for trisomy 21. They did not, of course, mention that the article found much lower rates of detection for other trisomies and genetic conditions, but for now we’ll ignore that oversight. The test is marketed as a way to detect Down’s syndrome, so let’s focus on just that as the test’s selling point.&lt;/p&gt;
&lt;p&gt;As I was doing research, I found a &lt;a href=&#34;https://obgyn.onlinelibrary.wiley.com/doi/full/10.1002/uog.14791&#34;&gt;more recent paper that did a meta-analysis&lt;/a&gt;, which is a compilation of the results of all previous research, in this case on tests like MaterniT 21. This paper is quite useful for evaluating the test as it aggregates information from many different researchers, lowering the concern that any one study was a fluke. It turns out that this meta-analysis has a number very close to what the brochure article cited – 99.2%, i.e., even higher.&lt;/p&gt;
&lt;p&gt;At this point you may think, “your concerns about this test seem unfounded.” Ah, but we have not yet discussed what these numbers &lt;em&gt;actually mean&lt;/em&gt;, which if you notice, the brochure did not provide any more information about. All it said was 97.9% positive-predictive value for a “high-risk cohort.”&lt;/p&gt;
&lt;p&gt;My intuition is that relatively few readers of the brochure would have any idea what that refers to. Most people just want to know how accurate the test is, and 97.9% sounds really high, so it must be good. In reality, it is much harder to know how well a test works, because the usefulness of a test &lt;em&gt;depends on how hard it is to find what the test detects.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Explaining this point is difficult, so I’ll use as an example that is easier to relate to. I’m sure you’ve come across the hand-held metal detectors of the kind that people like to use for treasure hunting on beaches (see picture below). Suppose you are a person who has taken up an interest in treasure hunting and you want to know how useful these metal detector will be in helping you find Blackbeard’s long-lost hoard.&lt;/p&gt;
&lt;p&gt;What you really want to know is, if the thing beeps, does it mean you’ve found treasure? The answer to this question just so happens to be the same calculation as the “positive-predictive value” that the test company put in their brochure.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/img/detect.png&#34; height=&#34;50%&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The detector can easily go wrong if it beeps when it comes across a worthless piece of scrap metal. On the other hand, the detector could also fail if it didn’t beep because a treasure hoard was buried too deep. The first case is called a &lt;em&gt;false positive&lt;/em&gt;: the machine beeped but you didn’t find any treasure. The second case is called a &lt;em&gt;false negative&lt;/em&gt;: you were standing on treasure and the machine didn’t help you out one bit.&lt;/p&gt;
&lt;p&gt;So let’s get back to the brochure. The &lt;em&gt;positive predictive value&lt;/em&gt; is equal to the chance that your metal detector’s beep means you hit pay dirt. But if you ask a metal detector salesperson what the chance is that you find treasure if the machine beeps, you might get some evasive answers/sarcastic smirks. He or she certainly won’t be able to tell you how likely it is that you find treasure, unless… the salesperson actually knows how much treasure there is in the sand (in which case you might wonder why they are still a salesperson). The point is, you can only calculate a positive predictive value for your detector if you know how likely it is you’ll be standing on treasure at any point along the beach. If treasure is impossibly rare–which it probably is, I hate to break it to you–you’ll be digging up a lot of empty sand or rusty beer cans even if the machine’s sensors are working fine.&lt;/p&gt;
&lt;p&gt;The point of this whole analogy is that knowing what you should do if the machine beeps &lt;em&gt;depends on how much treasure is in the sand.&lt;/em&gt; If you knew that you were standing right next to a sunken pirate ship, you would probably want to dig up every beep you hear. If you’re on the Jersey shore and the sand is littered with old Dr. Pepper cans and fish hooks, you might not want to dig up anything unless you need the exercise.&lt;/p&gt;
&lt;p&gt;Now I can finally explain the test company’s deception. They provided a positive predictive value, which is what you want to know: what’s the chance the fetus has Down’s syndrome if the test comes back positive? But LabCorp pulled this number from &lt;em&gt;a high-risk cohort&lt;/em&gt;. Doing so is no different than selling you a metal detector and telling you it’ll detect treasure 97% percent of the time so long as you’re standing on Blackbeard’s grave.&lt;/p&gt;
&lt;p&gt;So where did this high-risk cohort come from? What is very important to know (and completely ignored by the test brochure) is the fact that the risk of Down’s syndrome increases dramatically with age. The chart below is from the CDC, and you can see that the rate of a live birth with Down’s syndrome increases very rapidly once a woman reaches 40 years of age.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/img/downs.png&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now let’s go back to the papers they cited as proof of the positive predictive value. I’ll only focus on the meta-analysis as it represents a lot of combined research. Unfortunately, and to me somewhat surprisingly, the paper does not report the median age of women in the study. However, because we know the actual number of women who had children with Down’s syndrome from the paper, we can figure out how old the women were by calculating the percentage who ended up having babies with Down’s. I calculate this in the code below based on a table of data in the article. The code is shown for transparency’s sake, and I include the full table at the end of this blog post as a reference.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;journal_data &amp;lt;- readxl::read_xlsx(&amp;quot;downs_table.xlsx&amp;quot;)
print(paste0(&amp;quot;Percentage of live births with Down&amp;#39;s Syndrome in the meta-analysis study is: &amp;quot;,round(sum(journal_data$positives)/(sum(journal_data$positives) + sum(journal_data$negatives)),3)*100,&amp;quot;%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Percentage of live births with Down&amp;#39;s Syndrome in the meta-analysis study is: 4.6%&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A rate of 4.6% of babies with Down’s syndrome means that the women in the study were probably 46 years old on average, according to &lt;a href=&#34;https://www.ndss.org/about-down-syndrome/down-syndrome/&#34;&gt;data from the National Down Syndrome Society&lt;/a&gt;. By comparison, the CDC says that the rate of Down’s syndrome among newborns for the population as a whole is only 0.001%. The fetuses in the studies in this meta-analysis had a rate of Down’s syndrome &lt;strong&gt;50 times&lt;/strong&gt; higher than average. In other words, the testing company used the equivalent of a beach full of buried treasure to evaluate the usefulness of its metal detector.&lt;/p&gt;
&lt;p&gt;At this point, you might start to feel suspicious about the articles I cited. Were the scientists running these studies in cahoots with the testing company? Probably not. Doing these kinds of studies is expensive–they have to enroll women, then test them, then evaluate their babies after delivery–and if they had an average set of women, they wouldn’t have very many fetuses that would ultimately turn out to have Down’s, requiring them to enroll many more subjects in the study. In a similar sense, if you were designing a metal detector, you might go out and test it by burying some gold coins in the beach and see if it works rather than stroll aimlessly around until you found treasure. So we shouldn’t necessarily blame the scientists, though we certainly should question the LabCorp’s intentionally deceptive use of the study’s statistics in selling the test.&lt;/p&gt;
&lt;p&gt;We can get more useful positive predictive values by re-calculating the results from the article above and adjusting the Down’s syndrome prevalence to match younger women’s ages. I calculate this in the code block below and plot the positive predictive value by age:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;journal_data &amp;lt;- mutate(journal_data,detection=as.numeric(stringr::str_extract(detection,&amp;quot;[0-9]+&amp;quot;)),
                       fpr=as.numeric(stringr::str_extract(fpr,&amp;quot;[0-9]+&amp;quot;)))

sensitivity &amp;lt;- sum(journal_data$positives)/(sum(journal_data$positives) + (sum(journal_data$positives) - sum(journal_data$detection)))

specificity &amp;lt;- sum(journal_data$negatives)/(sum(journal_data$negatives) + sum(journal_data$fpr))

# need Down&amp;#39;s syndrome prevalence data by age
# Data taken from: https://journals.sagepub.com/doi/abs/10.1136/jms.9.1.2
# Using predicted values from model vs. observed values
# Morris, Dutton and Alberman (2002)

age_trends &amp;lt;- readxl::read_xlsx(&amp;quot;downs_estimates.xlsx&amp;quot;) %&amp;gt;% filter(age&amp;gt;13, age&amp;lt;53) %&amp;gt;% 
  mutate(pred_ods=as.numeric(stringr::str_remove(pred_ods,&amp;quot;1:&amp;quot;)),
         # convert odds to percentages
         pred_ods=1/pred_ods,
         # false positive rate
         fp=(1 - pred_ods)*(1 - specificity),
         ppv=pred_ods / (pred_ods + fp))

age_trends %&amp;gt;% 
  ggplot(aes(y=ppv,x=age)) +
  geom_line() + 
  ggtitle(&amp;quot;Chance of Fetus with Downs if MaterniT 21 Comes Back Positive&amp;quot;) +
  scale_y_continuous(labels=scales::percent) + 
  geom_hline(yintercept=0.5,linetype=2) +
  geom_hline(yintercept=0.979,linetype=3,colour=&amp;quot;red&amp;quot;) +
  theme(panel.background = element_blank()) +
  ylab(&amp;quot;Chance of Fetus with Down&amp;#39;s Syndrome&amp;quot;) +
  xlab(&amp;quot;Age of Mother&amp;quot;) +
  annotate(&amp;quot;text&amp;quot;,x=c(20,42),y=c(.52,.99),label=c(&amp;quot;50%&amp;quot;,&amp;quot;LabCorp Brochure: 97.9%&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/maternity21_files/figure-html/ppv-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, the chance that a fetus has Down’s Syndrome if the test comes back positive–the positive predictive value–does not start to rise sharply until a woman is in her late 30s and 40s. Even in her early 30s, a positive test result only means there is a 50 percent chance that the fetus has Down’s Syndrome. I calculated these numbers from the meta-analysis that showed the test was pretty accurate, so the numbers could even be lower with the results of a different, less optimistic study.&lt;/p&gt;
&lt;p&gt;You can see now that the test company’s positive predictive value of 97.9% only applies to women at the far end of the curve–around age 42 or 43 and higher. For the vast majority of women in child-bearing years, &lt;em&gt;this number is simply inaccurate&lt;/em&gt;. Technically, the test company can claim they included the proviso “in a high-risk cohort,” but it seems very unlikely anyone would go as far as I did to know what that means. And it turns out, it means &lt;em&gt;a lot&lt;/em&gt;. For women under 30, it’s still more likely than not that their fetus is free of Down’s even if the test comes back positive. If women and their partners were told, “even if the test comes back positive, there’s still only a 40 percent chance the fetus has Down’s Syndrome,” I do wonder how many people would still choose to take it.&lt;/p&gt;
&lt;p&gt;The real punch, though, is that this non-invasive test can be used as a reason to do a more dangerous, invasive test. One potential test a doctor might recommend next, amniocentesis, &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/16946222/#:~:text=The%20overall%20loss%20rates%20were,rate%20for%20amniocentesis%20over%20time.&#34;&gt;has a risk of miscarriage as high as 1 percent&lt;/a&gt;. Let’s suppose that a million women under 30 take this test. According to my analysis above, there would probably be 832 false positive results (test results that came back positive but the fetus does not have Down’s Syndrome). If everyone went on to do amniocentesis to confirm the diagnosis, that would result in 8 miscarriages of otherwise healthy pregnancies. Considering that there are about 4 million live births per year in the United States, and the average age of a woman giving birth is 27, these disturbing numbers are quite plausible.&lt;/p&gt;
&lt;p&gt;While it would seem that most people (and their doctors) would choose more invasive testing to confirm Down’s before reaching conclusions, it still strikes me as entirely plausible that someone would consider aborting the fetus based on the first, non-invasive positive result. That particular possibility frightens me, that someone might take the first relatively inaccurate test as a reason to terminate a pregnancy.&lt;/p&gt;
&lt;p&gt;For all these reasons, do share this research with others. It’s important to know what tests really do, and also to pressure Congress and regulators to force test providers to release clear and non-misleading statistics. It should be as easy for them to compile a chart like the one I did above. In fact, these kinds of plots should be included in all scholarly research on tests to avoid people mis-characterizing results.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;downs-syndrome-data-table&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Down’s Syndrome Data Table&lt;/h2&gt;
&lt;p&gt;This table derived from Table 2 in &lt;a href=&#34;https://obgyn.onlinelibrary.wiley.com/doi/full/10.1002/uog.14791&#34;&gt;Gil, Quezada, Revello, Akolekar, and Nicolaides (2015)&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;Study&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Method&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;GA&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;True Positives&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Detected&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;True Negatives&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;False Positives&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Chiu (2011)41&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;13 (—)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;86&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;146&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Ehrich (2011)42&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16 (8–36)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;410&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Palomaki (2011)43&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15 (8–21)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;212&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;209&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1471&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sehnert (2011)44&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15 (10–28)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;13&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Ashoor (2012)45&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12 (11–13)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;347&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Bianchi (2012)46&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15 (10–23)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;404&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Jiang (2012)48&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;— (10–34)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;887&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Lau (2012)49&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12 (11–28)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;97&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Nicolaides (2012)50&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12 (11–13)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1941&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Norton (2012)51&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16 (10–38)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;81&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;81&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2888&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Sparks (2012)53&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;18 (11–36)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;36&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;131&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Guex (2013)55&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;12 (11–13)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;146&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Liang (2013)57&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;21 (11–39)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;367&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Nicolaides (2013)59&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SNP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;13 (11–13)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;204&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Song (2013)61&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;16 (11–21)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1733&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Verweij (2013)62&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14 (10–28)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;17&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;486&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Bianchi (2014)63&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;17 (8–39)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1947&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Comas (2014)64&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSS/SNP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14 (9–23)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;311&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Pergament (2014)71&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;SNP&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;14 (7–40)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;58&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;905&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Porreco (2014)72&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;17 (9–37)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;137&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;137&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3185&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Shaw (2014)73&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&amp;gt; 12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;184&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Stumm (2014)74&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15 (11–32)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;41&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;40&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;430&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Quezada (2015)75&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;CSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;10 (10–11)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2753&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Song (2015)76&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;MPSS&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;9 (8–12)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;201&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>What Panel Data Is Really All About</title>
      <link>http://www.robertkubinec.com/post/fixed_effects/</link>
      <pubDate>Wed, 22 Apr 2020 15:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/fixed_effects/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;We’ve all been in that seminar where the author puts up a slide containing regression coefficients, and buried in the bottom line of the table we can see little Ys and Ns indicating the kind of panel data model employed. Quite often, these brief descriptions of models are taken as a mere statistical ritual, believed to be efficacious at appeasing the mercurial statistical deities, but rarely if ever investigated more thoroughly. Furthermore, no one wants to be in the difficult position of discussing panel data models, which inevitably boils down to a conversation laced with econometric terms generating more heat than light.&lt;/p&gt;
&lt;p&gt;So what if I told you … panel data, or data with two dimensions, such as repeated observations on multiple cases over time, is really not that complicated. In fact, there are only two basic ways to analyze panel data, which I will explain briefly in this piece, just as every panel dataset has two basic dimensions (cases and time). However, when we confuse these dimensions, bad things can happen. In fact, one of the most popular panel data models, the two-way fixed effects model–widely used in the social sciences–is in fact statistical nonsense because it does not clearly distinguish between these two dimensions. This statement should sound implausible to you–really?, but it’s quite easy to demonstrate, as I’ll show you in this post.&lt;/p&gt;
&lt;p&gt;This blog post is based on a recent publication with Jonathan Kropko which you can &lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0231349&#34;&gt;access here&lt;/a&gt;. In this post I provide a more reader-friendly overview of the article, dropping the academic-ese and focusing on substance as I think there are important issues for many people doing research.&lt;/p&gt;
&lt;p&gt;In short: &lt;strong&gt;there are an untold number of analyses of panel data affected by an issue that is almost impossible to identify because R and Stata obscure the problem.&lt;/strong&gt; Thanks to multi-collinearity checks that automatically drop predictors in regression models, a two-way fixed effects model can produce sensible-looking results that are not just irrelevant to the question at hand, but practically nonsense. Instead, we would all be better served by using simpler 1-way fixed effects models (intercepts on time points or cases/subjects, but not both).&lt;/p&gt;
&lt;div id=&#34;how-we-think-about-panel-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How We Think About Panel Data&lt;/h2&gt;
&lt;p&gt;Panel data is one of the richest resources we have for observational inference in the social and even natural sciences. By comparing cases to each other as they change over time, such as countries and gross domestic product (GDP), we can learn much more than we can by only examining isolated cases. However, researchers for decades have been told by econometricians and applied statisticians that they &lt;em&gt;must&lt;/em&gt; use a certain kind of panel data model or risk committing grave inferential errors. As a result, probably the most common panel data model is to include case fixed effects, or a unique intercept (i.e. a dummy variable) for every case in the panel data model. The second most likely is to include a fixed effect or intercept for every time point &lt;em&gt;and&lt;/em&gt; case in the data. The belief is that including all these intercepts will control for omitted variables because there are factors unique to cases or time points that will be “controlled for” by these intercepts.&lt;/p&gt;
&lt;p&gt;The result of this emphasis on the lurking dangers of panel data inference results in what I have decided to call the Cheeseburger Syndrome. This phenomenon was first studied by Saturday Night Live in a classic 1980s skit:&lt;/p&gt;
&lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/puJePACBoIo&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;p&gt;Applied researchers are in the position of customers in the video, continually asking for models that will analyze the variation in their panel data which actually exists. Applied statisticians are often the cooks, informing customers that regardless of what particular dataset they have, they will only ever be able to get a “cheeseburger.” As a result, there is remarkable uniformity of application of panel data models in the social sciences, even if these models don’t always fit the data very well.&lt;/p&gt;
&lt;p&gt;What we put forward in our piece linked above is that any statistical model should have, as its first requirement, that it match the researcher’s question. Problems of omitted variables are important, but necessarily secondary. It does not matter how good the cheeseburger is if the researcher really wants eggs easy over.&lt;/p&gt;
&lt;p&gt;In addition, fixed effects models &lt;em&gt;do not&lt;/em&gt; control for omitted variables. What fixed effect models do is isolate one dimension of variance in the model. As a result, any variables that don’t vary on that dimension are by definition removed from the model. This side-effect is trumpeted as the great inferential benefit of fixed effect models, but it has nothing to do with inference. Fixed effects (or their cousin, random effects/hierarchical models) are simply about selecting which part of the panel dataset is most germane to the analysis.&lt;/p&gt;
&lt;p&gt;The rule of thumb that we put forward in our paper is that fixed effects/dummy variables/intercepts on cases correspond to the following research question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How much does a case change in relation to itself over time?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While fixed effects/dummy variables/intercepts on time points correspond to the following research question:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How much does a case change relative to other cases?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some questions are fruitfully asked when comparing cases to themselves over time. For example, if a case is a human being, we might want to know whether obtaining more education leads to higher earnings. Conversely, if a case is a country, we might want to know if wealthier countries tend to be more democratic than poorer countries. Some questions primarily employ within-case variation, while others look at cross-sectional variation. Both are present in any panel dataset, and both are potentially interesting.&lt;/p&gt;
&lt;p&gt;If fixed effects enable &lt;em&gt;comparisons&lt;/em&gt;, then what happens if we have dummy variables/intercepts for every case and time point (the so-called two-way fixed effects model)? What comparison are we then making? As it turns out, the answer to this question is not very clear at all. I refer you to the paper above for a full exposition, but in essence, because cases and time points are nested, we end up making comparisons across both dimensions simultaneously, and this is just as obtuse as it sounds. There is no clear research question that matches this model.&lt;/p&gt;
&lt;p&gt;Furthermore, if the original aim was to remove omitted variables, these omitted variables inevitably end up in the estimate again because a two-way estimate necessarily relates to both dimensions of variance simultaneously. As a result, it is not very clear what the point is. The one known use of the model is for difference-in-difference estimation, but only with two time points (see our paper for more detail).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exposition-with-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exposition with Data&lt;/h2&gt;
&lt;p&gt;This all may sound to you like a nice story. But how can you really know we are telling the truth? There are plenty of equations in our paper, but there is nothing like being able to see the data. One of the contributions of our paper is to create a simulation for panel data where we can control the within-case and cross-sectional variation separately for the same panel data set. This allows us to compare all of the possible panel data models with the same dataset while including or excluding omitted variables and other issues.&lt;/p&gt;
&lt;p&gt;To show you how this works, I will generate a dataset with a single covariate in which the effect of that covariate is +1 for within-case comparisons and -3 for cross-sectional comparisons. There is noise in the data, but the effect is the same for all cases and for all cross-sections. The following code simulates fifty cases and fifty time points using our &lt;code&gt;panelsim&lt;/code&gt; package (currently available only via &lt;a href=&#34;http://www.github.com/saudiwin/panelsim&#34;&gt;Github&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# to install this package, use 
# remotes::install_github(&amp;quot;saudiwin/panelsim&amp;quot;)

require(panelsim)

# generate dataset with fixed coefficients within cases and cross-section
# no variation in effects across cases or time

gen_data &amp;lt;- tw_data(N=50,
                    T=50,
                    case.eff.mean = 1,
                    cross.eff.mean = -3,
                    cross.eff.sd = 0,
                    case.eff.sd = 0,
                    noise.sd=.25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because there is only one covariate, we can pretty easily visualize the relationships in the cross sectional and within-case variation. First, the value of the outcome/response &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; is shown for five cases, where one dot represents the value of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; for each time point for that case:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_data$data %&amp;gt;%
  filter(case&amp;lt;5) %&amp;gt;% 
  ggplot(aes(x=x,y=y)) +
           geom_point() +
           stat_smooth(method=&amp;quot;lm&amp;quot;) +
           facet_wrap(~case,scales=&amp;quot;free&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/fixed_effects_files/figure-html/case_out-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, there is a consistent positive relationship of approximately +1 between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; within cases. We can also examine the relationship for the cross-section by subsetting the data to each time point and plotting the cases for five of the time points:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_data$data %&amp;gt;%
  filter(time&amp;lt;5) %&amp;gt;% 
  ggplot(aes(x=x,y=y)) +
           geom_point() +
           stat_smooth(method=&amp;quot;lm&amp;quot;) +
           facet_wrap(~time,scales=&amp;quot;free&amp;quot;) +
  theme_minimal()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/fixed_effects_files/figure-html/time_out-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There is a separate and quite distinct relationship in the cross section between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;. Both components are present in the outcome. To find the actual coefficient, we can simply fit linear regression models on the generated data, first with intercepts/dummies for cases:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y ~ x + factor(case),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(case), data = gen_data$data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8932 -0.1599 -0.0015  0.1682  0.8304 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     0.773045   0.035151  21.992  &amp;lt; 2e-16 ***
## x               1.002403   0.018501  54.182  &amp;lt; 2e-16 ***
## factor(case)2  -0.184927   0.049529  -3.734 0.000193 ***
## factor(case)3   1.466627   0.050043  29.307  &amp;lt; 2e-16 ***
## factor(case)4  -1.363988   0.049898 -27.335  &amp;lt; 2e-16 ***
## factor(case)5  -0.884801   0.049660 -17.817  &amp;lt; 2e-16 ***
## factor(case)6   1.097485   0.049812  22.032  &amp;lt; 2e-16 ***
## factor(case)7  -0.992114   0.049705 -19.960  &amp;lt; 2e-16 ***
## factor(case)8  -1.597938   0.050057 -31.922  &amp;lt; 2e-16 ***
## factor(case)9  -2.126106   0.050445 -42.147  &amp;lt; 2e-16 ***
## factor(case)10 -0.028781   0.049525  -0.581 0.561205    
## factor(case)11 -1.171168   0.049831 -23.503  &amp;lt; 2e-16 ***
## factor(case)12 -0.132703   0.049526  -2.679 0.007423 ** 
## factor(case)13 -0.612141   0.049588 -12.344  &amp;lt; 2e-16 ***
## factor(case)14  0.866259   0.049705  17.428  &amp;lt; 2e-16 ***
## factor(case)15 -0.867871   0.049670 -17.473  &amp;lt; 2e-16 ***
## factor(case)16  0.514860   0.049600  10.380  &amp;lt; 2e-16 ***
## factor(case)17 -0.633858   0.049621 -12.774  &amp;lt; 2e-16 ***
## factor(case)18 -0.696507   0.049617 -14.038  &amp;lt; 2e-16 ***
## factor(case)19  0.806729   0.049680  16.239  &amp;lt; 2e-16 ***
## factor(case)20 -0.764169   0.049643 -15.393  &amp;lt; 2e-16 ***
## factor(case)21 -0.597111   0.049591 -12.041  &amp;lt; 2e-16 ***
## factor(case)22 -1.405450   0.049928 -28.150  &amp;lt; 2e-16 ***
## factor(case)23  0.261895   0.049546   5.286 1.36e-07 ***
## factor(case)24  0.973181   0.049738  19.566  &amp;lt; 2e-16 ***
## factor(case)25 -0.710288   0.049618 -14.315  &amp;lt; 2e-16 ***
## factor(case)26 -0.718412   0.049631 -14.475  &amp;lt; 2e-16 ***
## factor(case)27 -0.946765   0.049698 -19.050  &amp;lt; 2e-16 ***
## factor(case)28 -3.194416   0.051604 -61.902  &amp;lt; 2e-16 ***
## factor(case)29  0.120342   0.049531   2.430 0.015184 *  
## factor(case)30 -0.965050   0.049712 -19.413  &amp;lt; 2e-16 ***
## factor(case)31 -1.108207   0.049794 -22.256  &amp;lt; 2e-16 ***
## factor(case)32 -0.888764   0.049668 -17.894  &amp;lt; 2e-16 ***
## factor(case)33 -0.920184   0.049700 -18.515  &amp;lt; 2e-16 ***
## factor(case)34  0.588541   0.049613  11.863  &amp;lt; 2e-16 ***
## factor(case)35 -0.008302   0.049525  -0.168 0.866881    
## factor(case)36 -1.068951   0.049757 -21.484  &amp;lt; 2e-16 ***
## factor(case)37 -2.534691   0.050831 -49.865  &amp;lt; 2e-16 ***
## factor(case)38 -0.699826   0.049616 -14.105  &amp;lt; 2e-16 ***
## factor(case)39  0.389961   0.049578   7.866 5.46e-15 ***
## factor(case)40 -1.291089   0.049878 -25.885  &amp;lt; 2e-16 ***
## factor(case)41 -2.527182   0.050822 -49.726  &amp;lt; 2e-16 ***
## factor(case)42 -1.718180   0.050056 -34.325  &amp;lt; 2e-16 ***
## factor(case)43  0.539154   0.049589  10.872  &amp;lt; 2e-16 ***
## factor(case)44  0.946498   0.049741  19.029  &amp;lt; 2e-16 ***
## factor(case)45 -1.672768   0.050093 -33.393  &amp;lt; 2e-16 ***
## factor(case)46 -1.924235   0.050325 -38.236  &amp;lt; 2e-16 ***
## factor(case)47  0.297708   0.049551   6.008 2.16e-09 ***
## factor(case)48  0.045939   0.049527   0.928 0.353731    
## factor(case)49  1.051133   0.049777  21.117  &amp;lt; 2e-16 ***
## factor(case)50 -1.102510   0.049764 -22.155  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2476 on 2449 degrees of freedom
## Multiple R-squared:  0.9173, Adjusted R-squared:  0.9156 
## F-statistic: 543.2 on 50 and 2449 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can see in the regression coefficient above that the coefficient for &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is almost exactly +1.&lt;/p&gt;
&lt;p&gt;Next we can fit a model with cases/intercepts for time points (cross-sectional variation):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y ~ x + factor(time),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(time), data = gen_data$data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9114 -0.1674 -0.0046  0.1699  0.7753 
## 
## Coefficients:
##                Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## (Intercept)    -0.68479    0.03530  -19.400  &amp;lt; 2e-16 ***
## x              -2.99280    0.01924 -155.534  &amp;lt; 2e-16 ***
## factor(time)2   1.51868    0.05005   30.345  &amp;lt; 2e-16 ***
## factor(time)3   2.26458    0.05073   44.637  &amp;lt; 2e-16 ***
## factor(time)4  -1.04731    0.04975  -21.050  &amp;lt; 2e-16 ***
## factor(time)5   1.73263    0.05025   34.482  &amp;lt; 2e-16 ***
## factor(time)6   1.26293    0.04988   25.319  &amp;lt; 2e-16 ***
## factor(time)7   0.06635    0.04952    1.340  0.18044    
## factor(time)8   1.67967    0.05018   33.472  &amp;lt; 2e-16 ***
## factor(time)9  -0.75507    0.04964  -15.211  &amp;lt; 2e-16 ***
## factor(time)10 -0.55308    0.04957  -11.157  &amp;lt; 2e-16 ***
## factor(time)11  1.34417    0.04996   26.905  &amp;lt; 2e-16 ***
## factor(time)12  2.20623    0.05064   43.565  &amp;lt; 2e-16 ***
## factor(time)13  0.02975    0.04952    0.601  0.54803    
## factor(time)14  0.10429    0.04952    2.106  0.03530 *  
## factor(time)15  0.27429    0.04955    5.536 3.43e-08 ***
## factor(time)16  1.05558    0.04981   21.193  &amp;lt; 2e-16 ***
## factor(time)17 -0.40503    0.04954   -8.175 4.68e-16 ***
## factor(time)18  0.86367    0.04971   17.375  &amp;lt; 2e-16 ***
## factor(time)19  1.57862    0.05013   31.492  &amp;lt; 2e-16 ***
## factor(time)20 -0.01728    0.04952   -0.349  0.72717    
## factor(time)21  0.62057    0.04961   12.508  &amp;lt; 2e-16 ***
## factor(time)22 -1.38478    0.04995  -27.721  &amp;lt; 2e-16 ***
## factor(time)23  0.31574    0.04954    6.374 2.20e-10 ***
## factor(time)24  2.96579    0.05157   57.515  &amp;lt; 2e-16 ***
## factor(time)25  0.24178    0.04953    4.881 1.12e-06 ***
## factor(time)26  2.83592    0.05144   55.133  &amp;lt; 2e-16 ***
## factor(time)27 -0.86284    0.04968  -17.369  &amp;lt; 2e-16 ***
## factor(time)28  0.92505    0.04975   18.594  &amp;lt; 2e-16 ***
## factor(time)29  1.05202    0.04980   21.125  &amp;lt; 2e-16 ***
## factor(time)30  0.15122    0.04953    3.053  0.00229 ** 
## factor(time)31  1.81587    0.05033   36.082  &amp;lt; 2e-16 ***
## factor(time)32  1.38787    0.04999   27.763  &amp;lt; 2e-16 ***
## factor(time)33  1.87706    0.05038   37.257  &amp;lt; 2e-16 ***
## factor(time)34  1.76956    0.05029   35.184  &amp;lt; 2e-16 ***
## factor(time)35  1.55739    0.05016   31.048  &amp;lt; 2e-16 ***
## factor(time)36  0.57794    0.04960   11.653  &amp;lt; 2e-16 ***
## factor(time)37  0.10647    0.04952    2.150  0.03166 *  
## factor(time)38 -0.72418    0.04962  -14.594  &amp;lt; 2e-16 ***
## factor(time)39  1.80861    0.05033   35.938  &amp;lt; 2e-16 ***
## factor(time)40  0.24163    0.04953    4.879 1.14e-06 ***
## factor(time)41  1.27448    0.04994   25.521  &amp;lt; 2e-16 ***
## factor(time)42  1.82843    0.05029   36.361  &amp;lt; 2e-16 ***
## factor(time)43  1.57770    0.05011   31.487  &amp;lt; 2e-16 ***
## factor(time)44  1.50850    0.05006   30.135  &amp;lt; 2e-16 ***
## factor(time)45  0.23318    0.04953    4.708 2.64e-06 ***
## factor(time)46 -0.92491    0.04969  -18.613  &amp;lt; 2e-16 ***
## factor(time)47  1.97346    0.05044   39.125  &amp;lt; 2e-16 ***
## factor(time)48  0.60207    0.04961   12.135  &amp;lt; 2e-16 ***
## factor(time)49  0.75674    0.04964   15.244  &amp;lt; 2e-16 ***
## factor(time)50 -1.38077    0.04996  -27.637  &amp;lt; 2e-16 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2476 on 2449 degrees of freedom
## Multiple R-squared:  0.9173, Adjusted R-squared:  0.9156 
## F-statistic: 543.4 on 50 and 2449 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, the estimated coefficient is almost exactly what we generated. Success!&lt;/p&gt;
&lt;p&gt;However, this brings us back to one of the questions we started with. We know the within-case relationship and the cross-sectional relationship, so what happens if we put dummies/intercepts on both cases and time? Well let’s find out:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(lm(y~x + factor(case) + factor(time),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.87158 -0.16705 -0.00396  0.16311  0.78026 
## 
## Coefficients: (1 not defined because of singularities)
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     0.806372   0.087811   9.183  &amp;lt; 2e-16 ***
## x               1.000661   0.143392   6.978 3.84e-12 ***
## factor(case)2  -0.184865   0.049834  -3.710 0.000212 ***
## factor(case)3   1.465951   0.074525  19.671  &amp;lt; 2e-16 ***
## factor(case)4  -1.363415   0.068454 -19.917  &amp;lt; 2e-16 ***
## factor(case)5  -0.884457   0.057091 -15.492  &amp;lt; 2e-16 ***
## factor(case)6   1.096983   0.064585  16.985  &amp;lt; 2e-16 ***
## factor(case)7  -0.991716   0.059427 -16.688  &amp;lt; 2e-16 ***
## factor(case)8  -1.597253   0.075111 -21.265  &amp;lt; 2e-16 ***
## factor(case)9  -2.125203   0.089352 -23.785  &amp;lt; 2e-16 ***
## factor(case)10 -0.028795   0.049592  -0.581 0.561545    
## factor(case)11 -1.170649   0.065430 -17.892  &amp;lt; 2e-16 ***
## factor(case)12 -0.132674   0.049635  -2.673 0.007568 ** 
## factor(case)13 -0.611905   0.053240 -11.493  &amp;lt; 2e-16 ***
## factor(case)14  0.865860   0.059440  14.567  &amp;lt; 2e-16 ***
## factor(case)15 -0.867514   0.057639 -15.051  &amp;lt; 2e-16 ***
## factor(case)16  0.514605   0.053863   9.554  &amp;lt; 2e-16 ***
## factor(case)17 -0.633567   0.055045 -11.510  &amp;lt; 2e-16 ***
## factor(case)18 -0.696223   0.054818 -12.701  &amp;lt; 2e-16 ***
## factor(case)19  0.806361   0.058129  13.872  &amp;lt; 2e-16 ***
## factor(case)20 -0.763847   0.056206 -13.590  &amp;lt; 2e-16 ***
## factor(case)21 -0.596870   0.053399 -11.178  &amp;lt; 2e-16 ***
## factor(case)22 -1.404855   0.069741 -20.144  &amp;lt; 2e-16 ***
## factor(case)23  0.261759   0.050838   5.149 2.83e-07 ***
## factor(case)24  0.972749   0.061053  15.933  &amp;lt; 2e-16 ***
## factor(case)25 -0.710003   0.054856 -12.943  &amp;lt; 2e-16 ***
## factor(case)26 -0.718107   0.055565 -12.924  &amp;lt; 2e-16 ***
## factor(case)27 -0.946375   0.059073 -16.020  &amp;lt; 2e-16 ***
## factor(case)28 -3.193052   0.122837 -25.994  &amp;lt; 2e-16 ***
## factor(case)29  0.120274   0.049902   2.410 0.016019 *  
## factor(case)30 -0.964644   0.059789 -16.134  &amp;lt; 2e-16 ***
## factor(case)31 -1.107721   0.063746 -17.377  &amp;lt; 2e-16 ***
## factor(case)32 -0.888409   0.057538 -15.440  &amp;lt; 2e-16 ***
## factor(case)33 -0.919792   0.059160 -15.548  &amp;lt; 2e-16 ***
## factor(case)34  0.588263   0.054612  10.772  &amp;lt; 2e-16 ***
## factor(case)35 -0.008290   0.049589  -0.167 0.867240    
## factor(case)36 -1.068500   0.061960 -17.245  &amp;lt; 2e-16 ***
## factor(case)37 -2.533613   0.101653 -24.924  &amp;lt; 2e-16 ***
## factor(case)38 -0.699544   0.054758 -12.775  &amp;lt; 2e-16 ***
## factor(case)39  0.389746   0.052653   7.402 1.84e-13 ***
## factor(case)40 -1.290532   0.067565 -19.101  &amp;lt; 2e-16 ***
## factor(case)41 -2.526108   0.101383 -24.916  &amp;lt; 2e-16 ***
## factor(case)42 -1.717495   0.075071 -22.878  &amp;lt; 2e-16 ***
## factor(case)43  0.538917   0.053290  10.113  &amp;lt; 2e-16 ***
## factor(case)44  0.946063   0.061202  15.458  &amp;lt; 2e-16 ***
## factor(case)45 -1.672061   0.076523 -21.850  &amp;lt; 2e-16 ***
## factor(case)46 -1.923394   0.085197 -22.576  &amp;lt; 2e-16 ***
## factor(case)47  0.297558   0.051103   5.823 6.56e-09 ***
## factor(case)48  0.045901   0.049675   0.924 0.355567    
## factor(case)49  1.050663   0.062931  16.696  &amp;lt; 2e-16 ***
## factor(case)50 -1.102052   0.062325 -17.682  &amp;lt; 2e-16 ***
## factor(time)2   0.009865   0.089885   0.110 0.912618    
## factor(time)3  -0.027398   0.115374  -0.237 0.812314    
## factor(time)4  -0.039227   0.044426  -0.883 0.377342    
## factor(time)5  -0.038629   0.098266  -0.393 0.694277    
## factor(time)6   0.015206   0.081771   0.186 0.852496    
## factor(time)7  -0.052421   0.051843  -1.011 0.312051    
## factor(time)8  -0.009808   0.095634  -0.103 0.918319    
## factor(time)9  -0.029774   0.042955  -0.693 0.488287    
## factor(time)10 -0.073201   0.043597  -1.679 0.093275 .  
## factor(time)11 -0.034386   0.085805  -0.401 0.688639    
## factor(time)12  0.003032   0.112421   0.027 0.978486    
## factor(time)13 -0.092023   0.051904  -1.773 0.076362 .  
## factor(time)14  0.017896   0.051201   0.350 0.726720    
## factor(time)15 -0.099884   0.057487  -1.738 0.082424 .  
## factor(time)16 -0.059046   0.077744  -0.759 0.447632    
## factor(time)17 -0.066464   0.044756  -1.485 0.137669    
## factor(time)18 -0.039199   0.071529  -0.548 0.583733    
## factor(time)19 -0.040240   0.093375  -0.431 0.666546    
## factor(time)20 -0.019978   0.049628  -0.403 0.687311    
## factor(time)21 -0.021785   0.064293  -0.339 0.734760    
## factor(time)22 -0.016429   0.049358  -0.333 0.739273    
## factor(time)23  0.016278   0.055739   0.292 0.770279    
## factor(time)24 -0.021054   0.138843  -0.152 0.879486    
## factor(time)25 -0.016068   0.054798  -0.293 0.769382    
## factor(time)26 -0.054303   0.135548  -0.401 0.688739    
## factor(time)27 -0.034151   0.043223  -0.790 0.429545    
## factor(time)28 -0.075377   0.074360  -1.014 0.310839    
## factor(time)29 -0.046340   0.077258  -0.600 0.548686    
## factor(time)30 -0.062577   0.053830  -1.162 0.245150    
## factor(time)31 -0.049767   0.101325  -0.491 0.623362    
## factor(time)32 -0.037785   0.087273  -0.433 0.665091    
## factor(time)33 -0.052008   0.103393  -0.503 0.614999    
## factor(time)34 -0.059567   0.100140  -0.595 0.552009    
## factor(time)35 -0.104873   0.094761  -1.107 0.268534    
## factor(time)36 -0.010170   0.062857  -0.162 0.871484    
## factor(time)37 -0.082164   0.053289  -1.542 0.123243    
## factor(time)38 -0.057402   0.042945  -1.337 0.181464    
## factor(time)39 -0.056943   0.101323  -0.562 0.574169    
## factor(time)40  0.007376   0.054276   0.136 0.891908    
## factor(time)41 -0.070988   0.084778  -0.837 0.402484    
## factor(time)42  0.010645   0.099772   0.107 0.915043    
## factor(time)43 -0.013565   0.092496  -0.147 0.883418    
## factor(time)44 -0.015229   0.090355  -0.169 0.866166    
## factor(time)45  0.010920   0.054013   0.202 0.839797    
## factor(time)46 -0.060932   0.043387  -1.404 0.160328    
## factor(time)47 -0.020632   0.105521  -0.196 0.844996    
## factor(time)48 -0.038896   0.064256  -0.605 0.545015    
## factor(time)49  0.028808   0.066612   0.432 0.665437    
## factor(time)50        NA         NA      NA       NA    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2479 on 2401 degrees of freedom
## Multiple R-squared:  0.9187, Adjusted R-squared:  0.9154 
## F-statistic:   277 on 98 and 2401 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You might expect that perhaps the two-way coefficient would be somewhere between the cross-sectional and within-case coefficients. Nope. It’s equal in this simulation to roughly 1, but that depends on how much variance there is in the cross-section and the within-case components of the dataset as they are now being combined in a non-linear fashion (if you can’t wrap your mind around this, don’t worry, it’s nearly impossible to do and unfruitful if you do accomplish it). However, the coefficient is still equal to a value that appears reasonable, although it is much less precise, and we might be willing to traipse along with it. However, you should pay attention to what happened in the model summary above–one of the time dummies (&lt;code&gt;factor(time)50&lt;/code&gt;) came back with a missing (&lt;code&gt;NA&lt;/code&gt;) coefficient! What is that about?&lt;/p&gt;
&lt;p&gt;I’m so glad you asked. As we mentioned earlier, R has the ability to check for multi-collinearity in the predictor variables to avoid getting errors when attempting to estimate linear regression. Multi-collinearity can arise for reasons that have nothing to do with fixed effects, such as accidentally including variables that are sums of each other, etc. It often happens in fixed effects regression models when people try to include variables that only vary in the cross-section when using case fixed effects (a symptom of the Cheeseburger Syndrome mentioned previously). In this instance, though, there is no reason to think that there is multi-collinearity as we &lt;em&gt;generated&lt;/em&gt; the data with two continuous variables (&lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;). The data are perfect with no missingness. So how could this happen?&lt;/p&gt;
&lt;p&gt;This discovery is one of the central contributions of our paper. Whenever panel data has a single effect for the cross-section and for within-case variation–as is the dominant way of thinking about panel data–the two-way fixed effects coefficient is statistically unidentified. That is, trying to estimate it is equivalent to saying &lt;code&gt;2 + 2 = 5&lt;/code&gt; or the earth is as round as a square. As we show in the paper, it algebraically reduces to dividing by zero. R magically saved the day by dropping a variable, but we can show what would happen if R had not intervened.&lt;/p&gt;
&lt;p&gt;In the code below I manually estimate a regression using the matrix inversion formula, &lt;span class=&#34;math inline&#34;&gt;\((X^TX)^{-1}X^Ty\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; in this case is all of our predictor variables, including the case/time dummies:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;X &amp;lt;- model.matrix(y~x + factor(case) + factor(time),data=gen_data$data)
y &amp;lt;- gen_data$data$y
try(solve(t(X)%*%X)%*%t(X)%*%y)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Error in solve.default(t(X) %*% X) : 
##   system is computationally singular: reciprocal condition number = 5.63975e-19&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can see the error message: the system (matrix computation of OLS regression) is computationally singular. You’ll need to see the paper for how this all breaks down, but essentially the question you are putting to R is nonsensical. You simply cannot estimate a single joint effect while including all the variables. Instead, you need to drop one, which essentially means the effect is relative to the whichever intercept happened to be dropped (in this case time point 50). The coefficient could change if we simply re-arrange the labeling of the time fixed effects, as in the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_data$data$time &amp;lt;- as.numeric(factor(gen_data$data$time,levels=sample(unique(gen_data$data$time))))
summary(lm(y~x + factor(case) + factor(time),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.87158 -0.16705 -0.00396  0.16311  0.78026 
## 
## Coefficients: (1 not defined because of singularities)
##                 Estimate Std. Error t value Pr(&amp;gt;|t|)    
## (Intercept)     0.827062   0.058816  14.062  &amp;lt; 2e-16 ***
## x               0.892381   0.169360   5.269 1.49e-07 ***
## factor(case)2  -0.181065   0.049934  -3.626 0.000294 ***
## factor(case)3   1.423934   0.082322  17.297  &amp;lt; 2e-16 ***
## factor(case)4  -1.327772   0.074606 -17.797  &amp;lt; 2e-16 ***
## factor(case)5  -0.863081   0.059798 -14.433  &amp;lt; 2e-16 ***
## factor(case)6   1.065727   0.069627  15.306  &amp;lt; 2e-16 ***
## factor(case)7  -0.966975   0.062893 -15.375  &amp;lt; 2e-16 ***
## factor(case)8  -1.554645   0.083062 -18.717  &amp;lt; 2e-16 ***
## factor(case)9  -2.069070   0.100829 -20.521  &amp;lt; 2e-16 ***
## factor(case)10 -0.029652   0.049597  -0.598 0.549997    
## factor(case)11 -1.138408   0.070718 -16.098  &amp;lt; 2e-16 ***
## factor(case)12 -0.130897   0.049657  -2.636 0.008442 ** 
## factor(case)13 -0.597254   0.054619 -10.935  &amp;lt; 2e-16 ***
## factor(case)14  0.841102   0.062910  13.370  &amp;lt; 2e-16 ***
## factor(case)15 -0.845316   0.060527 -13.966  &amp;lt; 2e-16 ***
## factor(case)16  0.498709   0.055464   8.992  &amp;lt; 2e-16 ***
## factor(case)17 -0.615509   0.057060 -10.787  &amp;lt; 2e-16 ***
## factor(case)18 -0.678563   0.056754 -11.956  &amp;lt; 2e-16 ***
## factor(case)19  0.783446   0.061177  12.806  &amp;lt; 2e-16 ***
## factor(case)20 -0.743853   0.058618 -12.690  &amp;lt; 2e-16 ***
## factor(case)21 -0.581894   0.054834 -10.612  &amp;lt; 2e-16 ***
## factor(case)22 -1.367817   0.076249 -17.939  &amp;lt; 2e-16 ***
## factor(case)23  0.253267   0.051327   4.934 8.59e-07 ***
## factor(case)24  0.945844   0.065030  14.545  &amp;lt; 2e-16 ***
## factor(case)25 -0.692276   0.056805 -12.187  &amp;lt; 2e-16 ***
## factor(case)26 -0.699163   0.057758 -12.105  &amp;lt; 2e-16 ***
## factor(case)27 -0.922121   0.062427 -14.771  &amp;lt; 2e-16 ***
## factor(case)28 -3.108184   0.141697 -21.936  &amp;lt; 2e-16 ***
## factor(case)29  0.115992   0.050029   2.318 0.020507 *  
## factor(case)30 -0.939410   0.063371 -14.824  &amp;lt; 2e-16 ***
## factor(case)31 -1.077463   0.068540 -15.720  &amp;lt; 2e-16 ***
## factor(case)32 -0.866360   0.060394 -14.345  &amp;lt; 2e-16 ***
## factor(case)33 -0.895419   0.062541 -14.317  &amp;lt; 2e-16 ***
## factor(case)34  0.570972   0.056476  10.110  &amp;lt; 2e-16 ***
## factor(case)35 -0.007546   0.049593  -0.152 0.879081    
## factor(case)36 -1.040438   0.066215 -15.713  &amp;lt; 2e-16 ***
## factor(case)37 -2.466600   0.115948 -21.273  &amp;lt; 2e-16 ***
## factor(case)38 -0.681990   0.056674 -12.034  &amp;lt; 2e-16 ***
## factor(case)39  0.376359   0.053819   6.993 3.47e-12 ***
## factor(case)40 -1.255870   0.073466 -17.095  &amp;lt; 2e-16 ***
## factor(case)41 -2.459329   0.115618 -21.271  &amp;lt; 2e-16 ***
## factor(case)42 -1.674928   0.083011 -20.177  &amp;lt; 2e-16 ***
## factor(case)43  0.524163   0.054686   9.585  &amp;lt; 2e-16 ***
## factor(case)44  0.918966   0.065225  14.089  &amp;lt; 2e-16 ***
## factor(case)45 -1.628044   0.084840 -19.190  &amp;lt; 2e-16 ***
## factor(case)46 -1.871074   0.095680 -19.556  &amp;lt; 2e-16 ***
## factor(case)47  0.288205   0.051692   5.575 2.75e-08 ***
## factor(case)48  0.043567   0.049713   0.876 0.380918    
## factor(case)49  1.021395   0.067481  15.136  &amp;lt; 2e-16 ***
## factor(case)50 -1.073533   0.066692 -16.097  &amp;lt; 2e-16 ***
## factor(time)2  -0.069009   0.100571  -0.686 0.492671    
## factor(time)3  -0.108184   0.073336  -1.475 0.140295    
## factor(time)4   0.002837   0.043759   0.065 0.948307    
## factor(time)5  -0.011388   0.082981  -0.137 0.890857    
## factor(time)6  -0.079838   0.110453  -0.723 0.469861    
## factor(time)7  -0.066293   0.048598  -1.364 0.172660    
## factor(time)8  -0.135578   0.123915  -1.094 0.274015    
## factor(time)9  -0.149671   0.096133  -1.557 0.119623    
## factor(time)10 -0.135415   0.137057  -0.988 0.323244    
## factor(time)11 -0.046205   0.101477  -0.455 0.648917    
## factor(time)12 -0.094439   0.062068  -1.522 0.128252    
## factor(time)13 -0.056940   0.061062  -0.933 0.351172    
## factor(time)14 -0.074651   0.076729  -0.973 0.330690    
## factor(time)15 -0.064301   0.086107  -0.747 0.455286    
## factor(time)16 -0.148654   0.105819  -1.405 0.160210    
## factor(time)17 -0.109373   0.139416  -0.785 0.432820    
## factor(time)18 -0.023932   0.052512  -0.456 0.648610    
## factor(time)19 -0.054156   0.088109  -0.615 0.538842    
## factor(time)20 -0.035535   0.098979  -0.359 0.719611    
## factor(time)21 -0.116553   0.143594  -0.812 0.417053    
## factor(time)22 -0.010896   0.065124  -0.167 0.867139    
## factor(time)23 -0.030352   0.055015  -0.552 0.581207    
## factor(time)24 -0.097372   0.166084  -0.586 0.557742    
## factor(time)25 -0.033847   0.056850  -0.595 0.551644    
## factor(time)26 -0.144292   0.145023  -0.995 0.319857    
## factor(time)27 -0.076492   0.070012  -1.093 0.274696    
## factor(time)28 -0.081450   0.086159  -0.945 0.344576    
## factor(time)29 -0.059062   0.059659  -0.990 0.322275    
## factor(time)30 -0.069904   0.049340  -1.417 0.156679    
## factor(time)31 -0.116713   0.102264  -1.141 0.253863    
## factor(time)32 -0.050535   0.050595  -0.999 0.317977    
## factor(time)33 -0.025185   0.043191  -0.583 0.559876    
## factor(time)34 -0.026497   0.046296  -0.572 0.567147    
## factor(time)35 -0.059114   0.048596  -1.216 0.223940    
## factor(time)36 -0.109133   0.105935  -1.030 0.303025    
## factor(time)37 -0.119734   0.053185  -2.251 0.024457 *  
## factor(time)38 -0.042987   0.101938  -0.422 0.673288    
## factor(time)39 -0.035869   0.047662  -0.753 0.451788    
## factor(time)40 -0.113464   0.165575  -0.685 0.493239    
## factor(time)41 -0.059933   0.110559  -0.542 0.587805    
## factor(time)42 -0.009158   0.057266  -0.160 0.872963    
## factor(time)43 -0.126494   0.150871  -0.838 0.401878    
## factor(time)44 -0.059636   0.047396  -1.258 0.208424    
## factor(time)45 -0.039694   0.107192  -0.370 0.711183    
## factor(time)46 -0.056278   0.054291  -1.037 0.300026    
## factor(time)47 -0.146145   0.129554  -1.128 0.259404    
## factor(time)48 -0.088757   0.069468  -1.278 0.201491    
## factor(time)49 -0.136982   0.103233  -1.327 0.184661    
## factor(time)50        NA         NA      NA       NA    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 0.2479 on 2401 degrees of freedom
## Multiple R-squared:  0.9187, Adjusted R-squared:  0.9154 
## F-statistic:   277 on 98 and 2401 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the coefficient changed because we arbitrarily swapped the order of the time fixed effects, and as R will drop the last one in the case of multi-collinearity, the estimate changed.&lt;/p&gt;
&lt;p&gt;At this point, &lt;strong&gt;you should be concerned.&lt;/strong&gt; One of our frustrations with this piece is that although the working paper has been in circulation for years, no one seems to have cared about this &lt;em&gt;apparently quite important problem.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;You might wonder though that you’ve seen two-way fixed effects models where this didn’t happen. As we show in the paper, the two-way FE model &lt;em&gt;is&lt;/em&gt; identified if we generate the data differently. What we have to do is use a different &lt;em&gt;effect&lt;/em&gt; of &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for each time point/case, or what can be called a &lt;em&gt;varying slopes&lt;/em&gt; model (slope for regression coefficient). We are not varying the fixed effects/intercepts themselves, rather we are varying the relationship between &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; across time points and cases. We must do this to prevent the two-way FE model from being unidentified.&lt;/p&gt;
&lt;p&gt;To demonstrate this, we will generate new data except the coefficients will vary across case and time points randomly:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# make the slopes vary with the .sd parameters
gen_data &amp;lt;- tw_data(N=20,
                    T=50,
                    case.eff.mean = 1,
                    cross.eff.mean = -1,
                    cross.eff.sd =.25,
                    case.eff.sd =1,
                    noise.sd=1)

summary(lm(y~x + factor(case) + factor(time),data=gen_data$data))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## lm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.5458 -0.7073  0.0026  0.6446  3.1709 
## 
## Coefficients:
##                 Estimate Std. Error  t value Pr(&amp;gt;|t|)    
## (Intercept)    -0.673576   0.264996   -2.542 0.011188 *  
## x              -0.812224   0.005523 -147.067  &amp;lt; 2e-16 ***
## factor(case)2   0.431969   0.201817    2.140 0.032582 *  
## factor(case)3   0.169660   0.201920    0.840 0.400994    
## factor(case)4   0.216513   0.201806    1.073 0.283604    
## factor(case)5   0.377437   0.201835    1.870 0.061794 .  
## factor(case)6   0.233856   0.201762    1.159 0.246726    
## factor(case)7   0.291743   0.202146    1.443 0.149292    
## factor(case)8   0.040054   0.201779    0.199 0.842696    
## factor(case)9   0.182815   0.201763    0.906 0.365123    
## factor(case)10  0.197116   0.201782    0.977 0.328886    
## factor(case)11  0.264956   0.201782    1.313 0.189480    
## factor(case)12  0.075685   0.201799    0.375 0.707708    
## factor(case)13  0.249261   0.201795    1.235 0.217062    
## factor(case)14  0.118726   0.201765    0.588 0.556382    
## factor(case)15  0.494571   0.201828    2.450 0.014451 *  
## factor(case)16 -0.085249   0.203634   -0.419 0.675579    
## factor(case)17  0.383878   0.201766    1.903 0.057403 .  
## factor(case)18  0.290350   0.201837    1.439 0.150619    
## factor(case)19  0.483656   0.201790    2.397 0.016734 *  
## factor(case)20  0.287368   0.201792    1.424 0.154759    
## factor(time)2   0.596679   0.319058    1.870 0.061780 .  
## factor(time)3   1.407443   0.319158    4.410 1.16e-05 ***
## factor(time)4  -0.328379   0.320361   -1.025 0.305616    
## factor(time)5  -0.119690   0.319047   -0.375 0.707634    
## factor(time)6   1.539948   0.319015    4.827 1.62e-06 ***
## factor(time)7   0.338443   0.319042    1.061 0.289052    
## factor(time)8   0.528072   0.319048    1.655 0.098232 .  
## factor(time)9  -0.032228   0.319030   -0.101 0.919556    
## factor(time)10  0.765270   0.319056    2.399 0.016657 *  
## factor(time)11  0.789097   0.319058    2.473 0.013568 *  
## factor(time)12  2.149822   0.319199    6.735 2.87e-11 ***
## factor(time)13  0.118301   0.319032    0.371 0.710862    
## factor(time)14 -0.734958   0.319015   -2.304 0.021452 *  
## factor(time)15  0.888259   0.319109    2.784 0.005486 ** 
## factor(time)16  0.553245   0.319027    1.734 0.083221 .  
## factor(time)17  0.652004   0.319030    2.044 0.041264 *  
## factor(time)18  1.351653   0.319104    4.236 2.50e-05 ***
## factor(time)19  0.260828   0.319061    0.817 0.413860    
## factor(time)20 -0.662932   0.319060   -2.078 0.038005 *  
## factor(time)21  1.509116   0.319095    4.729 2.60e-06 ***
## factor(time)22 -0.014879   0.319029   -0.047 0.962810    
## factor(time)23 -0.538377   0.319023   -1.688 0.091827 .  
## factor(time)24  0.493220   0.319046    1.546 0.122464    
## factor(time)25 -1.820174   0.319100   -5.704 1.57e-08 ***
## factor(time)26 -0.571426   0.319015   -1.791 0.073583 .  
## factor(time)27  0.093017   0.319026    0.292 0.770683    
## factor(time)28 -1.328269   0.319035   -4.163 3.43e-05 ***
## factor(time)29  0.342019   0.319038    1.072 0.283983    
## factor(time)30 -0.774338   0.319014   -2.427 0.015401 *  
## factor(time)31  0.461852   0.319026    1.448 0.148040    
## factor(time)32  0.308478   0.319046    0.967 0.333858    
## factor(time)33  1.343319   0.319094    4.210 2.80e-05 ***
## factor(time)34  1.989206   0.319258    6.231 7.03e-10 ***
## factor(time)35 -0.422407   0.319019   -1.324 0.185801    
## factor(time)36  0.614837   0.319085    1.927 0.054299 .  
## factor(time)37 -0.416240   0.319061   -1.305 0.192359    
## factor(time)38  0.779448   0.319089    2.443 0.014762 *  
## factor(time)39 -0.847622   0.319015   -2.657 0.008019 ** 
## factor(time)40  3.245566   0.319204   10.168  &amp;lt; 2e-16 ***
## factor(time)41 -0.589393   0.319031   -1.847 0.064999 .  
## factor(time)42  0.509922   0.319178    1.598 0.110469    
## factor(time)43 -0.815928   0.319029   -2.558 0.010699 *  
## factor(time)44  1.200743   0.320664    3.745 0.000192 ***
## factor(time)45 -0.677540   0.319015   -2.124 0.033946 *  
## factor(time)46  0.329598   0.319049    1.033 0.301842    
## factor(time)47  0.254068   0.319037    0.796 0.426027    
## factor(time)48  0.728453   0.319070    2.283 0.022652 *  
## factor(time)49  2.376336   0.319302    7.442 2.25e-13 ***
## factor(time)50  0.525069   0.319064    1.646 0.100172    
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## Residual standard error: 1.009 on 930 degrees of freedom
## Multiple R-squared:  0.9625, Adjusted R-squared:  0.9598 
## F-statistic: 346.3 on 69 and 930 DF,  p-value: &amp;lt; 2.2e-16&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Bada bing bada boom. Now there are no missing coefficients. However, the coefficient on &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; isn’t any easier to interpret, and in fact it is in even less transparent as it involves taking averages of averages of coefficients in the cross-section and within-case variation (say that five times fast). It is also peculiar that the model can only be fit with this kind of variation in the &lt;em&gt;slopes&lt;/em&gt;, as opposed to the intercepts, as might appear more logical. The models with fixed effects only for cases or time points do not have this problem at all.&lt;/p&gt;
&lt;p&gt;What makes this lack of identification particularly malicious is that it is virtually impossible to identify unless someone takes the trouble as we did to generate data from scratch. Visually, the dataset with single coefficients for cross-sectional/within-case variation appears pristine. It is only deep under the hood that the problem appears. The fact that R (and Stata has similar behavior) can magically make the problem go away by changing the data only makes it less likely that the problem will ever be identified. We simply do not know how many papers in the literature have been affected by this problem (or by similar situations where the regression slopes are almost fixed across cases or time points).&lt;/p&gt;
&lt;p&gt;There is one important caveat to this post. The two-way fixed effects model &lt;em&gt;can&lt;/em&gt; be a difference-in -differences model, but &lt;em&gt;only&lt;/em&gt; if there are exactly two (2) time points. It is not possible to run a two-way FE model with many time points and call it difference-in-difference as there are the same difficulties in interpretation. We discuss this more in the paper.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;summary&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Summary&lt;/h2&gt;
&lt;p&gt;In summary, fixed effects models are very useful for panel data as they help isolate dimensions of variation that matter for research questions. They are not magical tools for causal inference, but rather frameworks for understanding data. It is important to think about which dimension (cross-section or within-case) is more relevant, and then go with that dimension. All other concerns, including modeling spatial or time autocorrelation, omitted variables and endogeneity are all secondary to this first and most important point, which is what comparisons can be drawn from the data?&lt;/p&gt;
&lt;p&gt;One important area that this can be applied to is allowing people to fit more models with fixed effects on &lt;em&gt;time points&lt;/em&gt; rather than cases. There are many questions that can be best understood by comparing cases to each other rather than cases to themselves over time. Studying long-run institutions like electoral systems, for example, can only be understood in terms of cross-sectional variation. There is nothing unique or special about the within-case model.&lt;/p&gt;
&lt;p&gt;Some claim that within-case variation is better because cases have less heterogeneity than the cross-section. However, there is no way this can be true a priori. If minimizing heterogeneity is the aim, then it is important to consider the time frame and how units change over time. For example, if we have a very long time series, say 200 years, and we are comparing the U.S. and Canada, then we might believe that the comparison of the U.S. of 1800 to the Canada of 1800 (the cross-section) has less noise than a comparison of the U.S. of 1800 to the U.S. of 2020 (within-case variation). We need to think more about our data and what it represents rather than taking the short-cut of employing the same model everyone else uses.&lt;/p&gt;
&lt;p&gt;While we did not discuss random effects/hierarchical models in this post, the same principles apply even if “partial pooling” is used rather than “no pooling.” Intercepts are designed to draw comparisons, and however the intercepts are modeled, it is important to think about what they are saying about the data, and whether that statement makes any sense.&lt;/p&gt;
&lt;p&gt;So if you don’t want to eat that cheeseburger… we release you. Go enjoy your tofu-based meat alternative with relish.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A Proposed Model for Partial Identification of SARS-CoV2 Infection Rates Given Observed Tests and Cases</title>
      <link>http://www.robertkubinec.com/post/kubinec_model_draft/</link>
      <pubDate>Sat, 28 Mar 2020 15:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/kubinec_model_draft/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;For an up to date version of this model, please see our paper at &lt;a href=&#34;https://osf.io/preprints/socarxiv/jp4wk/&#34; class=&#34;uri&#34;&gt;https://osf.io/preprints/socarxiv/jp4wk/&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulating Conjoint Survey Experiments</title>
      <link>http://www.robertkubinec.com/post/conjoint_power_simulation/</link>
      <pubDate>Mon, 11 Feb 2019 00:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/conjoint_power_simulation/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/kePrint/kePrint.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/lightable/lightable.css&#34; rel=&#34;stylesheet&#34; /&gt;


&lt;div id=&#34;background&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Background&lt;/h2&gt;
&lt;p&gt;Conjoint survey experiments have become more popular in political science since the publication of &lt;a href=&#34;http://hdl.handle.net/1721.1/84064&#34;&gt;Hainmueller, Hopkins and Yamamoto (2014)&lt;/a&gt;. However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see &lt;a href=&#34;https://osf.io/preprints/socarxiv/mrfcu/&#34;&gt;SocArchiv Draft&lt;/a&gt;). I employ both traditional power measures and newer statistics from &lt;a href=&#34;http://journals.sagepub.com/doi/abs/10.1177/1745691614551642&#34;&gt;Gelman and Carlin (2014)&lt;/a&gt; reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).&lt;/p&gt;
&lt;p&gt;The original Rmarkdown and saved simulation files can be downloaded from the &lt;a href=&#34;https://github.com/saudiwin/saudiwin.github.io/tree/sources/content&#34;&gt;site’s Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The packages required to run this simulation are listed in the code block below:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Required packages
require(ggplot2)
require(dplyr)
require(tidyr)
require(multiwayvcov)
require(lmtest)
require(stringr)
require(kableExtra)

# package MASS also used but not loaded

# != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows.
# to install parallelsugar:
# install.packages(&amp;#39;devtools&amp;#39;)
# library(devtools)
# install_github(&amp;#39;nathanvan/parallelsugar&amp;#39;)

# If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used

if(.Platform$OS.type==&amp;#39;windows&amp;#39;) {
  parallelfunc &amp;lt;- parallelsugar::mclapply_socket
} else {
  parallelfunc &amp;lt;- parallel::mclapply
}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation-set-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation Set-up&lt;/h2&gt;
&lt;p&gt;The following parameters control the range of coefficients tested and the number of simulations. The survey experiment design employs vignettes in which appeals and the actors making appeals are allowed to vary between respondents. Any one vignette has one actor and one appeal. The probability of assignment is assumed to be a simple random fraction of the number of appeal-actor combinations (14). If &lt;code&gt;run_sim&lt;/code&gt; is set to &lt;code&gt;TRUE&lt;/code&gt;, the simulation is run, otherwise the simulation results are loaded from an RDS file and plotted. Running the simulation will take approximately 6 to 12 hours depending on the number of cores and speed of the CPU.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Actually run the simulation or just load the data and look at it?
run_sim &amp;lt;- FALSE
# Max number of respondents fixed at 2700
num_resp &amp;lt;- 2700
# Number of iterations (breaks in sample size)
num_breaks &amp;lt;- 300
# Number of simulations to run per iteration
n_sims &amp;lt;- 1000&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I then create a grid of all possible actor-appeal combinations as I am using simple randomization of profiles before presenting them to respondents. There are two vectors of treatments (actors and appeals) that each have 7 separate treatments for a total of 14 separate possible treatments.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Two treatment variables producing a cross-product of 7x7
treatments1 &amp;lt;- c(&amp;#39;military&amp;#39;,&amp;#39;MOI&amp;#39;,&amp;#39;president&amp;#39;,&amp;#39;MOJ&amp;#39;,&amp;#39;parliament&amp;#39;,&amp;#39;municipality&amp;#39;,&amp;#39;government&amp;#39;)
treatments2 &amp;lt;- c(&amp;#39;exprop.firm&amp;#39;,&amp;#39;exprop.income&amp;#39;,&amp;#39;permit.reg&amp;#39;,&amp;#39;contracts.supply&amp;#39;,&amp;#39;permit.export&amp;#39;,&amp;#39;permit.import&amp;#39;,&amp;#39;reforms&amp;#39;)
total_treat &amp;lt;- length(c(treatments1,treatments2))
grid_pair &amp;lt;- as.matrix(expand.grid(treatments1,treatments2))
print(head(grid_pair))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##      Var1           Var2         
## [1,] &amp;quot;military&amp;quot;     &amp;quot;exprop.firm&amp;quot;
## [2,] &amp;quot;MOI&amp;quot;          &amp;quot;exprop.firm&amp;quot;
## [3,] &amp;quot;president&amp;quot;    &amp;quot;exprop.firm&amp;quot;
## [4,] &amp;quot;MOJ&amp;quot;          &amp;quot;exprop.firm&amp;quot;
## [5,] &amp;quot;parliament&amp;quot;   &amp;quot;exprop.firm&amp;quot;
## [6,] &amp;quot;municipality&amp;quot; &amp;quot;exprop.firm&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Simulation&lt;/h2&gt;
&lt;p&gt;To simulate the data, I first sample 14 coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; (one for each treatment &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt;) from a normal distribution with mean zero and standard deviation one. I then randomly sample from two profile combinations for each of the &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; respondents in accordance with simple random sampling. Two profile combinations, for a total of four tasks &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, are selected to reflect the fact that paired vignettes will be shown to each respondent as in the study design. I also sample a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Z_I\)&lt;/span&gt; that is a random binomial vector with probability of 0.2 (thus 20% of respondents will fall into this cell). A treatment interaction effect &lt;span class=&#34;math inline&#34;&gt;\(\beta_z\)&lt;/span&gt; is sampled from a normal distribution with mean 0.5 and standard deviation of 0.3 to provide a sampling distribution for the true effect, instead of assuming that the true effect is a fixed population value. Adding a distribution for &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt; reflects additional uncertainty beyond standard sampling distribution uncertainty. In this case, it represents additional measurement error between the true concept and the indicators used in the survey design.&lt;/p&gt;
&lt;p&gt;I also post-stratify some estimates with a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_I\)&lt;/span&gt; from a binomial distribution of probability .5 that has a constant effect on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; of &lt;span class=&#34;math inline&#34;&gt;\(+1\)&lt;/span&gt; (representing a fixed effect).&lt;/p&gt;
&lt;p&gt;I then randomly sample a pair of outcomes, for a total of four tasks &lt;span class=&#34;math inline&#34;&gt;\(T\)&lt;/span&gt;, for &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; in the range of &lt;span class=&#34;math inline&#34;&gt;\([1,10]\)&lt;/span&gt; by drawing a number from a multivariate normal distribution. The mean &lt;span class=&#34;math inline&#34;&gt;\(\mu_{it}\)&lt;/span&gt; of this normal distribution is equal to a linear model with an intercept of 5, the 14 dummy variables for treatment indicators &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt; with associated coefficients &lt;span class=&#34;math inline&#34;&gt;\(\beta_j\)&lt;/span&gt;, the interaction &lt;span class=&#34;math inline&#34;&gt;\(\beta_z\)&lt;/span&gt; between the pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(X_{ij}\)&lt;/span&gt;, and a post-stratification covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_i\)&lt;/span&gt;. To simplify matters, &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; is not given its own constituent term as I am not interested in the unconditional effect of &lt;span class=&#34;math inline&#34;&gt;\(Z_i\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt;, only the effect of &lt;span class=&#34;math inline&#34;&gt;\(X_{ijt}\)&lt;/span&gt; on &lt;span class=&#34;math inline&#34;&gt;\(Y_{it}\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(Z_{i}\)&lt;/span&gt;. Finally, I draw correlated errors from a multivariate normal distribution with mean of zero and length of 4 (equal to the number of tasks per respondent) to produce a &lt;span class=&#34;math inline&#34;&gt;\(4 \times 4\)&lt;/span&gt; variance matrix &lt;span class=&#34;math inline&#34;&gt;\(\varSigma_i\)&lt;/span&gt; with a diagonal of 4 and intra-respondent covariation of 1 (correlation of 0.5).&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
X_{ITJ} &amp;amp;\sim  \mathrm{B} \Big( \frac{1}{J \times 2} \Big)\\
B_{J} &amp;amp;\sim  \mathrm{N}(0,2)\\
\beta_z &amp;amp;\sim \mathrm{N}(0.5,0.3)\\
Z_I &amp;amp;\sim  \mathrm{B}(0.2)\\
Q_I &amp;amp;\sim  \mathrm{B}(0.5)\\
\mu_{it} &amp;amp;=  5 + \sum_{j=1}^{J} \sum_{t=1}^{T} \beta_j * X_{itj} + \beta_z * X_{it1} *Z_i + Q_i\\
Y_{it} &amp;amp;\sim  \mathrm{N}(\mu_{it},\varSigma_i)
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This process will produce some numbers outside the &lt;span class=&#34;math inline&#34;&gt;\([1,10]\)&lt;/span&gt; range; however, it is better to leave these values in as explicit truncation will violate the assumptions of the underlying causal model.&lt;/p&gt;
&lt;p&gt;I run 1000 simulations for each of 300 sequential sample sizes ranging from 100 to 2700. I then take the mean significant effect and report that as the likely significant effect size for that sample size. I also record the ratio of draws for which the effect is significant (the power). However, given that the true effect is not fixed, I interpret power as the ability detect a true effect greater than zero. I record both unadjusted p-values and p-values adjusted using the &lt;code&gt;cluster.vcov&lt;/code&gt; function from the multiwayvcov package by clustering around respondent ID to reflect the pairing of vignettes. I also use separate results when post-stratifying on a pre-treatment covariate &lt;span class=&#34;math inline&#34;&gt;\(Q_I\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In addition, I included M-errors (error of absolute magnitude of significant coefficients) and S-errors (incorrect sign of significant coefficients). M-errors provide an estimate of publication bias given that the &lt;span class=&#34;math inline&#34;&gt;\(p=0.05\)&lt;/span&gt; threshold is a hard boundary and will necessarily result in smaller effects being reported as statistically insignificant when in fact they are greater than zero. S-errors help determine the probability that an estimated effect is the correct sign even if it is significant. S-errors are particularly problematic in small samples when sampling error can produce large negative deviations that may be statistically significant.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;if(run_sim==TRUE) {
  file.create(&amp;#39;output_log.txt&amp;#39;,showWarnings = FALSE)
    
    # Need to randomize over the simulations so that parallelization works correctly on windows
    
    sampled_seq &amp;lt;- sample(seq(100,num_resp,length.out = num_breaks))
    
  all_sims &amp;lt;- parallelfunc(sampled_seq,function(x) {
  
    out_probs &amp;lt;- 1:n_sims
    cat(paste0(&amp;quot;Now running simulation on data sample size &amp;quot;,x),file=&amp;#39;output_log.txt&amp;#39;,sep=&amp;#39;\n&amp;#39;,append=TRUE)
  
    out_data &amp;lt;- lapply(1:n_sims, function(j) {
      
      total_probs &amp;lt;- sapply(1:x,function(x) {
        treat_rows &amp;lt;- sample(1:nrow(grid_pair),4)
        treatments_indiv &amp;lt;- c(grid_pair[treat_rows,])
        return(treatments_indiv)
      })
      
      by_resp &amp;lt;- t(total_probs)
      by_resp &amp;lt;- as_data_frame(by_resp)
      names(by_resp) &amp;lt;- c(paste0(&amp;#39;actor.&amp;#39;,1:4,&amp;quot;_cluster&amp;quot;,c(1,1,2,2)),paste0(&amp;#39;gift.&amp;#39;,1:4,&amp;quot;_cluster&amp;quot;,c(1,1,2,2)))
      by_resp$respondent &amp;lt;- paste0(&amp;#39;Respondent_&amp;#39;,1:nrow(by_resp))
      by_resp &amp;lt;- gather(by_resp,attribute,indicator,-respondent) %&amp;gt;% separate(attribute,into=c(&amp;#39;attribute&amp;#39;,&amp;#39;cluster&amp;#39;),sep=&amp;#39;_&amp;#39;) %&amp;gt;% 
        separate(attribute,into=c(&amp;#39;attribute&amp;#39;,&amp;#39;task&amp;#39;)) %&amp;gt;% spread(attribute,indicator)
      
      
      # Assign true coefficients for treatments
  
      #Beta_js
      
      coefs &amp;lt;- data_frame(coef_val=rnorm(n=length(c(treatments1,treatments2)),mean=0,sd=1),
                          treat_label=c(treatments1,treatments2))
      
      # Create cluster covariance in the errors
      
      sigma_matrix &amp;lt;- matrix(2,nrow=4,ncol=4)
      diag(sigma_matrix) &amp;lt;- 4
  
      # Add on the outcome as a normal draw, treatment coefficients, interaction coefficient, group errors/interaction by respondent
      
      by_resp &amp;lt;- gather(by_resp,treatment,appeal_type,actor,gift) %&amp;gt;% 
        left_join(coefs,by=c(&amp;#39;appeal_type&amp;#39;=&amp;#39;treat_label&amp;#39;))
      
      # Record interaction coefficient (true estimate of interest)
      
      true_effect &amp;lt;- rnorm(n=1,mean=0.5,sd=0.3)
      
      by_resp &amp;lt;- select(by_resp,-treatment) %&amp;gt;% spread(appeal_type,coef_val) %&amp;gt;%  group_by(respondent) %&amp;gt;% mutate(error=MASS::mvrnorm(1,mu=rep(0,4),Sigma=sigma_matrix)) %&amp;gt;% ungroup
      
      # interaction coefficient only in function if military==TRUE
      
      by_resp &amp;lt;- mutate(by_resp,int_coef=true_effect*rbinom(n = n(),prob = 0.2,size=1),
                        int_coef=if_else(military!=0,int_coef,0))
      by_resp &amp;lt;- lapply(by_resp, function(x) {
        if(is.double(x)) {
          x[is.na(x)] &amp;lt;- 0
        }
        return(x)
      }) %&amp;gt;% as_data_frame
      
      # To make the outcome, need to turn the dataset long
      # However, we now need to drop the reference categories
      # Drop one dummy from actor/gift to prevent multicollinearity = reforms + government combination
      
      out_var &amp;lt;- gather(by_resp,var_name,var_value,-respondent,-task,-cluster) %&amp;gt;% 
         filter(!(var_name %in% c(&amp;#39;reforms&amp;#39;,&amp;#39;government&amp;#39;))) %&amp;gt;% 
        group_by(respondent,task) %&amp;gt;% summarize(outcome=sum(var_value)+5)
      
      combined_data &amp;lt;- left_join(out_var,by_resp,by=c(&amp;#39;respondent&amp;#39;,&amp;#39;task&amp;#39;))
      
      
      # Re-estimate with a blocking variable
      
  
      combined_data$Q &amp;lt;- c(rep(1,floor(nrow(combined_data)/2)),
                           rep(0,ceiling(nrow(combined_data)/2)))
      
      combined_data$outcome &amp;lt;- if_else(combined_data$Q==1,combined_data$outcome+1,
                                       combined_data$outcome)
      
      # # Create data predictor matrix and estimate coefficients from the simulated dataset
      # 
      to_lm &amp;lt;- ungroup(combined_data) %&amp;gt;% select(contracts.supply:reforms,int_coef,Q)
      to_lm &amp;lt;- mutate_all(to_lm,funs(if_else(.!=0,1,.))) %&amp;gt;% mutate(outcome=combined_data$outcome)
      
      #No post-stratification
      # I don&amp;#39;t estimate a constituent term for int_coef because it is assumed to be zero
      
      results &amp;lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +
                      parliament + permit.export + permit.import + permit.reg + president + 
                      int_coef:military,data=to_lm)
      
      results_clust &amp;lt;- cluster.vcov(results,cluster = combined_data$respondent)
      pvals_adj &amp;lt;- coeftest(results,vcov.=results_clust)[-1,4]&amp;lt;0.05
      pvals_orig &amp;lt;- coeftest(results)[-1,4]&amp;lt;0.05
      
      total_sig_orig &amp;lt;- mean(pvals_orig)
      total_sig_adj &amp;lt;- mean(pvals_adj)
      
      int_sig_orig &amp;lt;- pvals_orig[&amp;#39;military:int_coef&amp;#39;]
      int_sig_adj &amp;lt;- pvals_adj[&amp;#39;military:int_coef&amp;#39;]
      
      
      # Now run the poststratification model
      
      results_ps &amp;lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +
                      parliament + permit.export + permit.import + permit.reg + president + 
                      int_coef:military + Q,data=to_lm)
      
      results_clust &amp;lt;- cluster.vcov(results,cluster = combined_data$respondent)
      pvals_adj &amp;lt;- coeftest(results_ps,vcov.=results_clust)[-1,4]&amp;lt;0.05
      pvals_orig &amp;lt;- coeftest(results_ps)[-1,4]&amp;lt;0.05
      
      total_sig_orig_blocker &amp;lt;- mean(pvals_orig)
      total_sig_adj_blocker &amp;lt;- mean(pvals_adj)
      
      int_sig_orig_blocker &amp;lt;- pvals_orig[&amp;#39;military:int_coef&amp;#39;]
      int_sig_adj_blocker &amp;lt;- pvals_adj[&amp;#39;military:int_coef&amp;#39;]
      
      out_results &amp;lt;- data_frame(int_sig_adj,int_sig_orig,int_sig_adj_blocker,int_sig_orig_blocker,
                                total_sig_adj,total_sig_orig,total_sig_adj_blocker,
                                total_sig_orig_blocker,abs_true_effect=abs(true_effect),
                                true_effect=true_effect,
                                est_effect=coef(results)[&amp;#39;military:int_coef&amp;#39;],
                                est_effect_ps=coef(results)[&amp;#39;military:int_coef&amp;#39;])
    })
    out_data &amp;lt;- bind_rows(out_data)
    
    return(out_data)
  },mc.cores=parallel::detectCores(),mc.preschedule=FALSE)
  #save the data for inspection
  
  all_sims_data &amp;lt;- bind_rows(all_sims) %&amp;gt;% mutate(sample_size=rep(sampled_seq,each=n_sims),
                                                  iter=rep(1:n_sims,times=num_breaks))

}

if(run_sim==TRUE) {
saveRDS(object = all_sims_data,file=&amp;#39;all_sims_data.rds&amp;#39;)
} else {
  all_sims_data &amp;lt;- readRDS(&amp;#39;all_sims_data.rds&amp;#39;)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This simulation yields a row with the significant effect of the interaction term for that simulation for a total of &lt;code&gt;n_sims&lt;/code&gt; draws. From this raw data I am able to calculate all of the necessary statistics mentioned above.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# add in different calculations

all_sims_data &amp;lt;- group_by(all_sims_data,sample_size)  %&amp;gt;% mutate(sigeffVorig=ifelse(int_sig_orig,
                                                                                     est_effect,
                                                                                     NA),
sigeffVadj=ifelse(int_sig_adj,est_effect,NA),
sigeffVps_orig=ifelse(int_sig_orig_blocker,est_effect_ps,NA),
sigeffVps_adj=ifelse(int_sig_adj_blocker,est_effect_ps,NA),
powerVorig=int_sig_orig &amp;amp; (true_effect&amp;gt;0),
powerVadj=int_sig_adj &amp;amp; (true_effect&amp;gt;0),
powerVps_orig=int_sig_orig_blocker &amp;amp; (true_effect &amp;gt; 0),
powerVps_adj=int_sig_adj_blocker &amp;amp; (true_effect &amp;gt; 0),
SerrVorig=ifelse(int_sig_orig,1-(sign(est_effect)==sign(true_effect)),NA),
SerrVadj=ifelse(int_sig_adj,1-(sign(est_effect)==sign(true_effect)),NA),
SerrVps_orig=ifelse(int_sig_orig_blocker,
                    1-(sign(est_effect_ps)==sign(true_effect)),NA),
SerrVps_adj=ifelse(int_sig_adj_blocker,
                   1-(sign(est_effect_ps)==sign(true_effect)),NA),
MerrVorig=ifelse(int_sig_orig,abs(est_effect)/abs_true_effect,NA),
MerrVadj=ifelse(int_sig_adj,abs(est_effect)/abs_true_effect,NA),
MerrVps_orig=ifelse(int_sig_orig_blocker,abs(est_effect_ps)/abs_true_effect,NA),
MerrVps_adj=ifelse(int_sig_adj_blocker,abs(est_effect_ps)/abs_true_effect,NA))

long_data &amp;lt;- select(all_sims_data,matches(&amp;#39;V|sample|iter&amp;#39;)) %&amp;gt;% gather(effect_type,result,-sample_size,-iter) %&amp;gt;% separate(effect_type,into=c(&amp;#39;estimate&amp;#39;,&amp;#39;estimation&amp;#39;),sep=&amp;#39;V&amp;#39;) %&amp;gt;% 
  mutate(estimate=factor(estimate,levels=c(&amp;#39;sigeff&amp;#39;,&amp;#39;power&amp;#39;,&amp;#39;Serr&amp;#39;,&amp;#39;Merr&amp;#39;),
                         labels=c(&amp;#39;Mean\nSignificant\nEffect&amp;#39;,
                                  &amp;#39;Mean\nPower&amp;#39;,
                                  &amp;#39;S-Error\nRate&amp;#39;,
                                  &amp;#39;M-Error\nRate&amp;#39;)),
         estimation=factor(estimation,levels=c(&amp;#39;adj&amp;#39;,&amp;#39;orig&amp;#39;,&amp;#39;ps_adj&amp;#39;,&amp;#39;ps_orig&amp;#39;),
                           labels=c(&amp;#39;No Post-Stratification\nClustered Errors\n&amp;#39;,
                                    &amp;#39;No Post-Stratification\nUn-clustered Errors\n&amp;#39;,
                                    &amp;#39;Post-Stratification\nClustered Errors\n&amp;#39;,
                                    &amp;#39;Post-Stratification\nUn-clustered Errors\n&amp;#39;)))

long_data_treatment &amp;lt;- select(all_sims_data,matches(&amp;#39;total|iter|sample&amp;#39;)) %&amp;gt;% gather(effect_type,result,-sample_size,-iter) %&amp;gt;%
mutate(effect_type=factor(effect_type,levels=c(&amp;#39;total_sig_adj&amp;#39;,
                                               &amp;#39;total_sig_orig&amp;#39;,
                                               &amp;#39;total_sig_adj_blocker&amp;#39;,
                                               &amp;#39;total_sig_orig_blocker&amp;#39;),
                          labels=c(&amp;#39;No Post-Stratification\nClustered Errors\n&amp;#39;,
                                   &amp;#39;No Post-Stratification\nUn-clustered Errors\n&amp;#39;,
                                   &amp;#39;Post-Stratification\nClustered Errors\n&amp;#39;,
                                   &amp;#39;Post-Stratification\nUn-clustered Errors\n&amp;#39;)))



# Plot a sample of the data (too big to display all of it)

long_data %&amp;gt;% ungroup %&amp;gt;% 
  slice(1:10) %&amp;gt;% 
  select(-estimation) %&amp;gt;% 
  mutate(estimate=str_replace(estimate,&amp;quot;\\n&amp;quot;,&amp;quot; &amp;quot;)) %&amp;gt;% 
  knitr::kable(.) %&amp;gt;% 
  kable_styling(font_size = 8)&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;font-size: 8px; margin-left: auto; margin-right: auto;&#34;&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
sample_size
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
iter
&lt;/th&gt;
&lt;th style=&#34;text-align:left;&#34;&gt;
estimate
&lt;/th&gt;
&lt;th style=&#34;text-align:right;&#34;&gt;
result
&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.6298429
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
2
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.3874088
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
3
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
4
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.1438379
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
5
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
0.5653086
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
6
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1.2689594
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
7
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
8
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
9
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
1334.783
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
10
&lt;/td&gt;
&lt;td style=&#34;text-align:left;&#34;&gt;
Mean Significant
Effect
&lt;/td&gt;
&lt;td style=&#34;text-align:right;&#34;&gt;
NA
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;plotting&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Plotting&lt;/h2&gt;
&lt;p&gt;I use the &lt;code&gt;gam&lt;/code&gt; function in the ggplot2 package to plot a smoothed regression line of the simulation draws for each sample size.&lt;/p&gt;
&lt;p&gt;First we can look at the difference that clustered errors makes across the different statistics. The only noticeable differences are at sample sizes smaller than 500. Clustering on respondents tends to result in smaller average significant effects, but it also results in increases in sign errors. This finding differs from the literature that considers clustering important to control for intra-respondent correlation, which in this simulation was fixed at 0.5. At sample sizes larger than 500, there does not appear to be any difference between clustered and un-clustered estimates.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,grepl(&amp;#39;No Post&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;clust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next I look at post-stratification as an option to improve the precision of estimates. For unclustered errors reported below, post-stratified estimates do have higher power and slightly lower average significant effects, and importantly, the post-stratified estimates worsen neither type S nor type M errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,grepl(&amp;#39;Un-clustered&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;post_unclust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Post-stratification appears to have a similar effect on clustered error estimations, although the differences are smaller. In smaller samples, post-stratified estimates do have smaller M-errors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
filter(long_data,!grepl(&amp;#39;Un-clustered&amp;#39;,estimation)) %&amp;gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&amp;#39;red&amp;#39;) +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
  facet_wrap(~estimate,scales=&amp;#39;free&amp;#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Accent&amp;#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Conjoint_Power_Simulation_files/figure-html/unclustered_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;post_clust_err.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally, I also report average numbers of significant coefficients for the 14 treatments. Given that the 14 treatments were sampled from a normal distribution with prior density in the positive values with a meaan of 0.5, in expectation 95% of estimates should be statisticall significant. While that upper limit is reached only in high sample numbers, it looks like the ratio for treatment effects reaches an acceptable level of 70 percent at about 500 sample respondents. Also, post-stratifying un-clustered models results in effects that are reported as significant at much higher rates, as would follow from the previous results about post-stratification.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;g_title &amp;lt;- guide_legend(title=&amp;#39;&amp;#39;)
ggplot(long_data_treatment,aes(y=result,x=sample_size,linetype=effect_type,colour=effect_type)) +
  theme_minimal() + stat_smooth() +
  xlab(&amp;#39;Sample Size&amp;#39;) + ylab(&amp;quot;&amp;quot;) +
theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&amp;#39;Dark2&amp;#39;) + guides(linetype=g_title,colour=g_title) +
  theme(legend.position = &amp;#39;bottom&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;#39;all_treat_rate.png&amp;#39;,units=&amp;#39;in&amp;#39;,width=6)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This simulation study shows that a sample size of approximately 1,000 respondents is enough to obtain high power while also lowering both the S and M-error rates for treatment interaction effects in this conjoint experiment. The treatment effects themselves are generally of high quality once the sample size reaches 500 because the total number of respondents in each treatment cell is considerably higher than in an interaction. Post-stratification appears to be a useful strategy to increase precision without inducing S or M errors; at the very least, post-stratification does not appear to have any adverse effects on the estimation.&lt;/p&gt;
&lt;p&gt;On the other hand, it appears that clustering errors increases the S-error rate at small sample sizes, a surprising finding considering that clustering methods are designed to inflate, not deflate, standard errors. Given that the S-error rate reveals the likelihood of making an error about the sign of the treatment effect, this is a potentially serious problem. For that reason I intend to report both clustered and un-clustered estimates in my analysis.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>idealstan: an R Package for Ideal Point Modeling with Stan</title>
      <link>http://www.robertkubinec.com/post/stancon2018_paper/</link>
      <pubDate>Wed, 14 Mar 2018 00:00:00 +0000</pubDate>
      <guid>http://www.robertkubinec.com/post/stancon2018_paper/</guid>
      <description>
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/htmlwidgets/htmlwidgets.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/jquery/jquery.min.js&#34;&gt;&lt;/script&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/datatables-binding/datatables.js&#34;&gt;&lt;/script&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/dt-core/css/jquery.dataTables.min.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;link href=&#34;http://www.robertkubinec.com/rmarkdown-libs/dt-core/css/jquery.dataTables.extra.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;http://www.robertkubinec.com/rmarkdown-libs/dt-core/js/jquery.dataTables.min.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;This is a paper that was presented at the &lt;a href=&#34;http://mc-stan.org/events/stancon2018/&#34;&gt;StanCon2018 conference&lt;/a&gt; on Bayesian inference with the Stan Hamiltonian Markov Chain Monte Carlo (MCMC) method. Video of my talk is available &lt;a href=&#34;https://www.youtube.com/watch?v=0ZjrLOosXwk&amp;amp;t=4s&#34;&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This notebook introduces &lt;a href=&#34;https://cran.r-project.org/package=idealstan&#34;&gt;&lt;code&gt;idealstan&lt;/code&gt;&lt;/a&gt;, a new R package front-end to &lt;a href=&#34;http://www.mc-stan.org&#34;&gt;&lt;code&gt;Stan&lt;/code&gt;&lt;/a&gt; that allows for flexible modeling of a class of latent variable models known as ideal point models. Ideal point modeling is a form of dimension reduction, and shares similarities with multi-dimensional scaling, factor analysis and item-response theory. While the parameterization employed by &lt;code&gt;idealstan&lt;/code&gt; is a derivation based on item-response theory, it is important to note that several other parameterizations are possible &lt;a name=cite-carroll2013&gt;&lt;/a&gt;&lt;a name=cite-armstrong2014&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-carroll2013&#34;&gt;Carroll, Lewis, Lo, Poole, and Rosenthal, 2013&lt;/a&gt;; &lt;a href=&#34;#bib-armstrong2014&#34;&gt;Armstrong, Bakker, Carroll, Hare, Poole, and Rosenthal, 2014&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;What distinguishes ideal point modeling from other latent space models is that the latent factor is bi-directional: in other words, the unobserved latent construct that underlies the data could increase or decrease as the observed stimuli either increase or decrease. This type of latent variable is very useful when the aim is to cluster units around a polarizing dimension, such as the left-right axis in politics or the &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-value debate in statistics. While many of the applications of ideal point models are in political science and economics, it certainly can be applied more broadly, especially as the approach is related to latent space models more generally, such as in &lt;a name=cite-hoff2002&gt;&lt;/a&gt;&lt;a href=&#34;#bib-hoff2002&#34;&gt;Hoff, Raftery, and Hancock (2002)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are well-established frequentist (&lt;a href=&#34;https://cran.r-project.org/web/packages/emIRT/index.html&#34;&gt;&lt;code&gt;emIRT&lt;/code&gt;&lt;/a&gt;) and Bayesian (&lt;a href=&#34;https://cran.r-project.org/web/packages/pscl/pscl.pdf&#34;&gt;&lt;code&gt;pscl&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/MCMCpack/index.html&#34;&gt;&lt;code&gt;MCMCpack&lt;/code&gt;&lt;/a&gt;) packages for ideal point inference with R. However, these packages offer standard models that are limited in scope to particular problems, especially when it comes to including predictors in the latent space. The R package &lt;code&gt;idealstan&lt;/code&gt; has three features that set it apart from existing approaches:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Stan offers significant flexibility compared to existing approaches in using non-conjugate priors and a clearer programming interface. This flexibility allows for more options for end users in ideal point modeling that can be used to test new theories and hypotheses instead of being limited in the range and type of models available. In particular, Stan makes it easier to add hierarchical and time-series priors on to parameters, which opens the door to further analysis of dynamic and time-varying social phenomena.&lt;/li&gt;
&lt;li&gt;Stan scales well with the number of parameters, which is an attribute of most dimension reduction methods. Advances in Hamiltonian Monte Carlo estimation, including the use of variational inference and soon parallel computing within chains, promise to make full Bayesian inference of these models practical even with very large datasets.&lt;/li&gt;
&lt;li&gt;Increasingly, Stan is being married to helpful and powerful diagnostic packages, including &lt;a href=&#34;https://cran.r-project.org/web/packages/bayesplot/index.html&#34;&gt;&lt;code&gt;bayesplot&lt;/code&gt;&lt;/a&gt;, &lt;a href=&#34;https://cran.r-project.org/web/packages/shinystan/index.html&#34;&gt;&lt;code&gt;shinystan&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://cran.r-project.org/web/packages/loo/index.html&#34;&gt;&lt;code&gt;loo&lt;/code&gt;&lt;/a&gt;, which extends the ability of &lt;code&gt;idealstan&lt;/code&gt; to provide not only cutting-edge Bayesian inference but also increasingly sophisticated tools for graphical analysis of resulting estimates. Given the complex nature of latent variable models, diagonistics are extremely important for determining when the model is behaving as it should (and what the model should be doing in the first place).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/saudiwin/idealstan&#34;&gt;&lt;code&gt;idealstan&lt;/code&gt;&lt;/a&gt; is an effort to build on prior efforts but also to offer new models that satisfy the increasing variety of applications to which ideal point models are being put. Both &lt;code&gt;pscl&lt;/code&gt; and &lt;code&gt;MCMCpack&lt;/code&gt; were designed for ideal point modeling of binary (logit) data from legislatures in which the outcome is made up of yes and no votes cast by legislators &lt;a name=cite-jackman2004&gt;&lt;/a&gt;&lt;a name=cite-quinn2002&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers, 2004&lt;/a&gt;; &lt;a href=&#34;#bib-quinn2002&#34;&gt;Martin and Quinn, 2002&lt;/a&gt;). More recently scholars have begun applying ideal point models to Twitter data &lt;a name=cite-barbera2015&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-barbera2015&#34;&gt;Barberá, 2015&lt;/a&gt;), massive campaign finance datasets &lt;a name=cite-bonica2014&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-bonica2014&#34;&gt;Bonica, 2014&lt;/a&gt;) and party campaign manifestos &lt;a name=cite-slapin2008&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-slapin2008&#34;&gt;Slapin and Proksch, 2008&lt;/a&gt;). This package offers both the traditional forms of ideal point models along with new extensions, including a version of ideal points that can take into account certain forms of missing data.&lt;/p&gt;
&lt;p&gt;In this notebook I first introduce ideal-point models and contrast them with “traditional” item response theory (IRT), and then I demonstrate the &lt;code&gt;idealstan&lt;/code&gt; package through simulations. I then perform two empirical analyses, the first of coffee product ratings from Amazon and the second with voting data taken from the 114th Senate. In the second example, I also show how &lt;code&gt;idealstan&lt;/code&gt; enables new estimation of strategic legislator absence, a type of data that is usually coded as missing in vote datasets.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ideal-point-models-as-a-subset-of-statistical-measurement&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Ideal Point Models as a Subset of Statistical Measurement&lt;/h2&gt;
&lt;p&gt;Measurement error models have a long history in applied statistics and are increasingly in demand as the amount of noisy observational data grows with the digital revolution. Canonical statistical models, particularly linear regression, assume that the predictors are measured without error, but in many situations in social science, the variable of interest cannot in fact be measured. Rather, proxies or indicators are used to stand in for the latent construct. Measurement models offer a way to match the indicators with the latent construct while making appropriate assumptions about the relationship.&lt;/p&gt;
&lt;p&gt;In other words, we suppose that the regression predictor matrix &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; is itself a function of certain indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c \in \{1 ... C\}\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ 
X = f(\forall I\_c \in \{1 ... C\})
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Different latent variable, latent space and measurement models can be distinguished in terms of the function &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt;. Fundamentally, &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; has to stipulate how &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; will change as the indicators change. It is possible to make very precise conditions for this relationship, which is the method applied in confirmatory factor analysis and structural equation modeling. It is also possible to let &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; make only minimal assumptions about this relationship, which is the method adopted by exploratory factor analysis and item-response theory (IRT), in addition to the other latent space models in the literature. It is also possible, of course, to have models that fall somewhere in between truly exploratory and truly confirmatory models.&lt;/p&gt;
&lt;p&gt;While much has been written about the different kids of &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; that can be used in measurement models, in this paper I focus on a metric relevant to ideal point models: as the values of &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt; increase, does &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; also increase so that &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; must be always non-decreasing? If that is the case, then the model can be thought of as falling into the domain of item-response theory and traditional factor analysis (in fact, IRT is itself a non-linear version of factor analysis per &lt;a name=cite-takane1986&gt;&lt;/a&gt;&lt;a href=&#34;#bib-takane1986&#34;&gt;Takane and de Leeuw (1986)&lt;/a&gt;). By contrast, ideal point models are based on a different set of assumptions in which &lt;span class=&#34;math inline&#34;&gt;\(X\)&lt;/span&gt; could increase or decrease as the indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt; increase or decrease; in other words, the relationship is bi-polar instead of uni-polar.&lt;/p&gt;
&lt;p&gt;Ideal point modeling originated from analysis of a common situation in policy research in which different legislators select from among competing policy alternatives based on the utility offered by each policy &lt;a name=cite-enelow198&gt;&lt;/a&gt;&lt;a name=cite-poole2008&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-enelow198&#34;&gt;Enelow and Hinich, 1984&lt;/a&gt;; &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&#34;&gt;Poole, Lewis, Lo, and Carroll, 2008&lt;/a&gt;). Supposing that the policies are evaluated on a uni-dimensional space, if the utility of the “Yes” position on the policy is greater than the utility of the “No” position to a particular legislator, then that legislator will choose that policy. While very simple, this model has a requirement that is different from much of traditional IRT estimation: the policy outcomes can have different meanings based on their position relative to the legislator in the policy space. For example, if a legislator is very liberal (has a high value on the latent construct), then that legislator is likely to vote yes on bills that are also liberal, such as single-payer health care. But if the bill is conservative, such as national defense, then that legislator is more likely to vote no. In other words, the meaning of the positions of the bills in the latent space depends on the ideal point of the legislator, and thus the mapping to the latent space &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; cannot be always non-decreasing. For some votes, a yes position may mean a high value of the latent space, while for others it may mean a lower value.&lt;/p&gt;
&lt;p&gt;By comparison, in traditional IRT, responses to stimuli (the indicators &lt;span class=&#34;math inline&#34;&gt;\(I\_c\)&lt;/span&gt;) reflect correct or incorrect answers, such as a student taking a test. Correct answers, which in the legislative context mean yes votes, always equal greater ability, while incorrect answers, which would be no votes, are always a sign of less ability. In this situation, &lt;span class=&#34;math inline&#34;&gt;\(f(\cdot)\)&lt;/span&gt; is always non-decreasing. Thus while ideal point models share much in common with IRT, they require different assumptions.&lt;/p&gt;
&lt;p&gt;A brief review of the mathematical notation will make the difference clear. The 2-PL IRT model takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
Y\_{ij} = \alpha\_j x\_i - \beta\_j
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; can be an outcome of any type (binary, ordinal, Poisson, etc.) that includes the correct or incorrect responses of &lt;span class=&#34;math inline&#34;&gt;\(I\)&lt;/span&gt; test takers to &lt;span class=&#34;math inline&#34;&gt;\(J\)&lt;/span&gt; test items, &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; are the discrimination parameters that control the narrowness, hence discrimination, of each item &lt;span class=&#34;math inline&#34;&gt;\(j\)&lt;/span&gt; on the test in the latent space, &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; are the test-taker ability parameters that reflect the ability of a person to answer an item correctly, and &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt; are the difficulty or average probability/value of a correct response (the intercept). In the traditional IRT format, the &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; discrimination parameters are always positive because a higher value (a correct answer) is always associated with higher ability &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;, while a lower value (an incorrect answer) is associated with less ability.&lt;/p&gt;
&lt;p&gt;However, if the requirement that the discrimination parameters &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt; are positive is removed, then this model can also be used for ideal point modeling. For most applications, ideal-point modeling using IRT is built on the standard 2-PL model &lt;a name=cite-gelman2005&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers, 2004&lt;/a&gt;; &lt;a href=&#34;#bib-gelman2005&#34;&gt;Bafumi, Gelman, Park, and Kaplan, 2005&lt;/a&gt;), with one notable exception: the discrimination parameters must be un-constrained, while in traditional IRT the discrimination parameters are constrained to all be either positive or negative. Constraining discrimination parameters to be positive is also the approach taken in the &lt;a href=&#34;https://cran.r-project.org/web/packages/edstan/vignettes/briefmanual.html&#34;&gt;&lt;code&gt;edstan&lt;/code&gt;&lt;/a&gt; package, which offers the full range of standard IRT models using Stan.&lt;/p&gt;
&lt;p&gt;Leaving the discrimination parameters un-consrained ensures that a higher or lower value of &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; could be associated with either a “correct” or “incorrect” answer as is necessary in an ideal point model. This happens because the latent space in an ideal point model is fundamentally a Euclidean distance that is rotation-invariant. The following figure makes this clear by showing a version of an ideal point model in which the ideal point (ability parameter) &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; is represented by a Normal distribution while the Yes and No points of a proposed policy are vertical lines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;If &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; is to the right of the Indifference line, the legislator will vote Yes on the proposal, whereas if she is to the left, she will vote No. However, a different policy (item) could have the Yes and No positions switched so that a Yes vote would correspond to -2.5 and vice versa for the No vote. Thus the binary outcomes do not have the same interpretation as the standard IRT model because the relationship between the outcomes and the ability points is contingent.&lt;/p&gt;
&lt;p&gt;If the discrimination parameters are unconstrained, &lt;a href=&#34;#bib-jackman2004&#34;&gt;Clinton, Jackman, and Rivers (2004)&lt;/a&gt; showed that the midpoint of the policy/item parameter, which is labeled on the plot as the indifference line, is identified as &lt;span class=&#34;math inline&#34;&gt;\(-\frac{\alpha\_j}{\beta\_j}\)&lt;/span&gt; in the original IRT model. This midpoint is important because it represents the point in the latent space in which an actor is indifferent, or has a 50% chance of voting yes or no. Unfortunately, the IRT paramerization of the ideal point model does not return the actual Yes or No positions of a bill/item, but these midpoints are sufficient to characterize the position of the items in the latent space.&lt;/p&gt;
&lt;p&gt;While this difference between ideal point models and standard IRT may seem trivial, it has serious consequences for modeling. The basic issue is the non-identifiability of the IRT model, which the switching polarity of the discrimination parameters only makes more difficult. Ultimately, it becomes necessary to constrain the polarity of some of the ability or discrimination parameters to identify the model. However, it is necessary to choose parameters which would avoid “splitting the likelihood”, such as constraining a legislator whose true ideal point is near zero (&lt;a href=&#34;#bib-gelman2005&#34;&gt;Bafumi, Gelman, Park, et al., 2005&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;idealstan&lt;/code&gt; exploits variational Bayesian (VB) inference &lt;a name=cite-NIPS2015_5758&gt;&lt;/a&gt;(&lt;a href=&#34;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&#34;&gt;Kucukelbir, Ranganath, Gelman, and Blei, 2015&lt;/a&gt;) to assist with finding parameters for identification. &lt;code&gt;idealstan&lt;/code&gt; first estimates an IRT model with rotation-unidentified parameters, which is not a problem for a single VB run. Then &lt;code&gt;idealstan&lt;/code&gt; can select those parameters which show the highest or lowest values, and these parameters are then constrained in terms of their polarity. This approach can quickly speed up the sometimes painful process of identifying a particular IRT ideal point model, although it is also possible to constrain parameters before estimation based on prior information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;missing-data-in-ideal-point-models&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Missing Data in Ideal Point Models&lt;/h2&gt;
&lt;p&gt;Because of the flexibility of Stan,&lt;code&gt;idealstan&lt;/code&gt; is not limited to implementing the standard IRT ideal point model. One of the modifications it offers is a model that can account for one-way censoring in the data, or what is called an absence-inflated ideal point model &lt;a name=cite-kubinec2017&gt;&lt;/a&gt;(&lt;a href=&#34;#bib-kubinec2017&#34;&gt;Kubinec, 2017a&lt;/a&gt;). This model was motivated by the application of ideal point models to legislatures in which legislator actions are more diverse than simply Yes or No votes. Legislators can also be absent on votes and in parliamentary systems as in Europe, abstentions are also common. Abstentions occur when a legislator votes on a policy but refuses to either vote Yes or No, resulting in a vote record of abstention that conventional models will treat as missing data. However, if these two additional vote outcomes–absent and abstention–are removed from the estimation by encoding them as missing data, then the additional information about legislator behavior present in this data is lost.&lt;/p&gt;
&lt;p&gt;As a solution for abstentions, I proposed in &lt;a href=&#34;#bib-kubinec2017&#34;&gt;Kubinec (2017a)&lt;/a&gt; to model abstentions as a middle category between yes and no votes by treating &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ij}\)&lt;/span&gt; as an ordered logistic outcome &lt;span class=&#34;math inline&#34;&gt;\(Y\_{ijk}\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(k\)&lt;/span&gt; in three possible vote choices: &lt;span class=&#34;math inline&#34;&gt;\(\{1,2,3\}\)&lt;/span&gt; (no, abstain, yes).&lt;/p&gt;
&lt;p&gt;While switching from a logit to an ordered logistic model is straightforward, modeling the censoring implied by legislator absences requires the estimation of additional parameters. To do so, I model absence as a binary outcome in a separate equation as a hurdle model. Intuitively, legislators only show up to vote if they are able to overcome the hurdle of being present. Because in an ideal point model we only want to estimate a single set of person parameters (ideal points), I include an additional set of item (bill) parameters in the hurdle component that signify the way in which being absent on a policy proposal is related to the ideological value of the legislation.&lt;/p&gt;
&lt;p&gt;The result is a two-stage model that inflates the ideal points by the probability that a legislator is absent on a particular bill. To create this model, I first start again with the essential 2-PL IRT model except that it now allows for cutpoints &lt;span class=&#34;math inline&#34;&gt;\(c\_k\)&lt;/span&gt; for the &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; vote outcomes (yes, abstain, no) with &lt;span class=&#34;math inline&#34;&gt;\(\zeta(\cdot)\)&lt;/span&gt; representing the logit function:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
    L(\beta,\alpha,x|Y\_{ijk}) = \prod\_{i-1}^{I} \prod\_{j=1}^{J}
    \begin{cases} 
    1 -  \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_1) &amp;amp; \text{if } K = 0 \\
    \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k-1}) - \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k})       &amp;amp; \text{if } 0 &amp;lt; k &amp;lt; K, \text{ and} \\
    \zeta(x\_{i}&amp;#39;\beta\_j - \alpha\_j - c\_{k-1}) - 0 &amp;amp; \text{if } k=K
    \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Except for the addition of the vote cutpoints, this model is identical to the standard 2-PL IRT form, with ability parameters &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;, discriminations &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt;, and difficulties &lt;span class=&#34;math inline&#34;&gt;\(a\_j\)&lt;/span&gt;. The cutpoints carve up the latent space so that as ability rises or falls, the legislator becomes more likely to vote yes or no with abstention falling in between these two categories.&lt;/p&gt;
&lt;p&gt;This ordered logit has to be embedded in the hurdle model to account for one-way censoring. Suppose that each legislator has a choice &lt;span class=&#34;math inline&#34;&gt;\(r \in \{0,1\}\)&lt;/span&gt; in which &lt;span class=&#34;math inline&#34;&gt;\(r=1\)&lt;/span&gt; if a legislator is absent and &lt;span class=&#34;math inline&#34;&gt;\(r=0\)&lt;/span&gt; otherwise. With this notation, the likelihood of the hurdle model takes the following form:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
    L(\beta,\alpha,X,Q,\gamma,\omega|Y\_{k},Y\_{r}) = 
    \prod\_{I}^{i=1} \prod\_{J}^{j=1}
    \begin{cases}
    \zeta(x\_{i}&amp;#39;\gamma\_j - \omega\_j ) &amp;amp; \text{if } r=0, \text{ and} \\
    (1-\zeta({x\_{i}&amp;#39;\gamma\_j - \omega\_j}))L(\beta,\alpha,X|Y\_{k1}) &amp;amp; \text{if } r=1
    \end{cases}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;For the hurdle model to be able to affect the legislator ideal points, a separate IRT 2-PL model is included to predict the binary outcome of absence or presence. In this secondary model, the &lt;span class=&#34;math inline&#34;&gt;\(\gamma\_j\)&lt;/span&gt; discrimination and &lt;span class=&#34;math inline&#34;&gt;\(\omega\_j\)&lt;/span&gt; difficulty parameters represent the salience of a particular piece of legislation to an individual legislator &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt;. Only if a bill clears the hurdle of salience will the legislator choose to vote on the legislation, i.e., reach the vote outcome model &lt;span class=&#34;math inline&#34;&gt;\(L(\beta,\alpha,X)\)&lt;/span&gt;. This device allows for the ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; to adjust for the fact that the decision of a legislator to show up to vote on a particular bill may be strategic instead of random. In addition, this model is more widely applicable to situations in which an ideal point model is employed and missing data may be a function of the persons’ ideal points.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Model Simulation&lt;/h2&gt;
&lt;p&gt;To identify the model, I placed &lt;span class=&#34;math inline&#34;&gt;\(N(0,1)\)&lt;/span&gt; parameters on the &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; ideal points and additional polarity constraints on either the ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; or the discrimination parameters &lt;span class=&#34;math inline&#34;&gt;\(\gamma\_j\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\alpha\_j\)&lt;/span&gt;. As a further restriction, I constrain one of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\_j\)&lt;/span&gt; to equal zero. These are the standard set of restrictions included in &lt;code&gt;idealstan&lt;/code&gt;, although the scales of parameters can be modified. The full set of priors is as follows:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
        c\_k - c\_{k-1} \sim N(0,5)\\
        \gamma\_j \sim N(0,2)\\
        \omega\_j \sim N(0,5)\\
        \beta\_j \sim N(0,2)\\
        \alpha\_j \sim N(0,5)\\
        x\_i \sim N(0,1)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class=&#34;math inline&#34;&gt;\(c\_k\)&lt;/span&gt; parameters are the cutpoints for the &lt;span class=&#34;math inline&#34;&gt;\(K-1\)&lt;/span&gt; ordinal outcomes. They are given a weakly informative prior on the differences between the cutpoints (see the prior help file in the Stan Development Github site). The rest of the priors are arbitrary as the scale of the latent variables is fixed only in the priors themselves.&lt;/p&gt;
&lt;p&gt;To demonstrate the package, I first generate data from this data-generating process using a function &lt;code&gt;id_sim_gen()&lt;/code&gt; built into &lt;code&gt;idealstan&lt;/code&gt;:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-1&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-1&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;],[1,3,4,4,4,3],[4,1,1,1,4,4],[4,2,3,1,3,4],[1,4,4,4,4,1],[1,4,3,4,4,4],[4,2,3,2,3,2],[1,4,4,4,4,2],[2,4,4,4,4,4],[4,1,1,1,3,4],[4,3,3,3,4,3]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;1&lt;\/th&gt;\n      &lt;th&gt;2&lt;\/th&gt;\n      &lt;th&gt;3&lt;\/th&gt;\n      &lt;th&gt;4&lt;\/th&gt;\n      &lt;th&gt;5&lt;\/th&gt;\n      &lt;th&gt;6&lt;\/th&gt;\n      &lt;th&gt;7&lt;\/th&gt;\n      &lt;th&gt;8&lt;\/th&gt;\n      &lt;th&gt;9&lt;\/th&gt;\n      &lt;th&gt;10&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:[1,2,3,4,5,6,7,8,9,10]},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;In this matrix, the legislators (persons) are represented by the rows and the bills (items) by the columns. The ordinal vote outcomes are numbered 1 to 3 (1=No,2=Abstain,3=Yes) and 4 represents absence.&lt;/p&gt;
&lt;p&gt;I can then take this simulated data and put it into the &lt;code&gt;id_estimate&lt;/code&gt; function. I also use the true values of the legislator ideal points &lt;span class=&#34;math inline&#34;&gt;\(x\_i\)&lt;/span&gt; for polarity constraints in order to be able to return the “true” latent variables. The &lt;code&gt;id_estimate&lt;/code&gt; function loads pre-compiled stan code a la &lt;a href=&#34;https://cran.r-project.org/web/packages/rstantools/index.html&#34;&gt;&lt;code&gt;rstantools&lt;/code&gt;&lt;/a&gt; and then returns an R object containing a compiled stan model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;true_legis &amp;lt;- ord_ideal_sim@simul_data$true_person
high_leg &amp;lt;- sort(true_legis,decreasing = T,index.return=T)
low_leg &amp;lt;- sort(true_legis,index.return=T)

ord_ideal_est &amp;lt;- id_estimate(idealdata=ord_ideal_sim,
                             model_type=4,
                             fixtype=&amp;#39;constrained&amp;#39;,
                             restrict_type=&amp;#39;constrain_twoway&amp;#39;,
                             restrict_ind_high = high_leg$ix[1:2],
                             restrict_ind_low=low_leg$ix[1:2],
                             refresh=500)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For testing simulation results, the package contains a residual plot for looking at differences between true and estimated values which are stored in the R object:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/check_true-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;The recovery of the true parameters is not perfect, but generally the errors sum to zero across the parameters. Recovering the true parameters is not usually of great interest in a latent variable model as the scale and rotation of the true values is rather arbitrary. However, this exercise is a basic test of the Stan model’s correspondence with the simulated data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-the-polarization-of-coffee&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical Example: The Polarization of Coffee&lt;/h2&gt;
&lt;p&gt;To demonstrate the model’s functionality, I first use a dataset of ordinal rankings draw from Amazon food product reviews. &lt;a name=cite-mcauley2013&gt;&lt;/a&gt;&lt;a href=&#34;#bib-mcauley2013&#34;&gt;McAuley and Leskovec (2013)&lt;/a&gt; collected over 500,000 Amazon food reviews from 1999 to 2012 which are all coded on the 1 to 5 ranking scale that users can post on each product. I focus on a subset of this data by collecting all the reviews that mention the word “coffee” at least a handful of times.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;just_coffee &amp;lt;- readRDS(&amp;#39;just_coffee.rds&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ideal point models use variation between products and users in reviews to identify the latent space. For these reasons, little information is contributed by products and/or users that have very few reviews, and we can remove them from the data first to reduce the dimensionality of the data. Because &lt;code&gt;idealstan&lt;/code&gt; is a full Bayesian model, these users and products do not cause any statistical problems, but they will slow estimation considerably as a large number of users only review one or two products.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coffee_count_prod &amp;lt;- group_by(just_coffee,ProductId) %&amp;gt;% summarize(tot_unique=length(unique(UserId))) %&amp;gt;% 
  filter(tot_unique&amp;lt;30)
coffee_count_user &amp;lt;- group_by(just_coffee,UserId) %&amp;gt;% summarize(tot_unique=length(unique(ProductId))) %&amp;gt;% 
  filter(tot_unique&amp;lt;3)
just_coffee &amp;lt;- anti_join(just_coffee,coffee_count_prod,by=&amp;#39;ProductId&amp;#39;) %&amp;gt;% 
  anti_join(coffee_count_user,by=&amp;#39;UserId&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The data is currently in long format. To bring the data into &lt;code&gt;idealstan&lt;/code&gt;, it first must be translated to wide format in which each item (product) is a column and each person (user) is a row. For this model, we will drop missing data as we will focus solely on the model’s ordinal rankings without accounting for the fact that users do not have reviews for all products.&lt;/p&gt;
&lt;p&gt;Even with inactive users removed from the data, this model is quite large with 11269 users and 429 products. However, we can take advantage of variational inference in Stan to obtain approximate posterior estimates in a reasonable amount of time. We will also take advantage of &lt;code&gt;idealstan&lt;/code&gt;’s use of variational inference for automatic model identification by running a non-identified (i.e., no constraints) model first, determining which of the users has a very high ideal point, and then constraining that ideal point in the final fitted model.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;coffee_model &amp;lt;- id_estimate(idealdata=coffee_data,
                            model_type=3,
                            fixtype=&amp;#39;vb&amp;#39;,
                            restrict_ind_high = 1,
                            restrict_params=&amp;#39;person&amp;#39;,
                            auto_id = F,
                            use_vb=T)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.042 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 420 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -6e+004             1.000            1.000
##    200    -5e+004             0.642            1.000
##    300    -5e+004             0.441            0.284
##    400    -5e+004             0.346            0.284
##    500    -5e+004             0.277            0.059
##    600    -5e+004             0.234            0.059
##    700    -5e+004             0.200            0.040
##    800    -5e+004             0.176            0.040
##    900    -5e+004             0.157            0.014
##   1000    -5e+004             0.142            0.014
##   1100    -5e+004             0.042            0.006   MEDIAN ELBO CONVERGED
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.
## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.035 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 350 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -5e+009             1.000            1.000
##    200    -1e+008            22.582           44.164
##    300    -9e+010            15.388            1.000
##    400    -4e+009            17.796           25.020
##    500    -2e+007            58.526           25.020
##    600    -2e+006            49.788           25.020
##    700    -1e+006            42.834            6.097
##    800    -5e+004            40.303           22.581
##    900    -5e+004            35.826            6.097
##   1000    -5e+004            32.244            6.097
##   1100    -5e+004            32.146            6.097   MAY BE DIVERGING... INSPECT ELBO
##   1200    -5e+004            27.730            1.114   MAY BE DIVERGING... INSPECT ELBO
##   1300    -5e+004            27.630            1.114   MAY BE DIVERGING... INSPECT ELBO
##   1400    -5e+004            25.128            0.016   MAY BE DIVERGING... INSPECT ELBO
##   1500    -5e+004             2.984            0.015   MAY BE DIVERGING... INSPECT ELBO
##   1600    -5e+004             2.374            0.005   MEDIAN ELBO CONVERGED   MAY BE DIVERGING... INSPECT ELBO
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;What is of primary interest in this model are the discrimination parameters for the products. The discriminations will tell us which coffee products tend to divide the users most strongly into two poles as the model is one-dimensional. We can first look at the distribution of discriminations via a histogram:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_all_hist(coffee_model,params = &amp;#39;regular_discrim&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/plot_discrim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To gain a sense of how polarizing these products, we can plot the product mid-point, or the point at which a user is indifferent between one rating score and a higher rating score, over the ideal point distribution. I color the user points by their actual ratings:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot_legis(coffee_model,group_color=F,group_overlap = T,
              person_labels=F,person_ci_alpha = .05,
              item_plot=429,
              group_labels=F,
              point_size=2,
              show_score = c(&amp;#39;1&amp;#39;,&amp;#39;2&amp;#39;,&amp;#39;3&amp;#39;,&amp;#39;4&amp;#39;,&amp;#39;5&amp;#39;),
              text_size_group = 4)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/high_pos_discrim-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As can be seen, the product midpoints nearly perfectly segment the user base. We also a large number of 1s and 5s in the observed ratings. This distribution shows that users tend to be very divided on this product, and also that their disagreement over the product is itself a reflection of an underlying dimension, their ideal points.&lt;/p&gt;
&lt;p&gt;Finally, I include here the top 10 most polarizing (highly discriminative) products from each end of the ideal point spectrum.&lt;/p&gt;
&lt;p&gt;First, the top 10 most positive discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-2&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-2&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;Kicking Horse Coffee 454 Horse Power Dark, Whole Bean&#34;,&#34;Amazing Grass Green Superfood Organic Powder&#34;,&#34;Starbucks Via Ready Brew Instant Coffee&#34;,&#34;Stephen&#39;s Gourmet Hot Cocoa, Candycane Cocoa&#34;,&#34;Prince of Peace Organic Tea, Oolong&#34;,&#34;NOW Foods Erythritol Natural Sweetener&#34;,&#34;illy Cappucino, 12 pack&#34;,&#34;Nestle Hot Cocoa Mix&#34;,&#34;Better than Milk Vegan Soy Powder&#34;,&#34;Ghirardelli Chocolate Sweet Ground Chocolate &amp;amp; Cocoa&#34;],[7.87259,7.864585,7.80567,7.805575,7.770375,7.770345,7.727985,7.33822,7.33797,7.28447]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Product Description&lt;\/th&gt;\n      &lt;th&gt;Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Second, the top 10 least positive negative discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-3&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-3&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;Melitta Cafe de Europa Gourmet Coffee&#34;,&#34;McCann&#39;s Steel Cut Irish Oatmeal&#34;,&#34;Stash Tea Gunpowder Green Loose Leaf Tea&#34;,&#34;Nutiva Organic Coconut Oil&#34;,&#34;Stash Tea Company English Breakfast&#34;,&#34;Equal Exchange Organic Coffee, Mind Body Soul, Whole Bean&#34;,&#34;Ghirardelli Premium Hot Beverage Mix, White Mocha, 19-Ounce Cans&#34;,&#34;Victorian Inn Instant Hot Cappucino, French Vanilla&#34;,&#34;Medaglia Doro Instant Espresso&#34;,&#34;Yogi Tea, Green Tea Super Antioxidant&#34;],[-9.79306,-9.785615,-9.70132,-9.616965,-7.77101,-7.75944,-7.74106,-7.73298,-7.17082,-7.13721]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Product Description&lt;\/th&gt;\n      &lt;th&gt;Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;While a full interpretation of these results would require significant work at examining the full range of products and their relevant discrimination scores, what is clear is that positive discrimination tends to have more large brands of coffee, including Starbucks and Illy, while the negative discrimination products tend to b esmaller brands, such as Melitta Cafe and Equal Exchange Organic Coffee. The most interesting finding in this analysis is that Ghirardelli products are at both ends of the scale: one type of Ghirardelli powdered drink, Ghirardelli Chocolate, has high positive discrimination, while Ghirardelli White Mocha has high negative discrimination. Intuitivelly, these products are appealing to different reviewers with very diverging preferences.&lt;/p&gt;
&lt;p&gt;In the next section, I apply the ideal point model to a more traditional domain–the U.S. Congress–and also examine the role of including missing data via inflation.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;empirical-example-114th-senate&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Empirical Example: 114th Senate&lt;/h2&gt;
&lt;p&gt;As an empirical example of the model, I use data from the &lt;a href=&#34;http://www.voteview.com&#34; class=&#34;uri&#34;&gt;http://www.voteview.com&lt;/a&gt; website on the voting record of the 114th US Senate. This dataset comes with the &lt;code&gt;idealstan&lt;/code&gt; package, and I then recode the data to correspond to a standard R matrix. However, because abstentions rarely happen in the US Congress, I do not include abstentions in this model and instead estimate a standard binary IRT 2-PL with inflation for missing data (absences). I then pass the matrix of vote records to the &lt;code&gt;id_make&lt;/code&gt; function to create an object suitable for running the model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/use_senate-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Given a suitable object from &lt;code&gt;id_make&lt;/code&gt;, the &lt;code&gt;id_estimate&lt;/code&gt; function can estimate a variety of IRT ideal point models, including 2-PL, ordinal, hierarchical and dynamic IRT (using random-walk priors). For this run we set the &lt;code&gt;model_type&lt;/code&gt; parameter to &lt;code&gt;2&lt;/code&gt;, which represents a 2-PL model with absence inflation. Because this dataset is of a significant size, I use variational inference instead of the full sampler to demonstrate its use. I do not, however, use the automatic identification option &lt;code&gt;auto_id&lt;/code&gt; because it is relatively easy to constrain particular legislator ideal points for the Senate. In this case, I constrain Ben Sasse, Marco Rubio, Berni Sanders, Ted Cruz, Harry Reid and Elizabeth Warren, all senators with pronounced ideological views.&lt;/p&gt;
&lt;p&gt;After estimating the model, I can look at a graphical display of the ideal points with the &lt;code&gt;id_plot&lt;/code&gt; function. The &lt;code&gt;id_plot&lt;/code&gt; function produces a &lt;code&gt;ggplot2&lt;/code&gt; object which can be further modified. The legislators are given colors based on their party affiliation. The general shape of the ideal point distribution reflects the nature of polarization in the US Congress, with relatively few moderates near the center of the distribution.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## ------------------------------------------------------------
## EXPERIMENTAL ALGORITHM:
##   This procedure has not been thoroughly tested and may be unstable
##   or buggy. The interface is subject to change.
## ------------------------------------------------------------
## 
## 
## 
## Gradient evaluation took 0.026 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 260 seconds.
## Adjust your expectations accordingly!
## 
## 
## Begin eta adaptation.
## Iteration:   1 / 250 [  0%]  (Adaptation)
## Iteration:  50 / 250 [ 20%]  (Adaptation)
## Iteration: 100 / 250 [ 40%]  (Adaptation)
## Iteration: 150 / 250 [ 60%]  (Adaptation)
## Iteration: 200 / 250 [ 80%]  (Adaptation)
## Success! Found best value [eta = 1] earlier than expected.
## 
## Begin stochastic gradient ascent.
##   iter       ELBO   delta_ELBO_mean   delta_ELBO_med   notes 
##    100    -2e+004             1.000            1.000
##    200    -2e+004             0.509            1.000
##    300    -2e+004             0.341            0.018
##    400    -2e+004             0.256            0.018
##    500    -2e+004             0.205            0.003   MEDIAN ELBO CONVERGED
## 
## Drawing a sample of size 1000 from the approximate posterior... 
## COMPLETED.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/run_114_model-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can also plot the bill (item) midpoints as a line of indifference that we overlay on top of the ideal points.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_plot(sen_est,person_ci_alpha=.1,item_plot=205,
        group_labels=T,
        abs_and_reg=&amp;#39;Vote Points&amp;#39;) + scale_colour_brewer(type=&amp;#39;qual&amp;#39;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/bill_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;For this particular piece of legislation, the midpoint is right in the middle of the ideal point distribution, showing that the bill has very high discrimination. Also, the rug lines at the bottom of the plot, which show the HPD for the midpoint, indicate that the model is uncertain about the votes of only a handful of Senators on this bill.&lt;/p&gt;
&lt;p&gt;We can similarly examine the absence midpoint for the same bill, which signifies the place on the ideal point spectrum at which a legislator is indifferent from showing up to vote. In this case, only very conservative Republicans chose not to show up to vote.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.robertkubinec.com/post/stancon2018_paper_files/figure-html/abs_bill_plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can use the bill absence parameters to also see which bills showed the highest discrimination in terms of absences, or in other words, for which bills did the absence of legislators signify that they intentionally did not show up? To do this, I sort the discrimination parameters from the underlying Stan object and then merge them with the bill labels from voteview.org.&lt;/p&gt;
&lt;p&gt;First we can look at the top 10 bills with liberal/Democrat absence discrimination:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-4&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-4&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;To ensure that the storage and transportation of petroleum coke is regulated in a manner that ensures the protection of public and ecological health.&#34;,&#34;To continue cleaning up fields and streams while protecting neighborhoods, generating affordable energy, and creating jobs.&#34;,&#34;To express the sense of the Senate that climate change is real and not a hoax.&#34;,&#34;To require the use of iron, steel, and manufactured goods produced in the United States in the construction of the Keystone XL Pipeline and facilities.&#34;,&#34;To express the sense of Congress regarding federally protected land.&#34;,&#34;To promote energy efficiency.&#34;,&#34;To provide limits on the designation of new federally protected land.&#34;,&#34;To express the sense of Congress regarding climate change.&#34;,&#34;To conform citizen suits under the Endangered Species Act of 1973.&#34;,&#34;To ensure that oil transported through the Keystone XL pipeline into the United States is used to reduce United States dependence on Middle Eastern oil.&#34;],[1.819605,1.688775,1.67506,1.66709,1.643,1.549565,1.52296,1.499735,1.483755,1.453105]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Bill Description&lt;\/th&gt;\n      &lt;th&gt;Absence Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;Interestingly, Democrats appeared to be strategically absent most often on bills about climate change and the Keystone XL pipeline.&lt;/p&gt;
&lt;p&gt;For conseratives the bills are as follows:&lt;/p&gt;
&lt;div id=&#34;htmlwidget-5&#34; style=&#34;width:100%;height:auto;&#34; class=&#34;datatables html-widget&#34;&gt;&lt;/div&gt;
&lt;script type=&#34;application/json&#34; data-for=&#34;htmlwidget-5&#34;&gt;{&#34;x&#34;:{&#34;filter&#34;:&#34;none&#34;,&#34;data&#34;:[[&#34;1&#34;,&#34;2&#34;,&#34;3&#34;,&#34;4&#34;,&#34;5&#34;,&#34;6&#34;,&#34;7&#34;,&#34;8&#34;,&#34;9&#34;,&#34;10&#34;],[&#34;To limit the availability of amounts authorized to be appropriated for overseas contingency operations pending relief from the spending limits under the Budget Control Act of 2011.&#34;,&#34;To strike the FOIA exemption.&#34;,&#34;To improve the bill.&#34;,&#34;To improve the requirements relating to removal of personal information from cyber threat indicators before sharing.&#34;,&#34;To strengthen the Justice for Victims of Trafficking Act by incorporating additional bipartisan amendments.&#34;,&#34;To protect information that is reasonably believed to be personal information or information that identifies a specific person.&#34;,&#34;To improve the definitions of cybersecurity threat and cyber threat indicator.&#34;,&#34;To exempt from the capability and process within the Department of Homeland Security communication between a private entity and the Federal Bureau of Investigation or the United States Secret Service regarding cybersecurity threats.&#34;,&#34;An original bill to improve cybersecurity in the United States through enhanced sharing of information about cybersecurity threats, and for other purposes.&#34;,&#34;To modify section105 to require DHS to review all cyber threat indicators and countermeasures in order to remove certain personal information.&#34;],[-3.956115,-3.639605,-3.63636,-3.57792,-3.516255,-3.42238,-3.15867,-3.12428,-3.075425,-2.992295]],&#34;container&#34;:&#34;&lt;table class=\&#34;display\&#34;&gt;\n  &lt;thead&gt;\n    &lt;tr&gt;\n      &lt;th&gt; &lt;\/th&gt;\n      &lt;th&gt;Bill Description&lt;\/th&gt;\n      &lt;th&gt;Absence Discrimination&lt;\/th&gt;\n    &lt;\/tr&gt;\n  &lt;\/thead&gt;\n&lt;\/table&gt;&#34;,&#34;options&#34;:{&#34;columnDefs&#34;:[{&#34;className&#34;:&#34;dt-right&#34;,&#34;targets&#34;:2},{&#34;orderable&#34;:false,&#34;targets&#34;:0}],&#34;order&#34;:[],&#34;autoWidth&#34;:false,&#34;orderClasses&#34;:false},&#34;selection&#34;:{&#34;mode&#34;:&#34;multiple&#34;,&#34;selected&#34;:null,&#34;target&#34;:&#34;row&#34;}},&#34;evals&#34;:[],&#34;jsHooks&#34;:[]}&lt;/script&gt;
&lt;p&gt;For Republicans, on the other hand, it appears that cybersecurity and inter-departmental information sharing were the laws in which they chose not to show up for ideological (or political) reasons. This could be due to concerns among the conservative base regarding government over-reach and collecting information.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;idealstan&lt;/code&gt; is an effort to put state-of-the-art ideal point models along with the power of full Bayesian analysis in the hands of applied researchers in the social sciences. While the empirical examples presented were from the US Congress, the model can be used in any situation in which the assumptions of the ideal point model (unconstrained ideal point space) apply, i.e., situations in which the latent space is fundamentally polarizing between people. While similar in function and form to the &lt;code&gt;edstan&lt;/code&gt; package, the &lt;code&gt;idealstan&lt;/code&gt; package has to use more complicated forms of identification because of the difficulty of leaving discrimination parameters unconstrained.&lt;/p&gt;
&lt;p&gt;Moving forward, I intend to add in more functionality to smoothly operate with &lt;code&gt;shinystan&lt;/code&gt; and &lt;code&gt;bayesplot&lt;/code&gt;, as well as add further types of explanatory IRT models. The intention is to have a package on which applied researchers can run different ideal point models and then examine how different modeling choices affect the results. In addition, I will continue to build more visualization options so that the results are easily digestable and publishable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;’ &lt;a name=bib-enelow198&gt;&lt;/a&gt;&lt;a href=&#34;#cite-enelow198&#34;&gt;[1]&lt;/a&gt; J. M. Enelow and M. J. Hinich. &lt;em&gt;The Spatial Theory of Voting: An Introduction&lt;/em&gt;. Cambridge University Press, 1984.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-takane1986&gt;&lt;/a&gt;&lt;a href=&#34;#cite-takane1986&#34;&gt;[2]&lt;/a&gt; Y. Takane and J. de Leeuw. “On the Relationship Between Item Response Theory and Factor Analysis of Discretized Variables”. In: &lt;em&gt;Psychometrika&lt;/em&gt; 52.3 (1986), pp. 393-408.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-hoff2002&gt;&lt;/a&gt;&lt;a href=&#34;#cite-hoff2002&#34;&gt;[3]&lt;/a&gt; P. D. Hoff, A. E. Raftery and M. S. Hancock. “Latent Space Approaches to Social Network Analysis”. In: &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt; 97.460 (2002), pp. 1090-1098.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-quinn2002&gt;&lt;/a&gt;&lt;a href=&#34;#cite-quinn2002&#34;&gt;[4]&lt;/a&gt; A. D. Martin and K. M. Quinn. “Dynamic Ideal Point Estimation via Markov Chain Monte Carlo for the U.S. Supreme Court, 1953-1999”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 10.2 (2002), pp. 134-153.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-jackman2004&gt;&lt;/a&gt;&lt;a href=&#34;#cite-jackman2004&#34;&gt;[5]&lt;/a&gt; J. Clinton, S. Jackman and D. Rivers. “The Statistical Analysis of Rollcall Data”. In: &lt;em&gt;American Political Science Review&lt;/em&gt; 98.2 (2004), pp. 355-370.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-gelman2005&gt;&lt;/a&gt;&lt;a href=&#34;#cite-gelman2005&#34;&gt;[6]&lt;/a&gt; J. Bafumi, A. Gelman, D. K. Park, et al. “Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 13.2 (2005), pp. 171-187.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-poole2008&gt;&lt;/a&gt;&lt;a href=&#34;#cite-poole2008&#34;&gt;[7]&lt;/a&gt; K. T. Poole, J. B. Lewis, J. Lo, et al. &lt;em&gt;Scaling Roll Call Votes with W-NOMINATE in R&lt;/em&gt;. Working Paper. Social Science Research Network, Oct. 06, 2008. URL: &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&#34; class=&#34;uri&#34;&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1276082&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-slapin2008&gt;&lt;/a&gt;&lt;a href=&#34;#cite-slapin2008&#34;&gt;[8]&lt;/a&gt; J. B. Slapin and S. Proksch. “A Scaling Model for Estimating Time-Series Party Positions from Texts”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 52.3 (2008), pp. 705-722.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-carroll2013&gt;&lt;/a&gt;&lt;a href=&#34;#cite-carroll2013&#34;&gt;[9]&lt;/a&gt; R. Carroll, J. B. Lewis, J. Lo, et al. “The Structure of Utility in Spatial Models of Voting”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 57.4 (2013), pp. 1008-1028.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-mcauley2013&gt;&lt;/a&gt;&lt;a href=&#34;#cite-mcauley2013&#34;&gt;[10]&lt;/a&gt; J. McAuley and J. Leskovec. “From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through Online Reviews”. In: &lt;em&gt;WWW&lt;/em&gt; (2013).&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-armstrong2014&gt;&lt;/a&gt;&lt;a href=&#34;#cite-armstrong2014&#34;&gt;[11]&lt;/a&gt; D. A. Armstrong, R. Bakker, R. Carroll, et al. &lt;em&gt;Analyzing Spatial Models of Choice and Judgment with R&lt;/em&gt;. CRC Press, 2014.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-bonica2014&gt;&lt;/a&gt;&lt;a href=&#34;#cite-bonica2014&#34;&gt;[12]&lt;/a&gt; A. Bonica. “Mapping the Ideological Marketplace”. In: &lt;em&gt;American Journal of Political Science&lt;/em&gt; 58.2 (2014), pp. 367-386.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-barbera2015&gt;&lt;/a&gt;&lt;a href=&#34;#cite-barbera2015&#34;&gt;[13]&lt;/a&gt; P. BarberÃ¡. “Birds of the Same Feather Tweet Together: Bayesian Ideal Point Estimation Using Twitter Data”. In: &lt;em&gt;Political Analysis&lt;/em&gt; 23 (2015), pp. 76-91.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-NIPS2015_5758&gt;&lt;/a&gt;&lt;a href=&#34;#cite-NIPS2015_5758&#34;&gt;[14]&lt;/a&gt; A. Kucukelbir, R. Ranganath, A. Gelman, et al. “Automatic Variational Inference in Stan”. In: &lt;em&gt;Advances in Neural Information Processing Systems 28&lt;/em&gt;. Ed. by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama and R. Garnett. Curran Associates, Inc., 2015, pp. 568-576. URL: &lt;a href=&#34;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&#34; class=&#34;uri&#34;&gt;http://papers.nips.cc/paper/5758-automatic-variational-inference-in-stan.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a name=bib-kubinec2017&gt;&lt;/a&gt;&lt;a href=&#34;#cite-kubinec2017&#34;&gt;[15]&lt;/a&gt; R. Kubinec. &lt;em&gt;Absence Makes the Ideal Points Sharper&lt;/em&gt;. Poster. Political Methodology Society, 2017.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
