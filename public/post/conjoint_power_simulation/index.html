<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.54.0" />
  <meta name="author" content="Robert Kubinec">
  <meta name="description" content="Postdoctoral Researcher">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/academicons.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather%7CRoboto+Mono">
  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  <link rel="alternate" href="http://www.robertkubinec.com/index.xml" type="application/rss+xml" title="">
  <link rel="feed" href="http://www.robertkubinec.com/index.xml" type="application/rss+xml" title="">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="http://www.robertkubinec.com/post/conjoint_power_simulation/">

  

  <title>Simulating Conjoint Survey Experiments | </title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/"></a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#posts">
            
            <span>Posts</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  
  <div class="article-header">
    <img src="/img//headers/norad.jpg" class="article-banner" itemprop="image">
    
  </div>
  

  <div class="article-container">
    <h1 itemprop="name">Simulating Conjoint Survey Experiments</h1>
    

<div class="article-metadata">

  <span class="article-date">
    <time datetime="2019-02-11 00:00:00 &#43;0000 UTC" itemprop="datePublished">
      Mon, Feb 11, 2019
    </time>
  </span>

  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=http%3a%2f%2fwww.robertkubinec.com%2fpost%2fconjoint_power_simulation%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Simulating%20Conjoint%20Survey%20Experiments&amp;url=http%3a%2f%2fwww.robertkubinec.com%2fpost%2fconjoint_power_simulation%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2fwww.robertkubinec.com%2fpost%2fconjoint_power_simulation%2f&amp;title=Simulating%20Conjoint%20Survey%20Experiments"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=http%3a%2f%2fwww.robertkubinec.com%2fpost%2fconjoint_power_simulation%2f&amp;title=Simulating%20Conjoint%20Survey%20Experiments"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Simulating%20Conjoint%20Survey%20Experiments&amp;body=http%3a%2f%2fwww.robertkubinec.com%2fpost%2fconjoint_power_simulation%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    <div class="article-style" itemprop="articleBody">
      
<script src="/rmarkdown-libs/kePrint/kePrint.js"></script>


<div id="background" class="section level2">
<h2>Background</h2>
<p>Conjoint survey experiments have become more popular in political science since the publication of <a href="http://hdl.handle.net/1721.1/84064">Hainmueller, Hopkins and Yamamoto (2014)</a>. However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see <a href="https://osf.io/preprints/socarxiv/mrfcu/">SocArchiv Draft</a>). I employ both traditional power measures and newer statistics from <a href="http://journals.sagepub.com/doi/abs/10.1177/1745691614551642">Gelman and Carlin (2014)</a> reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).</p>
<p>The original Rmarkdown and saved simulation files can be downloaded from the <a href="https://github.com/saudiwin/saudiwin.github.io/tree/sources/content">site’s Github</a>.</p>
<p>The packages required to run this simulation are listed in the code block below:</p>
<pre class="r"><code>#Required packages
require(ggplot2)
require(dplyr)
require(tidyr)
require(multiwayvcov)
require(lmtest)
require(stringr)
require(kableExtra)

# package MASS also used but not loaded

# != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows.
# to install parallelsugar:
# install.packages(&#39;devtools&#39;)
# library(devtools)
# install_github(&#39;nathanvan/parallelsugar&#39;)

# If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used

if(.Platform$OS.type==&#39;windows&#39;) {
  parallelfunc &lt;- parallelsugar::mclapply_socket
} else {
  parallelfunc &lt;- parallel::mclapply
}</code></pre>
</div>
<div id="simulation-set-up" class="section level2">
<h2>Simulation Set-up</h2>
<p>The following parameters control the range of coefficients tested and the number of simulations. The survey experiment design employs vignettes in which appeals and the actors making appeals are allowed to vary between respondents. Any one vignette has one actor and one appeal. The probability of assignment is assumed to be a simple random fraction of the number of appeal-actor combinations (14). If <code>run_sim</code> is set to <code>TRUE</code>, the simulation is run, otherwise the simulation results are loaded from an RDS file and plotted. Running the simulation will take approximately 6 to 12 hours depending on the number of cores and speed of the CPU.</p>
<pre class="r"><code>#Actually run the simulation or just load the data and look at it?
run_sim &lt;- FALSE
# Max number of respondents fixed at 2700
num_resp &lt;- 2700
# Number of iterations (breaks in sample size)
num_breaks &lt;- 300
# Number of simulations to run per iteration
n_sims &lt;- 1000</code></pre>
<p>I then create a grid of all possible actor-appeal combinations as I am using simple randomization of profiles before presenting them to respondents. There are two vectors of treatments (actors and appeals) that each have 7 separate treatments for a total of 14 separate possible treatments.</p>
<pre class="r"><code># Two treatment variables producing a cross-product of 7x7
treatments1 &lt;- c(&#39;military&#39;,&#39;MOI&#39;,&#39;president&#39;,&#39;MOJ&#39;,&#39;parliament&#39;,&#39;municipality&#39;,&#39;government&#39;)
treatments2 &lt;- c(&#39;exprop.firm&#39;,&#39;exprop.income&#39;,&#39;permit.reg&#39;,&#39;contracts.supply&#39;,&#39;permit.export&#39;,&#39;permit.import&#39;,&#39;reforms&#39;)
total_treat &lt;- length(c(treatments1,treatments2))
grid_pair &lt;- as.matrix(expand.grid(treatments1,treatments2))
print(head(grid_pair))</code></pre>
<pre><code>##      Var1           Var2         
## [1,] &quot;military&quot;     &quot;exprop.firm&quot;
## [2,] &quot;MOI&quot;          &quot;exprop.firm&quot;
## [3,] &quot;president&quot;    &quot;exprop.firm&quot;
## [4,] &quot;MOJ&quot;          &quot;exprop.firm&quot;
## [5,] &quot;parliament&quot;   &quot;exprop.firm&quot;
## [6,] &quot;municipality&quot; &quot;exprop.firm&quot;</code></pre>
</div>
<div id="simulation" class="section level2">
<h2>Simulation</h2>
<p>To simulate the data, I first sample 14 coefficients <span class="math inline">\(\beta_j\)</span> (one for each treatment <span class="math inline">\(J\)</span>) from a normal distribution with mean zero and standard deviation one. I then randomly sample from two profile combinations for each of the <span class="math inline">\(I\)</span> respondents in accordance with simple random sampling. Two profile combinations, for a total of four tasks <span class="math inline">\(T\)</span>, are selected to reflect the fact that paired vignettes will be shown to each respondent as in the study design. I also sample a pre-treatment covariate <span class="math inline">\(Z_I\)</span> that is a random binomial vector with probability of 0.2 (thus 20% of respondents will fall into this cell). A treatment interaction effect <span class="math inline">\(\beta_z\)</span> is sampled from a normal distribution with mean 0.5 and standard deviation of 0.3 to provide a sampling distribution for the true effect, instead of assuming that the true effect is a fixed population value. Adding a distribution for <span class="math inline">\(\beta_j\)</span> reflects additional uncertainty beyond standard sampling distribution uncertainty. In this case, it represents additional measurement error between the true concept and the indicators used in the survey design.</p>
<p>I also post-stratify some estimates with a pre-treatment covariate <span class="math inline">\(Q_I\)</span> from a binomial distribution of probability .5 that has a constant effect on <span class="math inline">\(Y_{it}\)</span> of <span class="math inline">\(+1\)</span> (representing a fixed effect).</p>
<p>I then randomly sample a pair of outcomes, for a total of four tasks <span class="math inline">\(T\)</span>, for <span class="math inline">\(Y_{it}\)</span> in the range of <span class="math inline">\([1,10]\)</span> by drawing a number from a multivariate normal distribution. The mean <span class="math inline">\(\mu_{it}\)</span> of this normal distribution is equal to a linear model with an intercept of 5, the 14 dummy variables for treatment indicators <span class="math inline">\(X_j\)</span> with associated coefficients <span class="math inline">\(\beta_j\)</span>, the interaction <span class="math inline">\(\beta_z\)</span> between the pre-treatment covariate <span class="math inline">\(Z_i\)</span> and <span class="math inline">\(X_{ij}\)</span>, and a post-stratification covariate <span class="math inline">\(Q_i\)</span>. To simplify matters, <span class="math inline">\(Z_i\)</span> is not given its own constituent term as I am not interested in the unconditional effect of <span class="math inline">\(Z_i\)</span> on <span class="math inline">\(Y_{it}\)</span>, only the effect of <span class="math inline">\(X_{ijt}\)</span> on <span class="math inline">\(Y_{it}\)</span> given <span class="math inline">\(Z_{i}\)</span>. Finally, I draw correlated errors from a multivariate normal distribution with mean of zero and length of 4 (equal to the number of tasks per respondent) to produce a <span class="math inline">\(4 \times 4\)</span> variance matrix <span class="math inline">\(\varSigma_i\)</span> with a diagonal of 4 and intra-respondent covariation of 1 (correlation of 0.5).</p>
<p><span class="math display">\[
\begin{aligned}
X_{ITJ} &amp;\sim  \mathrm{B} \Big( \frac{1}{J \times 2} \Big)\\
B_{J} &amp;\sim  \mathrm{N}(0,2)\\
\beta_z &amp;\sim \mathrm{N}(0.5,0.3)\\
Z_I &amp;\sim  \mathrm{B}(0.2)\\
Q_I &amp;\sim  \mathrm{B}(0.5)\\
\mu_{it} &amp;=  5 + \sum_{j=1}^{J} \sum_{t=1}^{T} \beta_j * X_{itj} + \beta_z * X_{it1} *Z_i + Q_i\\
Y_{it} &amp;\sim  \mathrm{N}(\mu_{it},\varSigma_i)
\end{aligned}
\]</span></p>
<p>This process will produce some numbers outside the <span class="math inline">\([1,10]\)</span> range; however, it is better to leave these values in as explicit truncation will violate the assumptions of the underlying causal model.</p>
<p>I run 1000 simulations for each of 300 sequential sample sizes ranging from 100 to 2700. I then take the mean significant effect and report that as the likely significant effect size for that sample size. I also record the ratio of draws for which the effect is significant (the power). However, given that the true effect is not fixed, I interpret power as the ability detect a true effect greater than zero. I record both unadjusted p-values and p-values adjusted using the <code>cluster.vcov</code> function from the multiwayvcov package by clustering around respondent ID to reflect the pairing of vignettes. I also use separate results when post-stratifying on a pre-treatment covariate <span class="math inline">\(Q_I\)</span>.</p>
<p>In addition, I included M-errors (error of absolute magnitude of significant coefficients) and S-errors (incorrect sign of significant coefficients). M-errors provide an estimate of publication bias given that the <span class="math inline">\(p=0.05\)</span> threshold is a hard boundary and will necessarily result in smaller effects being reported as statistically insignificant when in fact they are greater than zero. S-errors help determine the probability that an estimated effect is the correct sign even if it is significant. S-errors are particularly problematic in small samples when sampling error can produce large negative deviations that may be statistically significant.</p>
<pre class="r"><code>if(run_sim==TRUE) {
  file.create(&#39;output_log.txt&#39;,showWarnings = FALSE)
    
    # Need to randomize over the simulations so that parallelization works correctly on windows
    
    sampled_seq &lt;- sample(seq(100,num_resp,length.out = num_breaks))
    
  all_sims &lt;- parallelfunc(sampled_seq,function(x) {
  
    out_probs &lt;- 1:n_sims
    cat(paste0(&quot;Now running simulation on data sample size &quot;,x),file=&#39;output_log.txt&#39;,sep=&#39;\n&#39;,append=TRUE)
  
    out_data &lt;- lapply(1:n_sims, function(j) {
      
      total_probs &lt;- sapply(1:x,function(x) {
        treat_rows &lt;- sample(1:nrow(grid_pair),4)
        treatments_indiv &lt;- c(grid_pair[treat_rows,])
        return(treatments_indiv)
      })
      
      by_resp &lt;- t(total_probs)
      by_resp &lt;- as_data_frame(by_resp)
      names(by_resp) &lt;- c(paste0(&#39;actor.&#39;,1:4,&quot;_cluster&quot;,c(1,1,2,2)),paste0(&#39;gift.&#39;,1:4,&quot;_cluster&quot;,c(1,1,2,2)))
      by_resp$respondent &lt;- paste0(&#39;Respondent_&#39;,1:nrow(by_resp))
      by_resp &lt;- gather(by_resp,attribute,indicator,-respondent) %&gt;% separate(attribute,into=c(&#39;attribute&#39;,&#39;cluster&#39;),sep=&#39;_&#39;) %&gt;% 
        separate(attribute,into=c(&#39;attribute&#39;,&#39;task&#39;)) %&gt;% spread(attribute,indicator)
      
      
      # Assign true coefficients for treatments
  
      #Beta_js
      
      coefs &lt;- data_frame(coef_val=rnorm(n=length(c(treatments1,treatments2)),mean=0,sd=1),
                          treat_label=c(treatments1,treatments2))
      
      # Create cluster covariance in the errors
      
      sigma_matrix &lt;- matrix(2,nrow=4,ncol=4)
      diag(sigma_matrix) &lt;- 4
  
      # Add on the outcome as a normal draw, treatment coefficients, interaction coefficient, group errors/interaction by respondent
      
      by_resp &lt;- gather(by_resp,treatment,appeal_type,actor,gift) %&gt;% 
        left_join(coefs,by=c(&#39;appeal_type&#39;=&#39;treat_label&#39;))
      
      # Record interaction coefficient (true estimate of interest)
      
      true_effect &lt;- rnorm(n=1,mean=0.5,sd=0.3)
      
      by_resp &lt;- select(by_resp,-treatment) %&gt;% spread(appeal_type,coef_val) %&gt;%  group_by(respondent) %&gt;% mutate(error=MASS::mvrnorm(1,mu=rep(0,4),Sigma=sigma_matrix)) %&gt;% ungroup
      
      # interaction coefficient only in function if military==TRUE
      
      by_resp &lt;- mutate(by_resp,int_coef=true_effect*rbinom(n = n(),prob = 0.2,size=1),
                        int_coef=if_else(military!=0,int_coef,0))
      by_resp &lt;- lapply(by_resp, function(x) {
        if(is.double(x)) {
          x[is.na(x)] &lt;- 0
        }
        return(x)
      }) %&gt;% as_data_frame
      
      # To make the outcome, need to turn the dataset long
      # However, we now need to drop the reference categories
      # Drop one dummy from actor/gift to prevent multicollinearity = reforms + government combination
      
      out_var &lt;- gather(by_resp,var_name,var_value,-respondent,-task,-cluster) %&gt;% 
         filter(!(var_name %in% c(&#39;reforms&#39;,&#39;government&#39;))) %&gt;% 
        group_by(respondent,task) %&gt;% summarize(outcome=sum(var_value)+5)
      
      combined_data &lt;- left_join(out_var,by_resp,by=c(&#39;respondent&#39;,&#39;task&#39;))
      
      
      # Re-estimate with a blocking variable
      
  
      combined_data$Q &lt;- c(rep(1,floor(nrow(combined_data)/2)),
                           rep(0,ceiling(nrow(combined_data)/2)))
      
      combined_data$outcome &lt;- if_else(combined_data$Q==1,combined_data$outcome+1,
                                       combined_data$outcome)
      
      # # Create data predictor matrix and estimate coefficients from the simulated dataset
      # 
      to_lm &lt;- ungroup(combined_data) %&gt;% select(contracts.supply:reforms,int_coef,Q)
      to_lm &lt;- mutate_all(to_lm,funs(if_else(.!=0,1,.))) %&gt;% mutate(outcome=combined_data$outcome)
      
      #No post-stratification
      # I don&#39;t estimate a constituent term for int_coef because it is assumed to be zero
      
      results &lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +
                      parliament + permit.export + permit.import + permit.reg + president + 
                      int_coef:military,data=to_lm)
      
      results_clust &lt;- cluster.vcov(results,cluster = combined_data$respondent)
      pvals_adj &lt;- coeftest(results,vcov.=results_clust)[-1,4]&lt;0.05
      pvals_orig &lt;- coeftest(results)[-1,4]&lt;0.05
      
      total_sig_orig &lt;- mean(pvals_orig)
      total_sig_adj &lt;- mean(pvals_adj)
      
      int_sig_orig &lt;- pvals_orig[&#39;military:int_coef&#39;]
      int_sig_adj &lt;- pvals_adj[&#39;military:int_coef&#39;]
      
      
      # Now run the poststratification model
      
      results_ps &lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +
                      parliament + permit.export + permit.import + permit.reg + president + 
                      int_coef:military + Q,data=to_lm)
      
      results_clust &lt;- cluster.vcov(results,cluster = combined_data$respondent)
      pvals_adj &lt;- coeftest(results_ps,vcov.=results_clust)[-1,4]&lt;0.05
      pvals_orig &lt;- coeftest(results_ps)[-1,4]&lt;0.05
      
      total_sig_orig_blocker &lt;- mean(pvals_orig)
      total_sig_adj_blocker &lt;- mean(pvals_adj)
      
      int_sig_orig_blocker &lt;- pvals_orig[&#39;military:int_coef&#39;]
      int_sig_adj_blocker &lt;- pvals_adj[&#39;military:int_coef&#39;]
      
      out_results &lt;- data_frame(int_sig_adj,int_sig_orig,int_sig_adj_blocker,int_sig_orig_blocker,
                                total_sig_adj,total_sig_orig,total_sig_adj_blocker,
                                total_sig_orig_blocker,abs_true_effect=abs(true_effect),
                                true_effect=true_effect,
                                est_effect=coef(results)[&#39;military:int_coef&#39;],
                                est_effect_ps=coef(results)[&#39;military:int_coef&#39;])
    })
    out_data &lt;- bind_rows(out_data)
    
    return(out_data)
  },mc.cores=parallel::detectCores(),mc.preschedule=FALSE)
  #save the data for inspection
  
  all_sims_data &lt;- bind_rows(all_sims) %&gt;% mutate(sample_size=rep(sampled_seq,each=n_sims),
                                                  iter=rep(1:n_sims,times=num_breaks))

}

if(run_sim==TRUE) {
saveRDS(object = all_sims_data,file=&#39;all_sims_data.rds&#39;)
} else {
  all_sims_data &lt;- readRDS(&#39;all_sims_data.rds&#39;)
}</code></pre>
<p>This simulation yields a row with the significant effect of the interaction term for that simulation for a total of <code>n_sims</code> draws. From this raw data I am able to calculate all of the necessary statistics mentioned above.</p>
<pre class="r"><code># add in different calculations

all_sims_data &lt;- group_by(all_sims_data,sample_size)  %&gt;% mutate(sigeffVorig=ifelse(int_sig_orig,
                                                                                     est_effect,
                                                                                     NA),
sigeffVadj=ifelse(int_sig_adj,est_effect,NA),
sigeffVps_orig=ifelse(int_sig_orig_blocker,est_effect_ps,NA),
sigeffVps_adj=ifelse(int_sig_adj_blocker,est_effect_ps,NA),
powerVorig=int_sig_orig &amp; (true_effect&gt;0),
powerVadj=int_sig_adj &amp; (true_effect&gt;0),
powerVps_orig=int_sig_orig_blocker &amp; (true_effect &gt; 0),
powerVps_adj=int_sig_adj_blocker &amp; (true_effect &gt; 0),
SerrVorig=ifelse(int_sig_orig,1-(sign(est_effect)==sign(true_effect)),NA),
SerrVadj=ifelse(int_sig_adj,1-(sign(est_effect)==sign(true_effect)),NA),
SerrVps_orig=ifelse(int_sig_orig_blocker,
                    1-(sign(est_effect_ps)==sign(true_effect)),NA),
SerrVps_adj=ifelse(int_sig_adj_blocker,
                   1-(sign(est_effect_ps)==sign(true_effect)),NA),
MerrVorig=ifelse(int_sig_orig,abs(est_effect)/abs_true_effect,NA),
MerrVadj=ifelse(int_sig_adj,abs(est_effect)/abs_true_effect,NA),
MerrVps_orig=ifelse(int_sig_orig_blocker,abs(est_effect_ps)/abs_true_effect,NA),
MerrVps_adj=ifelse(int_sig_adj_blocker,abs(est_effect_ps)/abs_true_effect,NA))

long_data &lt;- select(all_sims_data,matches(&#39;V|sample|iter&#39;)) %&gt;% gather(effect_type,result,-sample_size,-iter) %&gt;% separate(effect_type,into=c(&#39;estimate&#39;,&#39;estimation&#39;),sep=&#39;V&#39;) %&gt;% 
  mutate(estimate=factor(estimate,levels=c(&#39;sigeff&#39;,&#39;power&#39;,&#39;Serr&#39;,&#39;Merr&#39;),
                         labels=c(&#39;Mean\nSignificant\nEffect&#39;,
                                  &#39;Mean\nPower&#39;,
                                  &#39;S-Error\nRate&#39;,
                                  &#39;M-Error\nRate&#39;)),
         estimation=factor(estimation,levels=c(&#39;adj&#39;,&#39;orig&#39;,&#39;ps_adj&#39;,&#39;ps_orig&#39;),
                           labels=c(&#39;No Post-Stratification\nClustered Errors\n&#39;,
                                    &#39;No Post-Stratification\nUn-clustered Errors\n&#39;,
                                    &#39;Post-Stratification\nClustered Errors\n&#39;,
                                    &#39;Post-Stratification\nUn-clustered Errors\n&#39;)))

long_data_treatment &lt;- select(all_sims_data,matches(&#39;total|iter|sample&#39;)) %&gt;% gather(effect_type,result,-sample_size,-iter) %&gt;%
mutate(effect_type=factor(effect_type,levels=c(&#39;total_sig_adj&#39;,
                                               &#39;total_sig_orig&#39;,
                                               &#39;total_sig_adj_blocker&#39;,
                                               &#39;total_sig_orig_blocker&#39;),
                          labels=c(&#39;No Post-Stratification\nClustered Errors\n&#39;,
                                   &#39;No Post-Stratification\nUn-clustered Errors\n&#39;,
                                   &#39;Post-Stratification\nClustered Errors\n&#39;,
                                   &#39;Post-Stratification\nUn-clustered Errors\n&#39;)))



# Plot a sample of the data (too big to display all of it)

long_data %&gt;% ungroup %&gt;% 
  slice(1:10) %&gt;% 
  select(-estimation) %&gt;% 
  mutate(estimate=str_replace(estimate,&quot;\\n&quot;,&quot; &quot;)) %&gt;% 
  knitr::kable(.) %&gt;% 
  kable_styling(font_size = 8)</code></pre>
<table class="table" style="font-size: 8px; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:right;">
sample_size
</th>
<th style="text-align:right;">
iter
</th>
<th style="text-align:left;">
estimate
</th>
<th style="text-align:right;">
result
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
0.6298429
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
0.3874088
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
1.1438379
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
0.5653086
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
1.2689594
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
NA
</td>
</tr>
<tr>
<td style="text-align:right;">
1334.783
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
Mean Significant Effect
</td>
<td style="text-align:right;">
NA
</td>
</tr>
</tbody>
</table>
</div>
<div id="plotting" class="section level2">
<h2>Plotting</h2>
<p>I use the <code>gam</code> function in the ggplot2 package to plot a smoothed regression line of the simulation draws for each sample size.</p>
<p>First we can look at the difference that clustered errors makes across the different statistics. The only noticeable differences are at sample sizes smaller than 500. Clustering on respondents tends to result in smaller average significant effects, but it also results in increases in sign errors. This finding differs from the literature that considers clustering important to control for intra-respondent correlation, which in this simulation was fixed at 0.5. At sample sizes larger than 500, there does not appear to be any difference between clustered and un-clustered estimates.</p>
<pre class="r"><code>g_title &lt;- guide_legend(title=&#39;&#39;)
filter(long_data,grepl(&#39;No Post&#39;,estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&#39;red&#39;) +
  xlab(&#39;Sample Size&#39;) + ylab(&quot;&quot;) +
  facet_wrap(~estimate,scales=&#39;free&#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&#39;Accent&#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &#39;bottom&#39;)</code></pre>
<p><img src="/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&#39;clust_err.png&#39;,units=&#39;in&#39;,width=6)</code></pre>
<p>Next I look at post-stratification as an option to improve the precision of estimates. For unclustered errors reported below, post-stratified estimates do have higher power and slightly lower average significant effects, and importantly, the post-stratified estimates worsen neither type S nor type M errors.</p>
<pre class="r"><code>g_title &lt;- guide_legend(title=&#39;&#39;)
filter(long_data,grepl(&#39;Un-clustered&#39;,estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&#39;red&#39;) +
  xlab(&#39;Sample Size&#39;) + ylab(&quot;&quot;) +
  facet_wrap(~estimate,scales=&#39;free&#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&#39;Accent&#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &#39;bottom&#39;)</code></pre>
<p><img src="/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&#39;post_unclust_err.png&#39;,units=&#39;in&#39;,width=6)</code></pre>
<p>Post-stratification appears to have a similar effect on clustered error estimations, although the differences are smaller. In smaller samples, post-stratified estimates do have smaller M-errors.</p>
<pre class="r"><code>g_title &lt;- guide_legend(title=&#39;&#39;)
filter(long_data,!grepl(&#39;Un-clustered&#39;,estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +
  theme_minimal() + stat_smooth(colour=&#39;red&#39;) +
  xlab(&#39;Sample Size&#39;) + ylab(&quot;&quot;) +
  facet_wrap(~estimate,scales=&#39;free&#39;) + theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&#39;Accent&#39;) + guides(colour=g_title,linetype=g_title) +
  theme(legend.position = &#39;bottom&#39;)</code></pre>
<p><img src="/post/Conjoint_Power_Simulation_files/figure-html/unclustered_plot-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&#39;post_clust_err.png&#39;,units=&#39;in&#39;,width=6)</code></pre>
<p>Finally, I also report average numbers of significant coefficients for the 14 treatments. Given that the 14 treatments were sampled from a normal distribution with prior density in the positive values with a meaan of 0.5, in expectation 95% of estimates should be statisticall significant. While that upper limit is reached only in high sample numbers, it looks like the ratio for treatment effects reaches an acceptable level of 70 percent at about 500 sample respondents. Also, post-stratifying un-clustered models results in effects that are reported as significant at much higher rates, as would follow from the previous results about post-stratification.</p>
<pre class="r"><code>g_title &lt;- guide_legend(title=&#39;&#39;)
ggplot(long_data_treatment,aes(y=result,x=sample_size,linetype=effect_type,colour=effect_type)) +
  theme_minimal() + stat_smooth() +
  xlab(&#39;Sample Size&#39;) + ylab(&quot;&quot;) +
theme(panel.grid.minor.y = element_blank(),
                                              panel.grid.major.y = element_blank()) +
  scale_color_brewer(palette=&#39;Dark2&#39;) + guides(linetype=g_title,colour=g_title) +
  theme(legend.position = &#39;bottom&#39;)</code></pre>
<p><img src="/post/Conjoint_Power_Simulation_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>ggsave(&#39;all_treat_rate.png&#39;,units=&#39;in&#39;,width=6)</code></pre>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>This simulation study shows that a sample size of approximately 1,000 respondents is enough to obtain high power while also lowering both the S and M-error rates for treatment interaction effects in this conjoint experiment. The treatment effects themselves are generally of high quality once the sample size reaches 500 because the total number of respondents in each treatment cell is considerably higher than in an interaction. Post-stratification appears to be a useful strategy to increase precision without inducing S or M errors; at the very least, post-stratification does not appear to have any adverse effects on the estimation.</p>
<p>On the other hand, it appears that clustering errors increases the S-error rate at small sample sizes, a surprising finding considering that clustering methods are designed to inflate, not deflate, standard errors. Given that the S-error rate reveals the likelihood of making an error about the sign of the treatment effect, this is a potentially serious problem. For that reason I intend to report both clustered and un-clustered estimates in my analysis.</p>
</div>

    </div>
  </div>

</article>

<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="http://www.robertkubinec.com/post/stancon2018_paper/"><span
      aria-hidden="true">&larr;</span> idealstan: an R Package for Ideal Point Modeling with Stan</a></li>
    

    
    <li class="next"><a href="http://www.robertkubinec.com/post/time_series/">idealstan Vignette: Time-Varying Ideal Points <span
      aria-hidden="true">&rarr;</span></a></li>
    
  </ul>
</nav>

</div>

<div class="article-container">
  <div id="disqus_thread"></div>
<script>
(function() {
var d = document, s = d.createElement('script');
s.src = 'https://robertkubinec.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2018 Robert Kubinec &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="/js/jquery-1.12.3.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/isotope.pkgd.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.1/imagesloaded.pkgd.min.js"></script>
    <script src="/js/hugo-academic.js"></script>
    

    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-67302886-1', 'auto');
        ga('send', 'pageview');

         
        var links = document.querySelectorAll('a');
        Array.prototype.map.call(links, function(item) {
            if (item.host != document.location.host) {
                item.addEventListener('click', function() {
                    var action = item.getAttribute('data-action') || 'follow';
                    ga('send', 'event', 'outbound', action, item.href);
                });
            }
        });
    </script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
    </script>
    <script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
    

  </body>
</html>

