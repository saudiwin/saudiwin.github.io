---
title: "Why People Are Doubting the AstraZeneca Vaccine Report"
subtitle: "Modeling the Garden of Forking Paths with Simulations"
author: "Robert Kubinec"
date: "2020-11-27T15:00:00"
output: blogdown::html_page
bibliography: BibTexDatabase.bib
header:
  title: "Why People Are Doubting the AstraZeneca Vaccine Report"
  summary: "People are very skeptical of AstraZeneca's attempt to spin the results of their vaccine trial to be more than 90 percent effective. I demonstrate in this blog post why skepticism of post hoc analyses is warranted."
  image: "headers/ft_header.png"
  date: "2020-11-19T15:00:00"
tags: ["R","Causal Inference", "COVID-19","Data Science"]
categories: ["R"]
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In this blog post, I use Gelman and Loken’s <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf">garden of forking paths</a> analysis to construct a simulation showing why skepticism of <a href="https://www.astrazeneca.com/media-centre/press-releases/2020/azd1222hlr.html">AstraZeneca’s vaccine results</a> is warranted at this early stage. This simulation uses the numbers from their press release to illustrate how noise in treatment regimes can undermine serendipitous results. As Gelman and Loken describe, researchers can stumble on conclusions which they are then very able to provide post hoc justifications for. Unfortunately, when incentives to get results are strong, human beings are also very likely to focus on the positive at the expense of obtaining the full picture.</p>
<p>Similar to my <a href="http://www.robertkubinec.com/post/vaccinepval/">last post on Pfizer’s vaccine</a>, I start with <a href="https://rpubs.com/ericnovik/692460">Eric Novik’s excellent blog post</a> on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:</p>
<p><span class="math display">\[
VE = 1 - \frac{p_t}{p_c}
\]</span></p>
<p>Where <span class="math inline">\(p_t\)</span> is the proportion of cases in the treatment (vaccinated) group with COVID-19 and <span class="math inline">\(p_c\)</span> is the proportion of cases in the control (un-vaccinated) group). In essence, if we assume the vaccinated group will have no more cases than the control group, this statistic will converge to 1 as <span class="math inline">\(p_t\)</span> goes to zero, so VE of 100% would be a case in which there are no cases in the treatment group.</p>
<p>They don’t give us all the information to figure out how many people were infected with COVID-19 in treatment versus control, but we can infer <span class="math inline">\(p_t\)</span> and <span class="math inline">\(p_c\)</span> by solving two equations given the fact that we know that the proportion infected in the whole trial was equal to <span class="math inline">\(\frac{131}{11636}\)</span> and the overall VE was 0.7:</p>
<p><span class="math display">\[\begin{align}
1 - \frac{p_t}{p_c} &amp;= 0.7\\
p_t + p_c &amp;= \frac{131}{11636}
\end{align}\]</span></p>
<p>Thankfully, this system has a single solution where <span class="math inline">\(p_t\)</span> is equal to  (~0.0026) and <span class="math inline">\(p_c\)</span> is equal to  (~0.0087). In other words, the proportion infected in the treatment group was about four times less than the control group. We’ll assume that these distributions are the correct ones, and then sample subgroup-varying <span class="math inline">\(p_{ti}\)</span> and <span class="math inline">\(p_{ci}\)</span> from Beta distributions that are fairly tight around these values. This will allow us to model treatment heterogeneity within the sample. The distribution of possible VEs given subgroup heterogeneity can be seen in the plot below:</p>
<pre class="r"><code># use the mean/variance parameterization of the beta distribution

pt &lt;- 393/151268
pc &lt;- 655/75634

# Generate values for VE given beta distributions for pt/pc

VE &lt;-  1 - rbeta(10000,pt*5000,(1-pt)*5000)/rbeta(10000,pc*5000,(1-pc)*5000)

hist(VE)</code></pre>
<p><img src="/post/astrazeneca_files/figure-html/veheg-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This plot shows that the true VE in the population is on average 0.7 but could vary substantially. It could be below 0.5 for a small subset of the population and above 0.9 for a small subset of the population. To simulate our data, we will draw VEs from this beta distribution separately for each of four possible subgroups <span class="math inline">\(i \in \{1, 2, 3, 4\}\)</span> in a hypothetical study:</p>
<p><span class="math display">\[
VE_i \sim 1 - \frac{Beta(5000p_{ti},5000(1-p_{ti}))}{Beta(5000p_{ci},5000(1-p_{ci}))}
\]</span></p>
<p>Four subgroups are chosen to be roughly equal to the size of the half-dose group in the AstraZeneca study given a sample size of 11,636. I add a further step in that I assume that the probability of VE being reported for a subgroup <span class="math inline">\(i\)</span>, which I term <span class="math inline">\(Pr(r=1)\)</span>, is increasing in the rate of VE. That is, as VE rises, the subgroup analysis is more likely to be reported. For these reasons, we can distinguish between the full range of VE estimates for a subgroup <span class="math inline">\(i\)</span>, <span class="math inline">\(VE_i\)</span>, and the particular subgroup <span class="math inline">\(i\)</span> that is reported, <span class="math inline">\(VE_{ir=1}\)</span>:</p>
<p><span class="math display">\[
Pr(VE_{ir}) = Pr(r|VE_i)VE_{ir=1} + (1 - Pr(r|VE_i))VE_{ir=0}
\]</span></p>
<p>This equation shows that the extent of the bias in using only reported results is equal to <span class="math inline">\((1 - Pr(r|VE_i))VE_{ir=0}\)</span>, or the probability that a result won’t be reported times the level of VE in the non-reported subgroups.</p>
<p>The R code to generate this model in terms of treatment/control COVID cases and whether or not subgroup analyses are reported is as follows:</p>
<pre class="r"><code># number of samples

N &lt;- 1000

sim_data &lt;- lapply(1:N,function(n) {
  
  # generate subgroup treatment/control COVID proportions
  
  sub_pt &lt;- rbeta(4,pt*5000,(1-pt)*5000)
  sub_pc &lt;- rbeta(4,pc*5000,(1-pc)*5000)
  
  
  # generate subgroup VEs
  
  VE &lt;-  1 - sub_pt/sub_pc
  
  # generate COVID case data with binomial distribution
  
  covid_t &lt;- sapply(sub_pt, function(pt) rbinom(n=1,size = floor(11636/4),prob=pt))
  covid_c &lt;- sapply(sub_pc, function(pc) rbinom(n=1,size = floor(11636/4),prob=pc))
  
  # output data
  
  tibble(draw=n,
         groups=1:4,
         sub_pt=sub_pt,
         sub_pc=sub_pc,
         VE=VE,
         VE_r=as.numeric(runif(n=4)&lt;plogis(-250 + 300*VE)),
         covid_t = covid_t,
         covid_c=covid_c)
  
}) %&gt;% bind_rows</code></pre>
<p>Given this simulation, only a minority (4%) of subgroup analyses are reported. The average VE for the reported subgroups is 77% as opposed to a VE of 69% across all simulations. As such, the simulation assumes that it is unlikely that subgroups with low vaccine efficacy will end up being reported, while subgroups with stronger efficacy are more likely to be reported.</p>
<p>For each random draw from the simulation, I can then fit a model that analyses only those analyses that are reported:</p>
<pre class="r"><code># modified code from Eric Novik

vac_model &lt;- &quot;data {
  int g; // number of sub-groups
  int&lt;lower=0&gt; r_c[g]; // num events, control
  int&lt;lower=0&gt; r_t[g]; // num events, treatment
  int&lt;lower=1&gt; n_c[g]; // num cases, control
  int&lt;lower=1&gt; n_t[g]; // num cases, treatment
}
parameters {
  vector&lt;lower=0, upper=1&gt;[g] p_c; // binomial p for control
  vector&lt;lower=0, upper=1&gt;[g] p_t; // binomial p for treatment 
}
transformed parameters {
  real VE       = mean(1 - p_t ./ p_c);  // average vaccine effectiveness across groups
}
model {
  p_t ~ beta(2,2); // weakly informative,
  p_c ~ beta(2,2); // centered around no effect
  r_c ~ binomial(n_c, p_c); // likelihood for control
  r_t ~ binomial(n_t, p_t); // likelihood for treatment
}
generated quantities {
  vector[g] effect   = p_t - p_c;      // treatment effect
  vector[g] log_odds = log(p_t ./ (1 - p_t)) -
                  log(p_c ./ (1 - p_c));
}&quot;

to_stan &lt;- cmdstan_model(write_stan_file(vac_model))



est_bias &lt;- lapply(unique(sim_data$draw), function(i) {
  
  sink(&quot;output.txt&quot;)
  
  this_data &lt;- filter(sim_data,draw==i,VE_r==1)
  
  stan_data &lt;- list(g=length(unique(this_data$groups)),
                    r_c=as.array(this_data$covid_c),
                    r_t=as.array(this_data$covid_t),
                  n_c=as.array(round(rep(floor(11636/4),
                                length(unique(this_data$groups)))/2)),
                  n_t=as.array(round(rep(floor(11636/4),
                                length(unique(this_data$groups)))/2)))
  
  if(stan_data$g&lt;1) {
    tibble(draw=i,
           VE=NA)
     
  } else {
    est_mod &lt;- to_stan$sample(data=stan_data,
                              seed=624018,
                              chains=1,iter_warmup=500,iter_sampling=1000,refresh=500)
  
    draws &lt;- est_mod$draws() %&gt;% as_draws_df
  
    tibble(draw=i,
         VE=draws$VE)
  }
  
  sink()
  
}) %&gt;% bind_rows</code></pre>
<p>I can then plot the density of the estimated reported VE from this simulation along with a line indicating the true average VE in the population:</p>
<pre class="r"><code>est_bias %&gt;% 
  ggplot(aes(x=VE)) +
  geom_density(fill=&quot;blue&quot;,alpha=0.5) +
  geom_vline(xintercept=mean(sim_data$VE),linetype=2) +
  theme_tufte() +
  xlim(c(0,1)) +
  xlab(&quot;Reported VE&quot;) +
  ylab(&quot;&quot;) +
  annotate(&quot;text&quot;,x=mean(sim_data$VE),y=3,label=paste0(&quot;True VE = &quot;,round(mean(sim_data$VE)*100,1),&quot;%&quot;))</code></pre>
<p><img src="/post/astrazeneca_files/figure-html/sample-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>As can be seen, it is much more likely that VEs higher than the true average VE will be reported. The extent of the bias is driven by the simulation and how much weight the simulation puts on discovering high VE values. Given the profit that AstraZeneca stands to gain, and the fame and prestige for the involved academics, it would seem logical that reported analyses from subgroups will tend to be subgroups that out-perform the average.</p>
<p>Note that this simulation shows how AstraZeneca could be reporting valid statistical results, yet these results can still be a biased estimate of what we want to know, which is how the vaccine works for the population as a whole. The possibility of random noise in subgroups and the fact that subgroups are likely to respond differently means that we should be skeptical of analyses that only report certain subgroups instead of all subgroups. Only when we understand the variability in subgroups can we say whether the reported finding in AstraZeneca’s press release represents a real break-through or simply luck. Of course, they can test it directly by doing another trial, which it seems to be is their intention. Serendipity, though, isn’t enough of a reason to trust these results unless we can examine all of their data.</p>
