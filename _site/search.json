[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Punk Rock Poli Sci",
    "section": "",
    "text": "I am a political economist at the University of South Carolina where I study broken political and economic institutions in the non-Western world. This website hosts papers, code and data from my research projects and also my blog, Punk Rock Poli Sci, that mixes poli sci (the politics of economic development) with a splash of punk rock (rethinking methods for scientific research in the 21st century). You can find my CV here.\nI am the author of two books:\nMaking Democracy Safe for Business: Corporate Politics During the Arab Uprisings\nThe Bayesian Hitman\nYou can subscribe to get email notifications about new blog posts here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLost in Transformation: The Horror and Wonder of Logit\n\n\nOnce You See What Logit Is, You’ll Love It Too\n\n\n\nR\n\n\nStatistics\n\n\nProportions\n\n\n\nThe logit model for binary outcomes is poorly understood because it involves thinking in non-linear spaces. In this post, I illustrate what logit is using maps–and why people who resist logit are a bit like flat earthers.\n\n\n\n\n\nApr 25, 2024\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nNew Survey Research on Tunisia and the IMF Deal\n\n\nTunisian Public More Open to the IMF than Its Leaders\n\n\n\nPolitical Economy\n\n\nInternational Monetary Fund\n\n\nTunisia\n\n\n\nTunisia’s leaders remain adamantly opposed to negotiating or supporting a deal with the IMF, but Tunisian citizens have more nuanced views on the topic.\n\n\n\n\n\nSep 25, 2023\n\n\nRobert Kubinec and Sujeong Shim\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Do Tunisians Really Think About President Kais Saied?\n\n\n\n\n\n\nR\n\n\nTunisia\n\n\nDemocracy\n\n\n\nI reproduce results from our recent survey experiment showing that people are opposed to President Kais Saied but prefer not to report their opposition directly.\n\n\n\n\n\nAug 16, 2023\n\n\nRobert Kubinec and Amr Yakout\n\n\n\n\n\n\n\n\n\n\n\n\nFixing Fractional Logit?\n\n\n\n\n\n\nR\n\n\nLimited Dependent Variables\n\n\nBeta Regression\n\n\n\nThe continuous Bernoulli model is a proposed alternative to the well-known fractional logit specification. I discuss in this blog post when that might or might not be the case.\n\n\n\n\n\nFeb 14, 2023\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s Logs Got to Do With It?\n\n\n\n\n\n\nR\n\n\nElections\n\n\nBeta Regression\n\n\n\nOrdered beta regression can give you comparable, scale-free ATEs that can still be understood in the scale of the original data–all without using logs.\n\n\n\n\n\nDec 15, 2022\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Turnout in Tunisia’s Constitutional Referendum\n\n\nHow much can we learn about true turnout from poll observing?\n\n\n\nR\n\n\nElections\n\n\nSimulations\n\n\nSampling\n\n\n\nI examine how well examining turnout from polling stations is likely to work in estimating turnout for Tunisia’s constitution, especially if we have trouble trusting the numbers of voters reported by the eletion commission.\n\n\n\n\n\nJul 25, 2022\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nWhat To Do (And Not to Do) with Modeling Proportions/Fractional Outcomes\n\n\nPresenting my new R package ordbetareg for bounded dependent variables.\n\n\n\nR\n\n\nProportions\n\n\nBeta Regression\n\n\n\nI discuss the available approaches in R for handling proportional/fractional outcomes, and present my new R package ordbetareg as a synthesis of the best of what is currently available.\n\n\n\n\n\nJun 26, 2022\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nEnergy Nationalism To the Rescue\n\n\nPutting Trade Barriers on Foreign Oil Could Help the Climate\n\n\n\nPolitical Economy\n\n\nClimate Change\n\n\n\n\n\n\n\n\n\nApr 2, 2022\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nThe Causal Representation of Panel Data: A Comment On Xu (2022)\n\n\nWhy Intercepts (Fixed Effects) Aren’t Latent Confounding Variables\n\n\n\nR\n\n\nFixed Effects\n\n\nPanel Data\n\n\nData Science\n\n\n\nI clarify the limitations of the current representation of panel data in causal frameworks and why these limitations lead to confusion about what causal methods can do with panel data.\n\n\n\n\n\nFeb 3, 2022\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nWhich Religious Groups Have the Most Sex?\n\n\nThere’s a lot of discussion about how religion affects people’s sex lives. But how much sex on average do different religious groups have?\n\n\n\nR\n\n\nSexuality\n\n\nOpinion Polling\n\n\nData Science\n\n\n\nHow much sex on average do different religious groups have? The findings might (or might not) surprise you.\n\n\n\n\n\nJun 21, 2021\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nWhy People Are Doubting the AstraZeneca Vaccine Report\n\n\nModeling the Garden of Forking Paths with Simulations\n\n\n\nR\n\n\nCausal Inference\n\n\nCOVID-19\n\n\nData Science\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nA More Realistic P-Value for the Pfizer Vaccine Report\n\n\nModeling the Vaccine Study Using the Beta-Binomial Distribution\n\n\n\nR\n\n\nCausal Inferece\n\n\nCOVID-19\n\n\nData Science\n\n\n\nThe clinical pre-registration for the Moderna vaccine defines clearly what statistic we should use to evaluate their trial, in contrast to their press release.\n\n\n\n\n\nNov 19, 2020\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nWhy You Should Be Careful About the MaterniT 21 Test\n\n\nNon-invasive screenings for Down’s syndrome have low accuracy for many women, but you won’t hear that from the test companies.\n\n\n\nR\n\n\nPublic Health\n\n\nMeta-analysis\n\n\nData Science\n\n\n\nIn this post I discuss the very popular MaterniT 21 test used to identify fetuses with Downe’s Syndrome and other genetic conditions, and show why it can’t really be trusted for women under 40.\n\n\n\n\n\nSep 13, 2020\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Panel Data Is Really All About\n\n\nIt’s not as confusing as it first appears, but it’s also frequently misused and poorly understood.\n\n\n\nR\n\n\nPanel Data\n\n\nFixed Effects\n\n\nData Science\n\n\n\nIn this post I talk about my recent publication with Jonathan Kropko about panel data. We de-mistify panel data, expose some serious weaknesses in common models like two-way fixed effects (2-way FE), and in general help people use panel data with (more) confidence.\n\n\n\n\n\nApr 22, 2020\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nA Proposed Model for Partial Identification of SARS-CoV2 Infection Rates Given Observed Tests and Cases\n\n\nHelping us understand what the observed numbers of cases and tests mean for COVID-19 spread.\n\n\n\nR\n\n\nMeasurement\n\n\nBayesian\n\n\nData Science\n\n\n\nA model that shows how to estimate the effect of suppression measures on the SARS-CoV2 without knowing the true infection rate.\n\n\n\n\n\nMar 28, 2020\n\n\nRobert Kubinec\n\n\n\n\n\n\n\n\n\n\n\n\nSimulating Conjoint Survey Experiments\n\n\nPower Curves, Clustered Errors, Type S and Type M Error Rates\n\n\n\nR\n\n\nExperiments\n\n\nCausal Inference\n\n\nData Science\n\n\n\nR code that shows how to simulate a conjoint experiment using Monte Carlo techniques to produce power curves and test for the effect of clustering in standard errors.\n\n\n\n\n\nFeb 11, 2019\n\n\nRobert Kubinec\n\n\n\n\n\n\nNo matching items",
    "crumbs": [
      "Punk Rock Poli Sci"
    ]
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "About this blog",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "posts/astrazeneca/index.html",
    "href": "posts/astrazeneca/index.html",
    "title": "Why People Are Doubting the AstraZeneca Vaccine Report",
    "section": "",
    "text": "In this blog post, I use Gelman and Loken’s garden of forking paths analysis to construct a simulation showing why skepticism of AstraZeneca’s vaccine results is warranted at this early stage. This simulation uses the numbers from their press release to illustrate how noise in treatment regimes can undermine serendipitous results. As Gelman and Loken describe, researchers can stumble on conclusions which they are then very able to provide post hoc justifications for. Unfortunately, when incentives to get results are strong, human beings are also very likely to focus on the positive at the expense of obtaining the full picture.\nSimilar to my last post on Pfizer’s vaccine, I start with Eric Novik’s excellent blog post on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:\n\\[\nVE = 1 - \\frac{p_t}{p_c}\n\\]\nWhere \\(p_t\\) is the proportion of cases in the treatment (vaccinated) group with COVID-19 and \\(p_c\\) is the proportion of cases in the control (un-vaccinated) group). In essence, if we assume the vaccinated group will have no more cases than the control group, this statistic will converge to 1 as \\(p_t\\) goes to zero, so VE of 100% would be a case in which there are no cases in the treatment group.\nThey don’t give us all the information to figure out how many people were infected with COVID-19 in treatment versus control, but we can infer \\(p_t\\) and \\(p_c\\) by solving two equations given the fact that we know that the proportion infected in the whole trial was equal to \\(\\frac{131}{11636}\\) and the overall VE was 0.7:\n\\[\\begin{align}\n1 - \\frac{p_t}{p_c} &= 0.7\\\\\np_t + p_c &= \\frac{131}{11636}\n\\end{align}\\]\nThankfully, this system has a single solution where \\(p_t\\) is equal to (~0.0026) and \\(p_c\\) is equal to (~0.0087). In other words, the proportion infected in the treatment group was about four times less than the control group. We’ll assume that these distributions are the correct ones, and then sample subgroup-varying \\(p_{ti}\\) and \\(p_{ci}\\) from Beta distributions that are fairly tight around these values. This will allow us to model treatment heterogeneity within the sample. The distribution of possible VEs given subgroup heterogeneity can be seen in the plot below:\n\n# use the mean/variance parameterization of the beta distribution\n\npt &lt;- 393/151268\npc &lt;- 655/75634\n\n# Generate values for VE given beta distributions for pt/pc\n\nVE &lt;-  1 - rbeta(10000,pt*5000,(1-pt)*5000)/rbeta(10000,pc*5000,(1-pc)*5000)\n\nhist(VE)\n\n\n\n\n\n\n\n\nThis plot shows that the true VE in the population is on average 0.7 but could vary substantially. It could be below 0.5 for a small subset of the population and above 0.9 for a small subset of the population. To simulate our data, we will draw VEs from this beta distribution separately for each of four possible subgroups \\(i \\in \\{1, 2, 3, 4\\}\\) in a hypothetical study:\n\\[\nVE_i \\sim 1 - \\frac{Beta(5000p_{ti},5000(1-p_{ti}))}{Beta(5000p_{ci},5000(1-p_{ci}))}\n\\]\nFour subgroups are chosen to be roughly equal to the size of the half-dose group in the AstraZeneca study given a sample size of 11,636. I add a further step in that I assume that the probability of VE being reported for a subgroup \\(i\\), which I term \\(Pr(r=1)\\), is increasing in the rate of VE. That is, as VE rises, the subgroup analysis is more likely to be reported. For these reasons, we can distinguish between the full range of VE estimates for a subgroup \\(i\\), \\(VE_i\\), and the particular subgroup \\(i\\) that is reported, \\(VE_{ir=1}\\):\n\\[\nPr(VE_{ir}) = Pr(r|VE_i)VE_{ir=1} + (1 - Pr(r|VE_i))VE_{ir=0}\n\\]\nThis equation shows that the extent of the bias in using only reported results is equal to \\((1 - Pr(r|VE_i))VE_{ir=0}\\), or the probability that a result won’t be reported times the level of VE in the non-reported subgroups.\nThe R code to generate this model in terms of treatment/control COVID cases and whether or not subgroup analyses are reported is as follows:\n\n# number of samples\n\nN &lt;- 1000\n\nsim_data &lt;- lapply(1:N,function(n) {\n  \n  # generate subgroup treatment/control COVID proportions\n  \n  sub_pt &lt;- rbeta(4,pt*5000,(1-pt)*5000)\n  sub_pc &lt;- rbeta(4,pc*5000,(1-pc)*5000)\n  \n  \n  # generate subgroup VEs\n  \n  VE &lt;-  1 - sub_pt/sub_pc\n  \n  # generate COVID case data with binomial distribution\n  \n  covid_t &lt;- sapply(sub_pt, function(pt) rbinom(n=1,size = floor(11636/4),prob=pt))\n  covid_c &lt;- sapply(sub_pc, function(pc) rbinom(n=1,size = floor(11636/4),prob=pc))\n  \n  # output data\n  \n  tibble(draw=n,\n         groups=1:4,\n         sub_pt=sub_pt,\n         sub_pc=sub_pc,\n         VE=VE,\n         VE_r=as.numeric(runif(n=4)&lt;plogis(-250 + 300*VE)),\n         covid_t = covid_t,\n         covid_c=covid_c)\n  \n}) %&gt;% bind_rows\n\nGiven this simulation, only a minority (4%) of subgroup analyses are reported. The average VE for the reported subgroups is 77% as opposed to a VE of 69% across all simulations. As such, the simulation assumes that it is unlikely that subgroups with low vaccine efficacy will end up being reported, while subgroups with stronger efficacy are more likely to be reported.\nFor each random draw from the simulation, I can then fit a model that analyses only those analyses that are reported:\n\n# modified code from Eric Novik\n\nvac_model &lt;- \"data {\n  int g; // number of sub-groups\n  array[g] int&lt;lower=0&gt; r_c; // num events, control\n  array[g] int&lt;lower=0&gt; r_t; // num events, treatment\n  array[g] int&lt;lower=1&gt; n_c; // num cases, control\n  array[g] int&lt;lower=1&gt; n_t; // num cases, treatment\n}\nparameters {\n  array[g] real&lt;lower=0, upper=1&gt; p_c; // binomial p for control\n  array[g] real&lt;lower=0, upper=1&gt; p_t; // binomial p for treatment \n}\ntransformed parameters {\n  real VE = mean(1 - to_vector(p_t) ./ to_vector(p_c));  // average vaccine effectiveness across groups\n}\nmodel {\n  p_t ~ beta(2, 2); // weakly informative,\n  p_c ~ beta(2, 2); // centered around no effect\n  r_c ~ binomial(n_c, p_c); // likelihood for control\n  r_t ~ binomial(n_t, p_t); // likelihood for treatment\n}\ngenerated quantities {\n  vector[g] effect   = to_vector(p_t) - to_vector(p_c);      // treatment effect\n  vector[g] log_odds = log(to_vector(p_t) ./ (1 - to_vector(p_t))) -\n                           log(to_vector(p_c) ./ (1 - to_vector(p_c)));\n}\"\n\nto_stan &lt;- cmdstan_model(write_stan_file(vac_model))\n\n\n\nest_bias &lt;- lapply(unique(sim_data$draw), function(i) {\n  \n  sink(\"output.txt\")\n  \n  this_data &lt;- filter(sim_data,draw==i,VE_r==1)\n  \n  stan_data &lt;- list(g=length(unique(this_data$groups)),\n                    r_c=as.array(this_data$covid_c),\n                    r_t=as.array(this_data$covid_t),\n                  n_c=as.array(round(rep(floor(11636/4),\n                                length(unique(this_data$groups)))/2)),\n                  n_t=as.array(round(rep(floor(11636/4),\n                                length(unique(this_data$groups)))/2)))\n  \n  if(stan_data$g&lt;1) {\n    tibble(draw=i,\n           VE=NA)\n     \n  } else {\n    est_mod &lt;- to_stan$sample(data=stan_data,\n                              seed=624018,\n                              chains=1,iter_warmup=500,iter_sampling=1000,refresh=500)\n  \n    draws &lt;- est_mod$draws() %&gt;% as_draws_df\n  \n    tibble(draw=i,\n         VE=draws$VE)\n  }\n  \n  sink()\n  \n}) %&gt;% bind_rows\n\nI can then plot the density of the estimated reported VE from this simulation along with a line indicating the true average VE in the population:\n\nest_bias %&gt;% \n  ggplot(aes(x=VE)) +\n  geom_density(fill=\"blue\",alpha=0.5) +\n  geom_vline(xintercept=mean(sim_data$VE),linetype=2) +\n  theme_tufte() +\n  xlim(c(0,1)) +\n  xlab(\"Reported VE\") +\n  ylab(\"\") +\n  annotate(\"text\",x=mean(sim_data$VE),y=3,label=paste0(\"True VE = \",round(mean(sim_data$VE)*100,1),\"%\"))\n\n\n\n\n\n\n\n\nAs can be seen, it is much more likely that VEs higher than the true average VE will be reported. The extent of the bias is driven by the simulation and how much weight the simulation puts on discovering high VE values. Given the profit that AstraZeneca stands to gain, and the fame and prestige for the involved academics, it would seem logical that reported analyses from subgroups will tend to be subgroups that out-perform the average.\nNote that this simulation shows how AstraZeneca could be reporting valid statistical results, yet these results can still be a biased estimate of what we want to know, which is how the vaccine works for the population as a whole. The possibility of random noise in subgroups and the fact that subgroups are likely to respond differently means that we should be skeptical of analyses that only report certain subgroups instead of all subgroups. Only when we understand the variability in subgroups can we say whether the reported finding in AstraZeneca’s press release represents a real break-through or simply luck. Of course, they can test it directly by doing another trial, which it seems to be is their intention. Serendipity, though, isn’t enough of a reason to trust these results unless we can examine all of their data."
  },
  {
    "objectID": "posts/energy_nationalism/index.html",
    "href": "posts/energy_nationalism/index.html",
    "title": "Energy Nationalism To the Rescue",
    "section": "",
    "text": "The Russian invasion of the Ukraine has raised a new specter of energy nationalism. Although only the United Kingdom and the United States have gone as far as outright banning Russian oil and gas, other European economies, including Germany, are seriously discussing weaning off of their main energy supplier. This crisis offers politicians an important opportunity to reframe climate change policy by arguing for reduced dependence on foreign oil and pivoting to domestic renewable energy. My research into trade and business politics shows that building trade barriers against oil could cement a new and durable coalition in favor of higher gas prices uniting oil companies, conservationists and realpolitikers.\nThe idea of using trade measures to fight climate change is not new, though oddly enough, rarely implemented. European countries have proposed various sorts of carbon-weighted tariffs, and the World Trade Organization and the World Bank have long discussed liberalizing trade in environmental goods like solar panels. Yet climate change negotiations often take place in isolation from trade deals, leaving the two areas of policy largely unaffected by each other.\nA trade policy that bans foreign energy imports is a potential gold mine for climate action because a ban obscures who is responsible for the cost of higher fuel prices. Voters have proven to be averse to any policies that directly increase the cost of carbon pollution, such as Washington State’s carbon tax ballot initiative failure in 2018. As Matto Mildenberger has shown, it has been very difficult to push through pro-climate policies because they fracture coalitions on both sides of the aisle, bringing together erstwhile enemies like unions and corporations. A ban on foreign energy, on the other hand, seems like an “America first” policy that privileges the domestic economy at the expense of foreigners—even though it will result in higher gas prices in both the short and long term.\nIn the short term, bans of energy imports lead to a surge in higher oil and gas prices as suppliers react to changes in the market. Over the long term, a ban on foreign energy will have the net effect of any tariff: it will permanently raise the domestic price of the good, in this case oil and gas. For the United States, both renewable energy and oil companies stand to benefit, though oil companies will reap the lion’s share of the gains as consumers pay more at the pump. We know that higher gas prices both decrease energy usage and push car buyers towards electric vehicles, ultimately reducing carbon consumption for decades.\nBy favoring domestic energy suppliers, the U.S. can act tough on authoritarian regimes like Russia, raise the price of gasoline to reduce carbon pollution, and secure the support of oil companies who will profit. In other words, we could trap oil companies like the monkey in Aesop’s fable, its hand around a nut inside a jar that it simply will not let go of.\nWe know from recent trade wars that it won’t be easy for businesses hurt by higher fuel prices to mobilize together to defeat energy bans. As part of my research with the Princeton Trade Study on President Trump’s trade war with China, we implemented an online survey experiment of U.S. businesspeople in which we randomly provided some managers with information about how the trade war had affected their industry. We found that conservative-leaning managers thought the trade war helped their firm—even if our data showed it had not–while liberals believed the opposite. The complexities of the impact of tariffs on supply chains meant that businesses found it difficult to know who to blame for rising costs and whether to take politically risky action to oppose the trade war.\nEnergy bans are of course only one part of larger climate reform. But the power of trade restrictions to reshape economies in ways that are politically expedient shows an important way forward for feasible and immediate climate action."
  },
  {
    "objectID": "posts/conjoint/conjoint.html",
    "href": "posts/conjoint/conjoint.html",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "",
    "text": "Conjoint survey experiments have become more popular in political science since the publication of Hainmueller, Hopkins and Yamamoto (2014). However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see SocArchiv Draft). I employ both traditional power measures and newer statistics from Gelman and Carlin (2014) reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).\nThe original Rmarkdown and saved simulation files can be downloaded from the site’s Github.\nThe packages required to run this simulation are listed in the code block below:\n\n#Required packages\nrequire(ggplot2)\nrequire(dplyr)\nrequire(tidyr)\nrequire(multiwayvcov)\nrequire(lmtest)\nrequire(stringr)\nrequire(kableExtra)\n\n# package MASS also used but not loaded\n\n# != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows.\n# to install parallelsugar:\n# install.packages('devtools')\n# library(devtools)\n# install_github('nathanvan/parallelsugar')\n\n# If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used\n\nif(.Platform$OS.type=='windows') {\n  parallelfunc &lt;- parallelsugar::mclapply_socket\n} else {\n  parallelfunc &lt;- parallel::mclapply\n}"
  },
  {
    "objectID": "posts/conjoint/conjoint.html#background",
    "href": "posts/conjoint/conjoint.html#background",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "",
    "text": "Conjoint survey experiments have become more popular in political science since the publication of Hainmueller, Hopkins and Yamamoto (2014). However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see SocArchiv Draft). I employ both traditional power measures and newer statistics from Gelman and Carlin (2014) reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).\nThe original Rmarkdown and saved simulation files can be downloaded from the site’s Github.\nThe packages required to run this simulation are listed in the code block below:\n\n#Required packages\nrequire(ggplot2)\nrequire(dplyr)\nrequire(tidyr)\nrequire(multiwayvcov)\nrequire(lmtest)\nrequire(stringr)\nrequire(kableExtra)\n\n# package MASS also used but not loaded\n\n# != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows.\n# to install parallelsugar:\n# install.packages('devtools')\n# library(devtools)\n# install_github('nathanvan/parallelsugar')\n\n# If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used\n\nif(.Platform$OS.type=='windows') {\n  parallelfunc &lt;- parallelsugar::mclapply_socket\n} else {\n  parallelfunc &lt;- parallel::mclapply\n}"
  },
  {
    "objectID": "posts/conjoint/conjoint.html#simulation-set-up",
    "href": "posts/conjoint/conjoint.html#simulation-set-up",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "Simulation Set-up",
    "text": "Simulation Set-up\nThe following parameters control the range of coefficients tested and the number of simulations. The survey experiment design employs vignettes in which appeals and the actors making appeals are allowed to vary between respondents. Any one vignette has one actor and one appeal. The probability of assignment is assumed to be a simple random fraction of the number of appeal-actor combinations (14). If run_sim is set to TRUE, the simulation is run, otherwise the simulation results are loaded from an RDS file and plotted. Running the simulation will take approximately 6 to 12 hours depending on the number of cores and speed of the CPU.\n\n#Actually run the simulation or just load the data and look at it?\nrun_sim &lt;- FALSE\n# Max number of respondents fixed at 2700\nnum_resp &lt;- 2700\n# Number of iterations (breaks in sample size)\nnum_breaks &lt;- 300\n# Number of simulations to run per iteration\nn_sims &lt;- 1000\n\nI then create a grid of all possible actor-appeal combinations as I am using simple randomization of profiles before presenting them to respondents. There are two vectors of treatments (actors and appeals) that each have 7 separate treatments for a total of 14 separate possible treatments.\n\n# Two treatment variables producing a cross-product of 7x7\ntreatments1 &lt;- c('military','MOI','president','MOJ','parliament','municipality','government')\ntreatments2 &lt;- c('exprop.firm','exprop.income','permit.reg','contracts.supply','permit.export','permit.import','reforms')\ntotal_treat &lt;- length(c(treatments1,treatments2))\ngrid_pair &lt;- as.matrix(expand.grid(treatments1,treatments2))\nprint(head(grid_pair))\n\n     Var1           Var2         \n[1,] \"military\"     \"exprop.firm\"\n[2,] \"MOI\"          \"exprop.firm\"\n[3,] \"president\"    \"exprop.firm\"\n[4,] \"MOJ\"          \"exprop.firm\"\n[5,] \"parliament\"   \"exprop.firm\"\n[6,] \"municipality\" \"exprop.firm\""
  },
  {
    "objectID": "posts/conjoint/conjoint.html#simulation",
    "href": "posts/conjoint/conjoint.html#simulation",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "Simulation",
    "text": "Simulation\nTo simulate the data, I first sample 14 coefficients \\(\\beta_j\\) (one for each treatment \\(J\\)) from a normal distribution with mean zero and standard deviation one. I then randomly sample from two profile combinations for each of the \\(I\\) respondents in accordance with simple random sampling. Two profile combinations, for a total of four tasks \\(T\\), are selected to reflect the fact that paired vignettes will be shown to each respondent as in the study design. I also sample a pre-treatment covariate \\(Z_I\\) that is a random binomial vector with probability of 0.2 (thus 20% of respondents will fall into this cell). A treatment interaction effect \\(\\beta_z\\) is sampled from a normal distribution with mean 0.5 and standard deviation of 0.3 to provide a sampling distribution for the true effect, instead of assuming that the true effect is a fixed population value. Adding a distribution for \\(\\beta_j\\) reflects additional uncertainty beyond standard sampling distribution uncertainty. In this case, it represents additional measurement error between the true concept and the indicators used in the survey design.\nI also post-stratify some estimates with a pre-treatment covariate \\(Q_I\\) from a binomial distribution of probability .5 that has a constant effect on \\(Y_{it}\\) of \\(+1\\) (representing a fixed effect).\nI then randomly sample a pair of outcomes, for a total of four tasks \\(T\\), for \\(Y_{it}\\) in the range of \\([1,10]\\) by drawing a number from a multivariate normal distribution. The mean \\(\\mu_{it}\\) of this normal distribution is equal to a linear model with an intercept of 5, the 14 dummy variables for treatment indicators \\(X_j\\) with associated coefficients \\(\\beta_j\\), the interaction \\(\\beta_z\\) between the pre-treatment covariate \\(Z_i\\) and \\(X_{ij}\\), and a post-stratification covariate \\(Q_i\\). To simplify matters, \\(Z_i\\) is not given its own constituent term as I am not interested in the unconditional effect of \\(Z_i\\) on \\(Y_{it}\\), only the effect of \\(X_{ijt}\\) on \\(Y_{it}\\) given \\(Z_{i}\\). Finally, I draw correlated errors from a multivariate normal distribution with mean of zero and length of 4 (equal to the number of tasks per respondent) to produce a \\(4 \\times 4\\) variance matrix \\(\\varSigma_i\\) with a diagonal of 4 and intra-respondent covariation of 1 (correlation of 0.5).\n\\[\n\\begin{aligned}\nX_{ITJ} &\\sim  \\mathrm{B} \\Big( \\frac{1}{J \\times 2} \\Big)\\\\\nB_{J} &\\sim  \\mathrm{N}(0,2)\\\\\n\\beta_z &\\sim \\mathrm{N}(0.5,0.3)\\\\\nZ_I &\\sim  \\mathrm{B}(0.2)\\\\\nQ_I &\\sim  \\mathrm{B}(0.5)\\\\\n\\mu_{it} &=  5 + \\sum_{j=1}^{J} \\sum_{t=1}^{T} \\beta_j * X_{itj} + \\beta_z * X_{it1} *Z_i + Q_i\\\\\nY_{it} &\\sim  \\mathrm{N}(\\mu_{it},\\varSigma_i)\n\\end{aligned}\n\\]\nThis process will produce some numbers outside the \\([1,10]\\) range; however, it is better to leave these values in as explicit truncation will violate the assumptions of the underlying causal model.\nI run 1000 simulations for each of 300 sequential sample sizes ranging from 100 to 2700. I then take the mean significant effect and report that as the likely significant effect size for that sample size. I also record the ratio of draws for which the effect is significant (the power). However, given that the true effect is not fixed, I interpret power as the ability detect a true effect greater than zero. I record both unadjusted p-values and p-values adjusted using the cluster.vcov function from the multiwayvcov package by clustering around respondent ID to reflect the pairing of vignettes. I also use separate results when post-stratifying on a pre-treatment covariate \\(Q_I\\).\nIn addition, I included M-errors (error of absolute magnitude of significant coefficients) and S-errors (incorrect sign of significant coefficients). M-errors provide an estimate of publication bias given that the \\(p=0.05\\) threshold is a hard boundary and will necessarily result in smaller effects being reported as statistically insignificant when in fact they are greater than zero. S-errors help determine the probability that an estimated effect is the correct sign even if it is significant. S-errors are particularly problematic in small samples when sampling error can produce large negative deviations that may be statistically significant.\n\nif(run_sim==TRUE) {\n  file.create('output_log.txt',showWarnings = FALSE)\n    \n    # Need to randomize over the simulations so that parallelization works correctly on windows\n    \n    sampled_seq &lt;- sample(seq(100,num_resp,length.out = num_breaks))\n    \n  all_sims &lt;- parallelfunc(sampled_seq,function(x) {\n  \n    out_probs &lt;- 1:n_sims\n    cat(paste0(\"Now running simulation on data sample size \",x),file='output_log.txt',sep='\\n',append=TRUE)\n  \n    out_data &lt;- lapply(1:n_sims, function(j) {\n      \n      total_probs &lt;- sapply(1:x,function(x) {\n        treat_rows &lt;- sample(1:nrow(grid_pair),4)\n        treatments_indiv &lt;- c(grid_pair[treat_rows,])\n        return(treatments_indiv)\n      })\n      \n      by_resp &lt;- t(total_probs)\n      by_resp &lt;- as_data_frame(by_resp)\n      names(by_resp) &lt;- c(paste0('actor.',1:4,\"_cluster\",c(1,1,2,2)),paste0('gift.',1:4,\"_cluster\",c(1,1,2,2)))\n      by_resp$respondent &lt;- paste0('Respondent_',1:nrow(by_resp))\n      by_resp &lt;- gather(by_resp,attribute,indicator,-respondent) %&gt;% separate(attribute,into=c('attribute','cluster'),sep='_') %&gt;% \n        separate(attribute,into=c('attribute','task')) %&gt;% spread(attribute,indicator)\n      \n      \n      # Assign true coefficients for treatments\n  \n      #Beta_js\n      \n      coefs &lt;- data_frame(coef_val=rnorm(n=length(c(treatments1,treatments2)),mean=0,sd=1),\n                          treat_label=c(treatments1,treatments2))\n      \n      # Create cluster covariance in the errors\n      \n      sigma_matrix &lt;- matrix(2,nrow=4,ncol=4)\n      diag(sigma_matrix) &lt;- 4\n  \n      # Add on the outcome as a normal draw, treatment coefficients, interaction coefficient, group errors/interaction by respondent\n      \n      by_resp &lt;- gather(by_resp,treatment,appeal_type,actor,gift) %&gt;% \n        left_join(coefs,by=c('appeal_type'='treat_label'))\n      \n      # Record interaction coefficient (true estimate of interest)\n      \n      true_effect &lt;- rnorm(n=1,mean=0.5,sd=0.3)\n      \n      by_resp &lt;- select(by_resp,-treatment) %&gt;% spread(appeal_type,coef_val) %&gt;%  group_by(respondent) %&gt;% mutate(error=MASS::mvrnorm(1,mu=rep(0,4),Sigma=sigma_matrix)) %&gt;% ungroup\n      \n      # interaction coefficient only in function if military==TRUE\n      \n      by_resp &lt;- mutate(by_resp,int_coef=true_effect*rbinom(n = n(),prob = 0.2,size=1),\n                        int_coef=if_else(military!=0,int_coef,0))\n      by_resp &lt;- lapply(by_resp, function(x) {\n        if(is.double(x)) {\n          x[is.na(x)] &lt;- 0\n        }\n        return(x)\n      }) %&gt;% as_data_frame\n      \n      # To make the outcome, need to turn the dataset long\n      # However, we now need to drop the reference categories\n      # Drop one dummy from actor/gift to prevent multicollinearity = reforms + government combination\n      \n      out_var &lt;- gather(by_resp,var_name,var_value,-respondent,-task,-cluster) %&gt;% \n         filter(!(var_name %in% c('reforms','government'))) %&gt;% \n        group_by(respondent,task) %&gt;% summarize(outcome=sum(var_value)+5)\n      \n      combined_data &lt;- left_join(out_var,by_resp,by=c('respondent','task'))\n      \n      \n      # Re-estimate with a blocking variable\n      \n  \n      combined_data$Q &lt;- c(rep(1,floor(nrow(combined_data)/2)),\n                           rep(0,ceiling(nrow(combined_data)/2)))\n      \n      combined_data$outcome &lt;- if_else(combined_data$Q==1,combined_data$outcome+1,\n                                       combined_data$outcome)\n      \n      # # Create data predictor matrix and estimate coefficients from the simulated dataset\n      # \n      to_lm &lt;- ungroup(combined_data) %&gt;% select(contracts.supply:reforms,int_coef,Q)\n      to_lm &lt;- mutate_all(to_lm,funs(if_else(.!=0,1,.))) %&gt;% mutate(outcome=combined_data$outcome)\n      \n      #No post-stratification\n      # I don't estimate a constituent term for int_coef because it is assumed to be zero\n      \n      results &lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +\n                      parliament + permit.export + permit.import + permit.reg + president + \n                      int_coef:military,data=to_lm)\n      \n      results_clust &lt;- cluster.vcov(results,cluster = combined_data$respondent)\n      pvals_adj &lt;- coeftest(results,vcov.=results_clust)[-1,4]&lt;0.05\n      pvals_orig &lt;- coeftest(results)[-1,4]&lt;0.05\n      \n      total_sig_orig &lt;- mean(pvals_orig)\n      total_sig_adj &lt;- mean(pvals_adj)\n      \n      int_sig_orig &lt;- pvals_orig['military:int_coef']\n      int_sig_adj &lt;- pvals_adj['military:int_coef']\n      \n      \n      # Now run the poststratification model\n      \n      results_ps &lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +\n                      parliament + permit.export + permit.import + permit.reg + president + \n                      int_coef:military + Q,data=to_lm)\n      \n      results_clust &lt;- cluster.vcov(results,cluster = combined_data$respondent)\n      pvals_adj &lt;- coeftest(results_ps,vcov.=results_clust)[-1,4]&lt;0.05\n      pvals_orig &lt;- coeftest(results_ps)[-1,4]&lt;0.05\n      \n      total_sig_orig_blocker &lt;- mean(pvals_orig)\n      total_sig_adj_blocker &lt;- mean(pvals_adj)\n      \n      int_sig_orig_blocker &lt;- pvals_orig['military:int_coef']\n      int_sig_adj_blocker &lt;- pvals_adj['military:int_coef']\n      \n      out_results &lt;- data_frame(int_sig_adj,int_sig_orig,int_sig_adj_blocker,int_sig_orig_blocker,\n                                total_sig_adj,total_sig_orig,total_sig_adj_blocker,\n                                total_sig_orig_blocker,abs_true_effect=abs(true_effect),\n                                true_effect=true_effect,\n                                est_effect=coef(results)['military:int_coef'],\n                                est_effect_ps=coef(results)['military:int_coef'])\n    })\n    out_data &lt;- bind_rows(out_data)\n    \n    return(out_data)\n  },mc.cores=parallel::detectCores(),mc.preschedule=FALSE)\n  #save the data for inspection\n  \n  all_sims_data &lt;- bind_rows(all_sims) %&gt;% mutate(sample_size=rep(sampled_seq,each=n_sims),\n                                                  iter=rep(1:n_sims,times=num_breaks))\n\n}\n\nif(run_sim==TRUE) {\nsaveRDS(object = all_sims_data,file='all_sims_data.rds')\n} else {\n  all_sims_data &lt;- readRDS('all_sims_data.rds')\n}\n\nThis simulation yields a row with the significant effect of the interaction term for that simulation for a total of n_sims draws. From this raw data I am able to calculate all of the necessary statistics mentioned above.\n\n# add in different calculations\n\nall_sims_data &lt;- group_by(all_sims_data,sample_size)  %&gt;% mutate(sigeffVorig=ifelse(int_sig_orig,\n                                                                                     est_effect,\n                                                                                     NA),\nsigeffVadj=ifelse(int_sig_adj,est_effect,NA),\nsigeffVps_orig=ifelse(int_sig_orig_blocker,est_effect_ps,NA),\nsigeffVps_adj=ifelse(int_sig_adj_blocker,est_effect_ps,NA),\npowerVorig=int_sig_orig & (true_effect&gt;0),\npowerVadj=int_sig_adj & (true_effect&gt;0),\npowerVps_orig=int_sig_orig_blocker & (true_effect &gt; 0),\npowerVps_adj=int_sig_adj_blocker & (true_effect &gt; 0),\nSerrVorig=ifelse(int_sig_orig,1-(sign(est_effect)==sign(true_effect)),NA),\nSerrVadj=ifelse(int_sig_adj,1-(sign(est_effect)==sign(true_effect)),NA),\nSerrVps_orig=ifelse(int_sig_orig_blocker,\n                    1-(sign(est_effect_ps)==sign(true_effect)),NA),\nSerrVps_adj=ifelse(int_sig_adj_blocker,\n                   1-(sign(est_effect_ps)==sign(true_effect)),NA),\nMerrVorig=ifelse(int_sig_orig,abs(est_effect)/abs_true_effect,NA),\nMerrVadj=ifelse(int_sig_adj,abs(est_effect)/abs_true_effect,NA),\nMerrVps_orig=ifelse(int_sig_orig_blocker,abs(est_effect_ps)/abs_true_effect,NA),\nMerrVps_adj=ifelse(int_sig_adj_blocker,abs(est_effect_ps)/abs_true_effect,NA))\n\nlong_data &lt;- select(all_sims_data,matches('V|sample|iter')) %&gt;% gather(effect_type,result,-sample_size,-iter) %&gt;% separate(effect_type,into=c('estimate','estimation'),sep='V') %&gt;% \n  mutate(estimate=factor(estimate,levels=c('sigeff','power','Serr','Merr'),\n                         labels=c('Mean\\nSignificant\\nEffect',\n                                  'Mean\\nPower',\n                                  'S-Error\\nRate',\n                                  'M-Error\\nRate')),\n         estimation=factor(estimation,levels=c('adj','orig','ps_adj','ps_orig'),\n                           labels=c('No Post-Stratification\\nClustered Errors\\n',\n                                    'No Post-Stratification\\nUn-clustered Errors\\n',\n                                    'Post-Stratification\\nClustered Errors\\n',\n                                    'Post-Stratification\\nUn-clustered Errors\\n')))\n\nlong_data_treatment &lt;- select(all_sims_data,matches('total|iter|sample')) %&gt;% gather(effect_type,result,-sample_size,-iter) %&gt;%\nmutate(effect_type=factor(effect_type,levels=c('total_sig_adj',\n                                               'total_sig_orig',\n                                               'total_sig_adj_blocker',\n                                               'total_sig_orig_blocker'),\n                          labels=c('No Post-Stratification\\nClustered Errors\\n',\n                                   'No Post-Stratification\\nUn-clustered Errors\\n',\n                                   'Post-Stratification\\nClustered Errors\\n',\n                                   'Post-Stratification\\nUn-clustered Errors\\n')))\n\n\n\n# Plot a sample of the data (too big to display all of it)\n\nlong_data %&gt;% ungroup %&gt;% \n  slice(1:10) %&gt;% \n  select(-estimation) %&gt;% \n  mutate(estimate=str_replace(estimate,\"\\\\n\",\" \")) %&gt;% \n  knitr::kable(.) %&gt;% \n  kable_styling(font_size = 8)\n\n\n\n\nsample_size\niter\nestimate\nresult\n\n\n\n\n1334.783\n1\nMean Significant Effect\n0.6298429\n\n\n1334.783\n2\nMean Significant Effect\n0.3874088\n\n\n1334.783\n3\nMean Significant Effect\nNA\n\n\n1334.783\n4\nMean Significant Effect\n1.1438379\n\n\n1334.783\n5\nMean Significant Effect\n0.5653086\n\n\n1334.783\n6\nMean Significant Effect\n1.2689594\n\n\n1334.783\n7\nMean Significant Effect\nNA\n\n\n1334.783\n8\nMean Significant Effect\nNA\n\n\n1334.783\n9\nMean Significant Effect\nNA\n\n\n1334.783\n10\nMean Significant Effect\nNA"
  },
  {
    "objectID": "posts/conjoint/conjoint.html#plotting",
    "href": "posts/conjoint/conjoint.html#plotting",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "Plotting",
    "text": "Plotting\nI use the gam function in the ggplot2 package to plot a smoothed regression line of the simulation draws for each sample size.\nFirst we can look at the difference that clustered errors makes across the different statistics. The only noticeable differences are at sample sizes smaller than 500. Clustering on respondents tends to result in smaller average significant effects, but it also results in increases in sign errors. This finding differs from the literature that considers clustering important to control for intra-respondent correlation, which in this simulation was fixed at 0.5. At sample sizes larger than 500, there does not appear to be any difference between clustered and un-clustered estimates.\n\ng_title &lt;- guide_legend(title='')\nfilter(long_data,grepl('No Post',estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +\n  theme_minimal() + stat_smooth(colour='red') +\n  xlab('Sample Size') + ylab(\"\") +\n  facet_wrap(~estimate,scales='free') + theme(panel.grid.minor.y = element_blank(),\n                                              panel.grid.major.y = element_blank()) +\n  scale_color_brewer(palette='Accent') + guides(colour=g_title,linetype=g_title) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggsave('clust_err.png',units='in',width=6)\n\nNext I look at post-stratification as an option to improve the precision of estimates. For unclustered errors reported below, post-stratified estimates do have higher power and slightly lower average significant effects, and importantly, the post-stratified estimates worsen neither type S nor type M errors.\n\ng_title &lt;- guide_legend(title='')\nfilter(long_data,grepl('Un-clustered',estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +\n  theme_minimal() + stat_smooth(colour='red') +\n  xlab('Sample Size') + ylab(\"\") +\n  facet_wrap(~estimate,scales='free') + theme(panel.grid.minor.y = element_blank(),\n                                              panel.grid.major.y = element_blank()) +\n  scale_color_brewer(palette='Accent') + guides(colour=g_title,linetype=g_title) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggsave('post_unclust_err.png',units='in',width=6)\n\nPost-stratification appears to have a similar effect on clustered error estimations, although the differences are smaller. In smaller samples, post-stratified estimates do have smaller M-errors.\n\ng_title &lt;- guide_legend(title='')\nfilter(long_data,!grepl('Un-clustered',estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +\n  theme_minimal() + stat_smooth(colour='red') +\n  xlab('Sample Size') + ylab(\"\") +\n  facet_wrap(~estimate,scales='free') + theme(panel.grid.minor.y = element_blank(),\n                                              panel.grid.major.y = element_blank()) +\n  scale_color_brewer(palette='Accent') + guides(colour=g_title,linetype=g_title) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggsave('post_clust_err.png',units='in',width=6)\n\nFinally, I also report average numbers of significant coefficients for the 14 treatments. Given that the 14 treatments were sampled from a normal distribution with prior density in the positive values with a meaan of 0.5, in expectation 95% of estimates should be statisticall significant. While that upper limit is reached only in high sample numbers, it looks like the ratio for treatment effects reaches an acceptable level of 70 percent at about 500 sample respondents. Also, post-stratifying un-clustered models results in effects that are reported as significant at much higher rates, as would follow from the previous results about post-stratification.\n\ng_title &lt;- guide_legend(title='')\nggplot(long_data_treatment,aes(y=result,x=sample_size,linetype=effect_type,colour=effect_type)) +\n  theme_minimal() + stat_smooth() +\n  xlab('Sample Size') + ylab(\"\") +\ntheme(panel.grid.minor.y = element_blank(),\n                                              panel.grid.major.y = element_blank()) +\n  scale_color_brewer(palette='Dark2') + guides(linetype=g_title,colour=g_title) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggsave('all_treat_rate.png',units='in',width=6)"
  },
  {
    "objectID": "posts/conjoint/conjoint.html#conclusion",
    "href": "posts/conjoint/conjoint.html#conclusion",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "Conclusion",
    "text": "Conclusion\nThis simulation study shows that a sample size of approximately 1,000 respondents is enough to obtain high power while also lowering both the S and M-error rates for treatment interaction effects in this conjoint experiment. The treatment effects themselves are generally of high quality once the sample size reaches 500 because the total number of respondents in each treatment cell is considerably higher than in an interaction. Post-stratification appears to be a useful strategy to increase precision without inducing S or M errors; at the very least, post-stratification does not appear to have any adverse effects on the estimation.\nOn the other hand, it appears that clustering errors increases the S-error rate at small sample sizes, a surprising finding considering that clustering methods are designed to inflate, not deflate, standard errors. Given that the S-error rate reveals the likelihood of making an error about the sign of the treatment effect, this is a potentially serious problem. For that reason I intend to report both clustered and un-clustered estimates in my analysis."
  },
  {
    "objectID": "posts/tunisia_imf/index.html",
    "href": "posts/tunisia_imf/index.html",
    "title": "New Survey Research on Tunisia and the IMF Deal",
    "section": "",
    "text": "Tunisia’s dictator, President Kais Saied, is pushing the country towards full-scale economic collapse by refusing to negotiate with the International Monetary Fund about a potential rescue deal. Neither the government nor the increasingly repressed opposition wants to take responsibility for the wrenching changes proposed by the IMF despite the offered loan package of $1.9 billion, such as privatization of state-owned enterprises and reducing public sector salaries. To gain insight into how ordinary Tunisians see this difficult situation, we implemented a national poll in July and August of 880 people using secure online recruitment tools. What we found is that the opinions of rank and file Tunisians are more diverse than their uncooperative leaders, but misunderstandings about the deal abound.\nTunisia is already in effective default as it has failed to pay suppliers of state-subsidized goods, including staples like sugar, flour and olive oil. Salaries to government employees have been delayed repeatedly. International default is looming with big loan repayments by the end of the year. For these reasons, an IMF deal that could avert an economic crisis is not an abstract outcome but rather a solution for a problem that all Tunisians face on a daily basis. Although President Saied’s ministers have negotiated with the IMF and advocated for the deal, he has argued against it in public because it would undermine the country’s sovereignty. As the country’s most popular political leader–albeit perhaps not quite as popular as he was a few years ago–his obstinacy is likely to undermine any negotiated compromise. Since he rejected an IMF deal last October, no further negotiations with the IMF have yet to produce any concrete results.\nOur online poll captures Tunisians’ mixed sentiments towards the IMF. On average, Tunisians give the IMF deal an approval rating of about 43 using a feeling thermometer scale of 1 to 100, indicating opposition but of a moderate kind. In addition, a significant number of Tunisians approve of the IMF deal very strongly. Approximately one in 10 Tunisians rates the deal at 80 or higher out of 100. Moreover, when we asked Tunisians in the poll to explain their support or opposition, we received a range of answers. Some of the rationales matched common refrains in the media, such as fears that the deal would increase poverty and others echoing President Saied’s concerns over the country relinquishing its sovereignty. A not insignificant number, though, expressed beliefs that the deal would be a needed support for the country’s dismal finances and could even improve the political situation.\nThese very mixed feelings could be driven by a lack of quality knowledge about the IMF deal despite its prominence in the media. Only 22 percent of Tunisians report that they understand the negotiations very well, even though 62 percent of the population reports that they hear about the deal at least once a week. When we asked Tunisians to choose the correct elements of the proposed deal from a list of options, they do select some of the core planks, such as privatization of state-owned enterprises and trimming public sector salaries. However, they also select policies that are not a part of the negotiations, especially reducing migration to European countries. While the IMF does not have any preferences about Tunisian immigration policies, the connection between Tunisia’s fiscal collapse and the European officials’ posturing about the “dangers” of immigration have connected the two issues in the minds of many Tunisians.\nAnother hindrance to apprehending the IMF negotiations, of course, is that Tunisian politicians in both the government and the opposition are competing to oppose the deal rather than trying to prepare the population for what may have to happen in the future. We examined how prone people are to partisanship by exposing a subset of respondents to messages from popular politicians, including President Saied and the labor union UGTT, that expressed opposition to the deal. The message, which said that the deal could “compromise Tunisia’s economic and political stability”, statistically increased concerns over violations of Tunisia’s sovereignty by the IMF, though the effect was noticeably concentrated among President Saied supporters. For these partisans, we only needed to remind them of President Saied’s opposition to reduce their support for the deal.\nFurthermore, even though President Saied’s high-minded rhetoric has yet to yield progress on the economic front, Tunisians do not appear to hold him responsible. When we asked which political and social actors Tunisians believe are most responsible for the “Tunisian government’s high government debts”, the largest number (over 30 percent) chose the Islamist party Ennahda, even though this group has been out of office since Saied’s coup in the summer of 2021. Tunisian respondents were also more likely to blame the powerful labor union UGTT that represents public sector workers than Saied. Although the president is certainly facing pressure for lack of success in easing the economic crisis, many Tunisians still do not see him as the primary reason for their economic problems–at least when compared to other players.\nAt the same time, our polling points to some ways that the IMF and other supporters of the deal could improve its messaging to Tunisians. First, younger Tunisians are much less likely to report hearing about the deal or understanding it. Only 20 percent of Tunisians in their twenties report hearing about the IMF deal on a daily basis compared with more than 50 percent of Tunisians in their sixties. Similarly, roughly twice as many men than women report hearing about the deal on a daily basis. Strategically focusing messaging on younger Tunisians and women could help reduce some of the misunderstanding and lack of clarity about what the deal entails. Furthermore, the IMF would do well by clarifying that immigration levels are not a precondition for Tunisia to sign a deal.\nUltimately, the fate of the deal lies in the hands of Tunisia’s political leaders, but popular pressure could help move the needle on the government’s tepid progress in negotiations. After all, the more complex reactions from Tunisians offer more opportunities for building popular support than the strong disapproval offered by the country’s fractured leadership."
  },
  {
    "objectID": "posts/flat_earth/index.html",
    "href": "posts/flat_earth/index.html",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "",
    "text": "Perhaps no other subject in applied statistics and machine learning has caused people as much trouble as the humble logit model. Most people who learn either subject start off with some form of linear regression, and having grasped it, are then told that if they have a variable with only two values, none of the rules apply. Horror of horrors: logit is a non-linear model, a doom from which few escape unscathed. Interpreting logit coefficients is a fraught exercise as not only applied researchers but also statisticians have endless arguments about how this should be done.\nIn this post I de-mystify logit—and show why it’s such a useful model—by using maps. Yes, non-linear spaces can be tricky to understand, but as humans we can think reasonably well in up to three dimensions. If you know what the difference between a map and a globe is, then you can run and interpret logit just fine.\nI’m also going to argue based on this analogy that people who assiduously avoid logit are not so dissimilar from flat earthers. It’s true that moving from a map to a globe induces additional complications, but the alternative is run into significant problems in navigation. Similarly, applying the workhouse OLS model to a binary outcome isn’t wrong, but it can lead you astray when you want to figure out the distance between two points. Once you read this post, I hope you’ll see why logit is not all that complicated and should be the preferred model with a binary outcome."
  },
  {
    "objectID": "posts/flat_earth/index.html#b-is-for-bernoulli",
    "href": "posts/flat_earth/index.html#b-is-for-bernoulli",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "B Is For Bernoulli",
    "text": "B Is For Bernoulli\nI’ll start first with a review of what a logit model actually is as there is some confusion on this score. Logit is the wrong name for what most people are estimating; the model is in fact the Bernoulli distribution. This distribution has the following form for a variable \\(Y\\) that takes on two values, i.e. 0 or 1:\n\\[\nP(Y = k) = \\begin{cases} p, & \\text{if } k=1, \\\\1-p, & \\text{if } k=0. \\end{cases}\n\\]\nAs soon as the math appears, of course, it looks complicated. This formula is actually the simplest statistical distribution once you break it down. What it says is that, on average, the probability that a variable \\(Y\\) is equal to 1 is determined by the parameter \\(p\\), which represents a probability or a proportion. Suppose that \\(Y\\) has 10 values, 30% of which are zeroes (0s) and 70% of which are 1s. In that case, \\(p\\) would be equal to 0.7 or 70% because 70% of the time \\(Y\\) is equal to 1. The probability of 0s is then \\(1-p\\) or 100% - 70% = 30%. Easy peezy.\nWhat is important to remember, though, is that \\(p\\) is a proportion, so it can’t be greater than 1 or less than 0. (That’s why the Bernoulli and Beta distributions are connected, which I discuss in my other post on the topic). You can’t have \\(Y\\) be equal to 1 more than 100% of the time or less than 0% of the time; that’s nonsensical.\nYou’ve probably noticed that so far we haven’t mentioned the word “logit” at all. Where does this tricky little monster come in?\nWell if all we want to do is calculate the average proportion of 1s in \\(Y\\) then all we need is \\(p\\). But what if we want to see if there is a relationship between some covariate \\(X\\), say how old a person is, and a binary variable like \\(Y\\)? For example, suppose we want to know if older people are more likely to vote for Biden: voting can take on two values, either voting (1) or not voting (0). As an example, I’ll generate some plausible-looking age data for a bunch of citizens that we’ll call our covariate \\(X\\):\n\nX &lt;- rpois(1000, 45)\nggplot(tibble(x=X),aes(x=x)) + geom_histogram() + theme_barbie()\n\n\n\n\n\n\n\n\nWe see that the average person in our data is about 45 years old and the age range is between roughly 20 and 60. What we want to understand is whether on average older (or younger) people tend to vote more for Biden. But we have a problem - \\(p\\) in the Bernoulli distribution cannot be any bigger than 1 or less than 0, and our age variable \\(X\\) goes from 20 to 70. Yikes! We can’t just pop \\(X\\) into \\(p\\) and without going way outside that range.\nIf you take a look at \\(X\\), you might think, “OK, well, just divide \\(X\\) by 100 and it’ll be between 0 and 1.” Great thought! Let’s do that and plot \\(X\\) again:\n\nggplot(tibble(x=X/100),aes(x=x)) + geom_histogram() + theme_barbie()\n\n\n\n\n\n\n\n\nWe just squashed \\(X\\) to be small enough that we could just pop it into the Bernoulli distribution:\n\\[\nP(Y = k) = \\begin{cases} X, & \\text{if } k=1, \\\\1-X, & \\text{if } k=0. \\end{cases}\n\\]\nHere we simply replaced \\(p\\) with our covariate \\(X\\). Very nice! We can even generate random data using a simple algorithm from this function:\n\nDraw a random number between 0 and 1.\nIf the random number is less than \\(X\\), set \\(Y\\) equal to 1, and 0 otherwise.\n\nHere’s what our random data \\(Y\\) look like compared to \\(X\\):\n\nY &lt;- as.numeric(runif(length(X))&lt;(X/100))\n\nggplot(data=tibble(Y,X),aes(y=Y,x=X/100)) +\n  geom_point(alpha=0.5) +\n  stat_smooth() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nWhat our plot shows is that when X is high, we tend to get more 1s in \\(Y\\), and when \\(X\\) is low, we tend to get more 0s. The blue line shows what the average value of \\(Y\\) is for a given age. When age is quite high, about 0.65 or 65 years, then \\(Y\\) is about 0.65 or 65% on average (as we expect given the 1:1 relationship).\nIf you look at this formula, though, you might think something is a bit off. Do we think that someone’s age as a fraction will be exactly equal to their probability of voting for Biden? Will voters who are 70 years old have an exactly 70% chance of voting for him? We only have one option for describing that relationship, and a 1:1 correspondence seems very limiting. So what do we do if we think this relationship is more nuanced?\nTo allow the relationship to be less exact, we’ll take a page out of linear regression and add a coefficient to multiply \\(X\\), which we’ll call \\(\\beta\\). We’ll add \\(\\beta\\) to our little formula:\n\\[\nP(Y = k) = \\begin{cases} \\beta X, & \\text{if } k=1, \\\\1-\\beta X, & \\text{if } k=0. \\end{cases}\n\\]\nNow let’s say \\(\\beta\\) is equal to 0.5. Then, the probability that someone will vote for Biden if they are 70 years old is only 35%. If \\(\\beta\\) goes up, then that relationship would be stronger. Adding a coefficient allows us to express the relationship between \\(X\\) and \\(Y\\) with a lot more nuance.\nYou might see some further issues, though. What if \\(\\beta\\) is equal to 0? In that case, the probability that someone votes for Biden is also equal to exactly 0%. In probability world, that’s not a good thing—we can’t be 100% certain that someone will or will not vote for Biden. We’ll get around this problem by adding another parameter called the intercept, \\(\\alpha\\):\n\\[\nP(Y = k) = \\begin{cases} \\alpha + \\beta X, & \\text{if } k=1, \\\\1-(\\alpha + \\beta X), & \\text{if } k=0. \\end{cases}\n\\]\nThis is a handy dandy trick to allow \\(\\beta\\) to equal 0 without having the probability of \\(Y\\) to be equal to 0 as well. Say \\(\\alpha\\) is 0.35--then if \\(\\beta\\) is 0, 35% of people would still vote Biden. \\(\\alpha\\) could be thought of as a baseline rate—without considering the effect of age, how many people vote for Biden? Once we add \\(\\alpha\\), this equation is the same as the slope of a line (good ol’ \\(mx + b\\)), which makes it a linear model (linear means a straight line).\nWe now have a linear model we want to stick in \\(p\\) to model the relationship between age (\\(X\\)) and voting for Biden (\\(Y\\)). We’ve made progress, but we’re still gonna have issues squashing \\(X\\) into a proportion. The trick is that we need \\(\\alpha + \\beta X\\) to always output a number between 0 and 1. We can’t have a proportion greater than 1 or less than 0. For these reasons, to use this model we have to pick specific values for \\(\\alpha\\) and \\(\\beta\\) that allow us to keep within those bounds. For example, we could set \\(\\alpha\\) to 0.1 (a baseline rate of 10% of 1s) and \\(\\beta\\) to 0.5. Let’s simulate some data with those numbers and plot it:\n\nY &lt;- as.numeric(runif(length(X))&lt;(0.1 + .5*(X/100)))\n\nggplot(data=tibble(Y,X),aes(y=Y,x=X/100)) +\n  geom_point(alpha=0.5) +\n  stat_smooth() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nWe now have a much weaker relationship between age and voting: voting for Biden increases a bit as people age, but not much. But we do now have a much more nuanced relationship than we started out with by using the divide by 100 method. Very cool!\nHowever, you might be wondering—what if we want even more diverse relationships? What if \\(\\beta\\) were greater than 1? Or 3? Or 10? Let’s plot what happens when we set \\(\\alpha\\) = 0.1 and \\(\\beta\\)=3:\n\nY &lt;- as.numeric(runif(length(X))&lt;(0.1 + 3*(X/100)))\n\nggplot(data=tibble(Y,X),aes(y=Y,x=X/100)) +\n  geom_point(alpha=0.5) +\n  stat_smooth() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nYikes! Now we’re only getting values for \\(Y\\) that are equal to 1. What happened to all the 0s? When we multiply \\(X\\) by 3, we now get values way bigger than 100%. For an age of 50, \\(X/100\\) = 0.5, and \\(3\\cdot X=1.5\\) or 150%. Shucks! This means we can’t just pick whatever values we want for \\(\\beta\\) or \\(\\alpha\\). That’s frustrating especially if we think that relationship could be really strong and we want \\(\\beta\\) to be large."
  },
  {
    "objectID": "posts/flat_earth/index.html#not-all-heroes-are-linear",
    "href": "posts/flat_earth/index.html#not-all-heroes-are-linear",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "Not All Heroes Are Linear",
    "text": "Not All Heroes Are Linear\nWe now have two options. We can keep on guessing what values might work for \\(\\alpha\\) and \\(\\beta\\), or… we could get a map. A map that would allow us to translate or transform our linear model \\(\\alpha + \\beta X\\) into the cramped space of \\(p\\). Like the teleporter in Star Trek, we need something that can move our value of \\(\\alpha + \\beta X\\) to the right spot on \\(p\\) without going over 1 and under 0.\nThis is where our good friend logit comes in. Logit is a function that can take in any number–literally any number–and magically transform it to a number that is strictly between 0 and 1. It’s a magical map that let’s us move from the world of ages and to the world of proportions between 0 and 1. NB: Technically this is not the logit function but rather the inverse logit function. But everyone calls the model “logit” so I’ll just use that term.\nThe function itself is a bit ugly, but I’ll include it here along with our linear model as the input and \\(p\\) as the output:\n\\[\np = \\frac{1}{1 + e^{-(\\alpha + \\beta X)}}\n\\]\nI won’t spend a lot of time on explaining the function other than to note that it has the number \\(e\\), which is where a lot of the magic comes from. We can test whether it can really do these transformations by plotting a range of numbers for our linear model and seeing what the logit function spits out:\n\nbeta &lt;- .2\nalpha &lt;- -10\n\nlogit &lt;- function(x)  { 1 / (1 + exp(-(x))) }\n\ntibble(linear_model=alpha + beta * X) %&gt;% \n  mutate(p=logit(linear_model)) %&gt;% \n  ggplot(aes(y=p, x=linear_model)) +\n  geom_point() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nOn the bottom you can see all the values for our linear model when \\(\\alpha=-10\\) and \\(\\beta=0.2\\). Across the range of ages, the linear model runs from -2.5 to 2.5 or way outside the bounds of \\(p\\). But our logit function spits out a number for \\(p\\) that is between 0 and 1. It also has this cool bend-y shape, or what’s known as a sigmoid. Basically, the line is straight or linear right around the midpoint of \\(p\\) or 0.5. When it gets close to the end points of \\(p\\), or proportions that are very high or very low, it starts to bend like a driver avoiding a barrier in a car.\nIf you notice, though, while the line curves, it never changes the direction. When the linear model goes up, so does \\(p\\). That’s why we can use this function–we will learn about the direction and magnitude of the relationship between age and voting. We just learn that information while respecting the bounds of proportions and the full range of our covariates. Pretty cool!"
  },
  {
    "objectID": "posts/flat_earth/index.html#a-whole-new-world",
    "href": "posts/flat_earth/index.html#a-whole-new-world",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "A Whole New World",
    "text": "A Whole New World\nOften people get confused or suspicious at this point. What’s up with this weird S-curve thing? Linear models are nice and straight. Why do we need the bend-y thing?\nTo understand this, we can return back to our analogies of maps. The maps that we use are all two-dimensional: we can put them on a table and they lie flat. However, we know that they aren’t completely accurate because the Earth is not flat, it’s round. A globe is the truly accurate representation of the Earth. A flattened map is going to inevitably cause distortions.\nAs a result, when we trace a flight path on a flat map, the plane’s journey is always curved as in the picture below:\n\nThis flight path–which happens to be for a very long flight from Paris to Tahiti–seems bizarrely roundabout on the map. You would think that the straightest path between Paris and Tahiti would be a straight line. This would be true if the Earth were flat, but it’s not. The Earth is round, and the map is a linear projection of a 3-D object onto a 2-D surface. When we do that and calculate the most direct route, we end up with this weird bend-y shape: just like our logit plot above.\nBy the same analogy, our linear model is our simplified version of reality that allows us to understand the relationship between \\(X\\) and \\(Y\\). Logit is the transformation that takes us from this simplified world to the real world of \\(Y\\) via \\(p\\). It does that by warping the space that the straight line of our linear model travels. To see this, let’s take another look at our logit plot:\n\nbeta &lt;- .2\nalpha &lt;- -10\n\nlogit &lt;- function(x)  { 1 / (1 + exp(-(x))) }\n\ntibble(linear_model=alpha + beta * X) %&gt;% \n  mutate(p=logit(linear_model)) %&gt;% \n  ggplot(aes(y=p, x=linear_model)) +\n  geom_point() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nWhen the linear model is 0–or \\(p\\) is around 0.5–the logit function allows \\(p\\) to change relatively quickly as the linear model changes. However, when the linear model gets very low or very high–i.e., \\(p\\) is close to 0 or 1–the logit function only lets \\(p\\) change slowly as the linear model changes. To get from a \\(p\\) of 0.1 to almost 0 requires the linear model to move about -2 points on the \\(x\\) axis, but to get from a \\(p\\) of 0.5 to a \\(p\\) of 0.25 only requires a movement of about -0.5 in the linear model space.\nThe magical part of logit is that it creates more space as the linear model gets very big or very small. The linear model keeps moving at the same rate (\\(\\beta\\)) but \\(p\\), the proportion, does not. \\(p\\) slows down as though more and more space kept appearing. Like Jim Carrey in the Truman Show who can never escape from his artificial world, the linear model can never escape the bounds of \\(p\\). The closer it gets to the bounds, the farther it has to travel–the more warped the linear model space becomes relative to \\(p\\). In other words, at the edges the linear model map is less accurate and so we see a stronger bend in the logit function to compensate."
  },
  {
    "objectID": "posts/flat_earth/index.html#flat-earthers-and-the-logit-function",
    "href": "posts/flat_earth/index.html#flat-earthers-and-the-logit-function",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "Flat Earthers and the Logit Function",
    "text": "Flat Earthers and the Logit Function\nWhat is logit then? A handy dandy scaling function that maps a linear model to a proportion between 0 and 1. That’s all it does. The actual distribution is the Bernoulli (though sadly, no one calls it that). A logit function isn’t special—there are other similar functions, just as there are a lot of map coordinate transformations (talk to a GIS person sometime; it’ll bore you to tears).\nThere are people who really don’t like logit. Usually it’s something to do with how the function complicates things and makes these bend-y shapes. It does require some thinking as we have to think a bit about what our linear model coefficient \\(\\beta\\) means. The linear model is a straight line, but the space it is passing through changes as \\(p\\) changes. What that means substantively is that \\(\\beta\\), or the effect of age on the proportion voting for Biden, will vary depending on how big or small the proportion voting for Biden is. If that proportion is small, a big value of \\(\\beta\\) will result in much smaller movement in voting than when the proportion voting is closer to 50%.\nIt is this bend-y (or non-linear) nature of logit that leads some people to just ignore the Bernoulli and use a linear model. That is an easy way to deal with the problem, but unfortunately, we cannot simply ignore this transformation. Like flat earthers, if we ignore the fact that the Earth is round, we won’t be able to calculate distances accurately. If we have a linear model that can get larger or smaller than 0% or 100%, then we will end up with nonsensical predictions from our linear model like a plane that ends up at the wrong airport because the pilot believed the Earth was flat.\nSo how do you interpret a logit model coefficients? There are two ways once you understand what the logit function does:\n\nLeave it as it is. The coefficient \\(\\beta\\) is the relationship of the covariate (\\(X\\)) to the outcome (\\(Y\\)) in the linear or “flat” space. The larger \\(\\beta\\) is, the stronger the relationship is between the covariate and the outcome. No, we don’t know exactly how much the proportion will change as \\(X\\) changes, but we can still interpret \\(\\beta\\) as expressing what that change looks like in the simple space of the linear model.\nConvert it to changes in \\(Y\\) via the proportion \\(p\\). Just like a plane figuring out distances using transformation of coordinates, we can figure out the average change in a proportion \\(p\\) by averaging (or marginalizing) over different logit functions for a given \\(\\beta\\). I won’t go into the technical details, but we have very handy R packages to calculate what are called marginal effects: how much does the “real world” \\(p\\) change as our linear model coefficient \\(\\beta\\) changes?\n\nThere are plenty of vignettes about how to calculate marginal effects on the R package marginaleffects website. Here I’ll show how to calculate a marginal effect of \\(X\\) on \\(Y\\) with our linear model coefficients of \\(\\alpha=-10\\) and \\(\\beta=0.2\\). To do so I’ll first simulate data with these values and then fit a logit (i.e. Bernoulli) model with the R function glm:\n\n# simulate our outcome Y given our linear model parameters\n\nY &lt;- as.numeric(runif(length(X))&lt;logit(alpha + beta*X))\n\n# make a dataset and fit the model for Y and X\n# note that glm says the family is \"Binomial\"\n# Bernoulli is a special case of the Binomial\n# and our link (scaling) function is logit!\n\nsim_data &lt;- tibble(Y=Y, X=X)\nlogit_fit &lt;- glm(Y ~ X,data=sim_data,family=binomial(link=\"logit\"))\n\nsummary(logit_fit)\n\n\nCall:\nglm(formula = Y ~ X, family = binomial(link = \"logit\"), data = sim_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2091  -0.7986  -0.4686   0.8678   2.5201  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -9.78933    0.70117  -13.96   &lt;2e-16 ***\nX            0.19578    0.01484   13.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1250.71  on 999  degrees of freedom\nResidual deviance:  998.47  on 998  degrees of freedom\nAIC: 1002.5\n\nNumber of Fisher Scoring iterations: 5\n\n\nAbove we see the output of the glm command that shows us the values of \\(\\alpha\\) (the intercept, as noted in the command output) and \\(\\beta\\), which is listed as the Estimate for X. These are the values of these parameters in the linear, or simple, space. There is nothing wrong with these coefficients: they tell you how much \\(X\\) is changing with respect to \\(Y\\) in the linear space. How much actual change in \\(Y\\) happens, though, depends on exactly where we are (or how high or low the proportion of 1s is in \\(Y\\)).\nIf we want to convert these relationships to the distances/effects in the outcome \\(Y\\) (i.e. changes in the proportion \\(p\\)), we can use the avg_slopes function from marginaleffects:\n\nlibrary(marginaleffects)\n\navg_slopes(logit_fit,variables=\"X\")\n\n\n Term Estimate Std. Error    z Pr(&gt;|z|)  2.5 % 97.5 %\n    X   0.0324    0.00158 20.5   &lt;0.001 0.0293 0.0355\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high \n\n\nNote that this Estimate is way smaller than the Estimate for \\(\\beta\\) from the glm command. That estimate was from the linear space and so is much bigger. The marginaleffects command is telling us how much \\(p\\), or the proportion of 1s in \\(Y\\), changed as our covariate \\(X\\) increased. It did this by averaging over different logit functions given changes in \\(X\\) (you can see the package info for more technical details). On average, as age increased by 1 year, the proportion voting for Biden increased by 0.0325 or 3.25% with a confidence interval of (2.93%, 3.56%).\nVery cool! Now that you understand logit models and how to interpret linear model coefficients in both the linear space and via marginal effects, you know pretty much everything you need to know about this wonderful little function logit. Next time you have a binary outcome like voting for Biden, or anything else, give logit a go! It’s your map to a whole new world.\n\nAll By Myself Jasmine GIFfrom All By Myself GIFs"
  },
  {
    "objectID": "posts/kais_saied_results/index.html",
    "href": "posts/kais_saied_results/index.html",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "",
    "text": "This Rmarkdown document contains code and data for the Kais Saied survey experiment by Robert Kubinec and Amr Yakout. This survey was fielded using an online panel in Tunisia during August of 2023. The survey experiment involved providing half the sample a direct question about whether they supported Kais Saied’s move to suspend parliament and centralize power in his hands, and the other half of the sample had a randomized response question designed to allow them to answer truthfully without being identified. This experiment was pre-registered and the pre-registration can be accessed from this link.\nThis document was drafted to allow people to verify our results using the raw data. The survey data included has been stripped of any identifying information about respondents, but it does include the actual answers from the survey so that all analyses can be reproduced. You can access the code file and the raw data from this Github repository: https://github.com/saudiwin/saudiwin.github.io/tree/sources/content/post (file is entitled kais_saied_results.Rmd and the data file is in the data/ subfolder). The survey data can be found in the data/ folder as kais_saied_survey.csv.\nThis Quarto document includes embedded R code that loads the survey data and estimates the Stan model for proportion of people who support Kais Saied, and compares it to the direct question to see if there is sensitive survey bias. At present there are a total of 913 completed responses.\nAll questions about the analysis should be directed to Robert Kubinec at rmk7@nyu.edu."
  },
  {
    "objectID": "posts/kais_saied_results/index.html#robustness-check-time-of-completion",
    "href": "posts/kais_saied_results/index.html#robustness-check-time-of-completion",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Robustness check: time of completion",
    "text": "Robustness check: time of completion\nIn this section I show some general statistics for the data. In particular, I look at how long it takes people to complete the survey. In the plot below I show the average duration in minutes over time for each survey response. The plot shows that most people took the survey in about 10 to 30 minutes:\n\nsurvey_data %&gt;% \n  filter(`Duration (in seconds)`&lt;10000) %&gt;% \n  mutate(Duration=`Duration (in seconds)`/60) %&gt;% \n  ggplot(aes(y=Duration,x=StartDate)) +\n  geom_point() +\n  scale_y_log10() +\n  geom_smooth()\n\n\n\n\n\n\n\n\nThe plot also shows that data collection started in late July and continued until mid-August. Data came in fairly regularly on a daily basis."
  },
  {
    "objectID": "posts/kais_saied_results/index.html#survey-design",
    "href": "posts/kais_saied_results/index.html#survey-design",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Survey Design",
    "text": "Survey Design\nIn the survey we implemented a form of a randomized response question that is designed to help people report answers on a survey that they may be embarrassed about or in danger if they report. The question was originally developed for private medical information like abortion or HIV infections, and has been extended to cover crime, drug use and corruption reporting. See Hout and Heijden (2002), Gingerich et al. (2016) and Blair, Imai, and Zhou (2015) for more information.\nThe theory behind a randomized response is to inject randomness into the survey question. Theoretically it is similar to approaches in cryptography that use random numbers to encode online data using keys. In our case, we used naturally ocurring randomness related to people’s birthdays. Because we did not ask for respondents’ birth dates, and the month in which one is born is essentially random, we can use that naturally occurring randomness to encode their responses.\nTo test for how much we could encourage people to respond truthfully, we randomly assigned each respondent to receive one of the two questions below:\nDirect Question\n\nDo you oppose President Kais Saied’s moves to change Tunisia’s constitution and close the parliament?\n\nYes\nNo\n\n\nRandomized Response\nWe understand that politics in Tunisia is sensitive right now. This question is worded so that you can tell us what you think but still protect your privacy. Because we don’t know when your mother was born, we also won’t know for sure your political opinion.\n\nMy mother’s birth date is in January, February or March.\nI oppose President Kais Saied’s moves to change Tunisia’s constitution and close the parliament.\n\nPlease pick the answer that best represents whether these statements are true of you:\n\nBoth statements are true OR neither is true.\nOne of the two statements is true.\n\nWhile explaining how this question works is beyond the scope of this note, we again refer to the linked paper above for the statistical mechanics. As long as the respondent reads and follows the instructions, they can report their opposition (or support) for Kais Saied’s power grab without us being able to know their true response. Essentially, we can’t separate their answer from whether their mother’s birthday is in a given month, and as a result, the individual answers are encoded. For any respondent, we have no idea whether they oppose Saied or not.\nHowever, the beauty of this method is that even though we don’t know any one person’s response, we can estimate how many in people in total support or oppose Saied. When we aggregate responses, we can take into account that on average \\(\\frac{1}{4}\\) of our respondents will have mothers who were born in those months. Using statistical models that we estimate below, we can find out how many people truly oppose Saied, and also how many people appear to be under-reporting their opposition relative to the direct question."
  },
  {
    "objectID": "posts/kais_saied_results/index.html#sensitivity-estimates",
    "href": "posts/kais_saied_results/index.html#sensitivity-estimates",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Sensitivity Estimates",
    "text": "Sensitivity Estimates\nIn this section we show several different ways that we calculate the true number of people who are opposed to Kais Saied. We first use a simple Bayesian calculation based on the beta distribution that was specified in the pre-registration linked above. We then look at other existing R packages that are used for randomized response questions and then we implement a custom Bayesian model that allows us to jointly model both the randomized response and the direct question. We also use this custom model to allow us to adjust for biases in using online panels that may not be fully representative.\n\n# proportion selecting birthday response\n\nlambda &lt;- table(survey_data$kais_rr)\nlambda &lt;- lambda[2]/sum(lambda)\n\nN &lt;- sum(as.numeric(!is.na(survey_data$kais_rr) & survey_data$FL_63_DO_saied_sensitive==1))\n\nto_stan_data &lt;- list(success_cm=N*((lambda - .75)/(-.5)),\n                     fail_cm= N*(1-((lambda - .75)/-.5)),\n                     success_d=sum(as.numeric(survey_data$kais_direct==\"Yes\" & survey_data$FL_63_DO_saied_nonsensitive==1),na.rm=T),\n                     fail_d=sum(as.numeric(survey_data$kais_direct==\"No\" & survey_data$FL_63_DO_saied_nonsensitive==1),na.rm=T))\n\nfit_mod &lt;- sen_mod$sample(chains=4, cores=4, data=to_stan_data,refresh=0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nfit_mod$summary()\n\n# A tibble: 5 × 10\n  variable          mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__          -853.    -852.    1.23   1.01   -855.    -851.     1.00    1998.\n2 pi_hat           0.507    0.507 0.0244 0.0250    0.468    0.548  1.00    3616.\n3 d_hat            0.298    0.298 0.0218 0.0218    0.262    0.334  1.00    3943.\n4 d_hat_binomi…    0.298    0.297 0.0212 0.0211    0.263    0.333  1.00    3934.\n5 estimand         0.210    0.210 0.0323 0.0319    0.157    0.264  1.00    3622.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nWe can plot the estimated bias as the difference between the average proportion of those responding affirmative to the direct question and the calculated level of opposition to Saied from the randomized response question:\n\nestimand &lt;- c(fit_mod$draws(\"estimand\"))\n\nmcmc_dens(fit_mod$draws(\"estimand\")) + \n  theme_tufte() +\n  labs(x=\"Percent Increase Over Direct Question\") +\n  scale_x_continuous(labels=scales::percent_format()) + \n  ggtitle(\"Difference Between Direct Question and Encrypted Question\",\n          subtitle=\"Opposition to Saied's Coup\")\n\n\n\n\n\n\n\nggsave(\"bias_calc.png\",width=5,height=4)\n\nOur basic model shows that approximately 21% of respondents reported anti-Saied preferences in the randomized response method versus the direct question. The 5% - 95% uncertainty interval is from 15.7 to 26.4 .\nThe full posterior density of the bias-corrected estimate is:\n\nmcmc_dens(fit_mod$draws(\"pi_hat\")) + theme_tufte()\n\n\n\n\n\n\n\n\nComparing the two estimates as intervals (pi_hat is the true level of opposition to Saied and d_hat is the proportion from the direct question):\n\nmcmc_intervals(fit_mod$draws(c(\"pi_hat\",\"d_hat\"))) + theme_tufte()"
  },
  {
    "objectID": "posts/kais_saied_results/index.html#alternative-estimation-rrreg",
    "href": "posts/kais_saied_results/index.html#alternative-estimation-rrreg",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Alternative Estimation: RRreg",
    "text": "Alternative Estimation: RRreg\nIn this section we use the R package RRreg to check these calculations using a more traditional means for estimating sensitive proportions.\n\nest_rreg &lt;- RRuni(response=as.numeric(comp_data_rr$kais_rr==\"Both statements are true OR neither is true.\" & comp_data_rr$FL_63_DO_saied_sensitive==1),\n                  p=.25,\n                  model=\"Crosswise\",MLest = TRUE)\n\nsummary(est_rreg)\n\nCrosswise Model with p = 0.25\nSample size: 425\n\n   Estimate   StdErr      z  Pr(&gt;|z|)    \npi 0.507059 0.048563 10.441 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ns1 &lt;- summary(est_rreg)\n\nLet’s compare these estimates with plots:\n\nest_beta &lt;- fit_mod$summary()\n\nest_data &lt;- tibble(estimate=c(s1$coefficients[,1],\n                              est_beta$median[est_beta$variable==\"pi_hat\"]),\n                   low=c(s1$coefficients[,1]-1.96*(s1$coefficients[,2]),\n                         est_beta$q5[est_beta$variable==\"pi_hat\"]),\n                   high=c(s1$coefficients[,1]+1.96*(s1$coefficients[,2]),\n                         est_beta$q95[est_beta$variable==\"pi_hat\"]),\n                   estimator=c(\"RRreg\",\"Bayes\"))\n\nest_data %&gt;% \n  ggplot(aes(y=estimate,x=estimator)) +\n  geom_linerange(aes(ymin=low,\n                     ymax=high)) +\n  geom_point() +\n   theme_tufte()\n\n\n\n\n\n\n\n\nWe can see that they are similar estimates, although the R package has higher uncertainty than the simple Bayesian estimate."
  },
  {
    "objectID": "posts/kais_saied_results/index.html#alternative-estimator-rr",
    "href": "posts/kais_saied_results/index.html#alternative-estimator-rr",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Alternative Estimator: rr",
    "text": "Alternative Estimator: rr\nWe can also use the R package rr which uses the EM algorithm as opposed to RRreg which uses more conventional optimization.\n\ncomp_data_rr$gender_two &lt;- factor(comp_data_rr$gender,\n                                  exclude=\"Other:\")\ncomp_data_rr &lt;- filter(comp_data_rr, !is.na(gender_two))\n# need to switch the outcome\ncomp_data_rr$kais_rr_num_switch &lt;- 1 - comp_data_rr$kais_rr_num\nrr_est &lt;- rrreg(kais_rr_num_switch ~ as.character(gender_two), \n                data=comp_data_rr,\n                      p=.75,\n                      design='mirrored')\n\n# predict sensitive trait\n\npred_rr &lt;- predict(rr_est,quasi.bayes = T)\nmean(pred_rr$est)\n\n[1] 0.5006138\n\nmean(pred_rr$est[comp_data_rr$gender_two==\"Male\"],na.rm=T)\n\n[1] 0.5191125\n\nmean(pred_rr$est[comp_data_rr$gender_two==\"Female\"],na.rm=T)\n\n[1] 0.4720857\n\npred_rr_mean &lt;- predict(rr_est,quasi.bayes = T,avg=T)\n\nThis number seems very close to our other estimators. Again, the uncertainty seems larger than the simple Bayesian method. Minimal gender differences over who is more or less likely to oppose Saied.\nWe can plot this estimate against the others:\n\nest_data &lt;- tibble(estimate=c(s1$coefficients[,1],\n                              est_beta$median[est_beta$variable==\"pi_hat\"],\n                              pred_rr_mean$est),\n                   low=c(s1$coefficients[,1]-1.96*(s1$coefficients[,2]),\n                         est_beta$q5[est_beta$variable==\"pi_hat\"],\n                         pred_rr_mean$ci.lower),\n                   high=c(s1$coefficients[,1]+1.96*(s1$coefficients[,2]),\n                         est_beta$q95[est_beta$variable==\"pi_hat\"],\n                         pred_rr_mean$ci.upper),\n                   estimator=c(\"RRreg\",\"Bayes\",\"EM\"))\n\nest_data %&gt;% \n  ggplot(aes(y=estimate,x=estimator)) +\n  geom_linerange(aes(ymin=low,\n                     ymax=high)) +\n  geom_point() + theme_tufte()"
  },
  {
    "objectID": "posts/kais_saied_results/index.html#brms-model",
    "href": "posts/kais_saied_results/index.html#brms-model",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "BRMS Model",
    "text": "BRMS Model\nFinally, we define a brms custom family for the randomized response model using the confusion matrix approach of Hout and Heijden (2002). This model is a variety of the Bernoulli distribution that takes into account the known probabilities of obtaining the true response. It jointly models both the randomized response model and the direct question, allowing us to estimate the bias directly rather than post-estimation. By implementing it in brms, we can make use of brms features like multilevel regression to allow us to do post-stratification adjustment of the survey responses with population data from Tunisia’s 2014 census. This is our preferred specification.\n\nlibrary(brms)\n\nstan_funs &lt;- '\n  real sens_reg_lpmf(int y, real mu, real bias, matrix P, int T) {\n  \n    // generalized RR model from van der Hout and van der Heijden (2002)\n    // also see R package RRreg\n    \n    real out;\n    // need to impose a constraint on bias where it cannot be larger than mu\n    real bias_trans = mu * bias;\n    \n    if(T==1) {\n    \n      // treatment distribution (crosswise model)\n      \n      if(y==1) {\n      \n        out = P[2,1] * (1 - mu) + P[2,2] * mu;\n      \n      } else if(y==0) {\n      \n       out = P[1,1]*(1-mu) + P[1,2]*mu;\n      \n      }\n\n    } else if (T==0) {\n    \n      // control = direct question\n    \n      if(y==1) {\n      \n        out = mu - bias_trans;\n      \n      } else if(y==0) {\n      \n        out = (1 - mu) + bias_trans;\n      \n      }\n    \n    }\n    \n    return log(out); \n\n  }\n  \n  int sens_reg_rng(real mu, real bias, matrix P, int T) {\n  \n    real bias_trans = mu*bias;\n  \n    if(T==1) {\n    \n      return bernoulli_rng(P[2,1] * (1 - mu) + P[2,2] * mu);\n      \n    } else {\n    \n      return bernoulli_rng(mu - bias_trans);\n    \n    }\n\n  }'\n\n# define custom family\n\nfamily_sens_reg &lt;- custom_family(\"sens_reg\",\n                                 dpars=c(\"mu\",\"bias\"),\n                                 links=c(\"logit\",\"logit\"),\n                                 type=\"int\",\n                                 lb=c(NA,NA),\n                                 ub=c(NA,NA),\n                                 vars=c(\"P\",\"vint1[n]\"))\n\n# define log-likelihood\n\nlog_lik_sens_reg &lt;- function(i, prep) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  y &lt;- prep$data$Y[i]\n  treatment &lt;- prep$data$vint1[i]\n  bias &lt;- brms::get_dpar(prep, \"bias\", i = i)\n  \n  bias_trans &lt;- bias*mu\n  \n  if(treatment==1) {\n    \n    if(y==1) {\n    \n      return(log(P[2,1] * (1 - mu) + P[2,2] * mu))\n  \n    } else {\n      \n      return(log(P[1,1]*(1-mu) + P[1,2]*mu))\n    \n    }\n    \n  } else {\n    \n    if(y==1) {\n      \n      return(log(mu - bias_trans))\n      \n    } else {\n      \n      return(log((1 - mu) + bias_trans))\n      \n    }\n    \n  }\n  \n\n}\n\n# define posterior predictions\n\nposterior_predict_sens_reg &lt;- function(i, prep, ...) {\n  \n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  bias &lt;- brms::get_dpar(prep, \"bias\", i = i)\n  y &lt;- prep$data$Y[i]\n  treatment &lt;- prep$data$vint1[i]\n  \n  bias_trans &lt;- mu*bias\n\n  if(treatment==1) {\n    \n    out &lt;- rbinom(n=length(mu),size=1,prob=P[2,1] * (1 - mu) + P[2,2] * mu)\n    \n  } else {\n    \n    out &lt;- rbinom(n=length(mu),size=1,prob=mu - bias_trans)\n    \n  }\n  \n  return(out)\n  \n}\n\n# define posterior expectation (equal to latent variable pi)\n\nposterior_epred_sens_reg &lt;- function(prep,...) {\n\n  mu &lt;- brms::get_dpar(prep, \"mu\")\n  bias &lt;- brms::get_dpar(prep, \"bias\")\n\n  mu\n\n}\n\nposterior_epred_bias_sens_reg &lt;- function(prep,...) {\n\n  mu &lt;- brms::get_dpar(prep, \"mu\")\n  bias &lt;- brms::get_dpar(prep, \"bias\")\n\n  bias*mu\n\n}\n\nP &lt;- getPW(\"Warner\",p=.25)\n\nall_stanvars &lt;- stanvar(x=P,block = \"data\") + \n  stanvar(scode=stan_funs,block=\"functions\")\n\nsurvey_data$age_cat_order &lt;- ordered(survey_data$age_cat)\n\nfit1 &lt;- brm(bf(kais_combined | vint(treatment) ~ gender*mo(age_cat_order) + (1|gov)), \n            data=survey_data,\n            family=family_sens_reg,\n            stanvars=all_stanvars,\n            prior=prior(beta(1,1),class=\"bias\") + \n              prior(normal(0,5), class=\"b\"),\n            chains=2,cores=2,control=list(max_treedepth=11,\n                                          adapt_delta=0.95),\n            backend = \"cmdstanr\")\n\nRunning MCMC with 2 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 44.9 seconds.\nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 61.2 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 53.1 seconds.\nTotal execution time: 61.3 seconds.\n\npp_check(fit1, type=\"bars\",ndraws=500)\n\n\n\n\n\n\n\nloo(fit1)\n\n\nComputed from 2000 by 880 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -576.8  8.9\np_loo        10.6  0.5\nlooic      1153.7 17.9\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\ngov_cats &lt;- ranef(fit1,groups = \"gov\")$gov[,,1] %&gt;% as_tibble %&gt;% \n  mutate(level=row.names(ranef(fit1,groups = \"gov\")$gov[,,1]))\n\nPlot this additional type of estimation:\n\nget_latent_draws &lt;- posterior_epred(fit1)\n\nget_latent_est &lt;- tibble(median=median(get_latent_draws),\n                         high=quantile(apply(get_latent_draws, 1, median),.95),\n                         low=quantile(apply(get_latent_draws, 1, median),.05))\n\n# get estimate of bias\n\nprep_bias &lt;- prepare_predictions(fit1)\n\nbias_trans &lt;- posterior_epred_bias_sens_reg(prep_bias)\n\nbias_trans_est &lt;- tibble(bias_est_low=quantile(apply(bias_trans,1,median),.05),\n                         bias_est=median(apply(bias_trans,1,median)),\n                         bias_est_high=quantile(apply(bias_trans,1,median),.95))\n\nest_data &lt;- tibble(estimate=c(s1$coefficients[,1],\n                              est_beta$median[est_beta$variable==\"pi_hat\"],\n                              pred_rr_mean$est,\n                              get_latent_est$median),\n                   low=c(s1$coefficients[,1]-1.96*(s1$coefficients[,2]),\n                         est_beta$q5[est_beta$variable==\"pi_hat\"],\n                         pred_rr_mean$ci.lower,\n                         get_latent_est$low),\n                   high=c(s1$coefficients[,1]+1.96*(s1$coefficients[,2]),\n                         est_beta$q95[est_beta$variable==\"pi_hat\"],\n                         pred_rr_mean$ci.upper,\n                         get_latent_est$high),\n                   estimator=c(\"RRreg\",\"Bayes\",\"EM\",\"Bayes_Reg\"))\n\nest_data %&gt;% \n  ggplot(aes(y=estimate,x=estimator)) +\n  geom_linerange(aes(ymin=low,\n                     ymax=high)) +\n  geom_point() + theme_tufte()\n\n\n\n\n\n\n\nknitr::kable(bias_trans_est)\n\n\n\n\nbias_est_low\nbias_est\nbias_est_high\n\n\n\n\n0.1356792\n0.229125\n0.3196246\n\n\n\n\n\nAgain, we see that all of the estimates are quite similar. In the last section we compare them directly with a simulation, which shows that the two Bayesian estimators are both accurate and have good coverage, while RRreg is accurate but understates uncertainty in general."
  },
  {
    "objectID": "posts/kais_saied_results/index.html#adjustment-with-mrp",
    "href": "posts/kais_saied_results/index.html#adjustment-with-mrp",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Adjustment with MRP",
    "text": "Adjustment with MRP\nGiven that the data come from an online panel, we want to use Tunisian 2014 census data to adjust these findings by age, sex and governorate. This will account for a considerable (though not all) amount of sample selection bias due to the online survey frame. We will use random effects to model the influence of the sample frame on the outcome.\n\nhead(census)\n\n# A tibble: 6 × 6\n  gender gov     pop age_cat age_cat_order    prop\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;ord&gt;           &lt;dbl&gt;\n1 Male   Tunis 17446 15-19   15-19         0.00120\n2 Female Tunis 16984 15-19   15-19         0.00117\n3 Male   Tunis 54142 20-24   20-24         0.00373\n4 Female Tunis 51111 20-24   20-24         0.00352\n5 Male   Tunis 48773 25-29   25-29         0.00336\n6 Female Tunis 45668 25-29   25-29         0.00315\n\n\nWe will predict the sensitive trait for each census category, then stratify by summing over the proportion of population in each cell from the census data. We then plot this adjusted estimate.\n\n# do for each condition, then average\n\ncensus_treat &lt;- mutate(census, treatment=1)\ncensus_notreat &lt;- mutate(census, treatment=0)\n\npred_data &lt;- bind_rows(census_treat,\n                       census_notreat) %&gt;% \n  mutate(prop=prop/2)\n\ntunisia_pred &lt;- posterior_epred(fit1, newdata=pred_data) %&gt;% \n  as_tibble %&gt;% \n  mutate(draw=1:n()) %&gt;% \n  gather(key=\"key\",\n         value=\"estimate\",-draw) %&gt;% \n  mutate(draw=paste0(\"V\",draw)) %&gt;% \n  spread(key=\"draw\",value=\"estimate\") %&gt;% \n  mutate(key=as.numeric(stringr::str_remove(key, \"V\")))\n\ntunisia_pred &lt;- left_join(tunisia_pred, \n                          mutate(pred_data, key=1:n())) %&gt;% \n  left_join(census) %&gt;% \n  gather(key = \"draw\",\n         value= \"estimate\",\n         matches(\"V\",ignore.case=F))\n\n# aggregate to highest level\n\nagg_est &lt;- tunisia_pred %&gt;% \n  group_by(draw) %&gt;% \n  summarize(est_adj = sum(estimate * prop))\n\nagg_est %&gt;% \n  ggplot(aes(x=est_adj)) +\n  geom_density(fill=\"blue\",alpha=0.5,\n               colour=NA) + theme_tufte() +\n  labs(y=\"\",x=\"Percent Opposing Saied's Coup\") +\n  scale_x_continuous(labels=scales::percent_format()) +\n  geom_vline(aes(xintercept=mean(est_adj)),linetype=2) +\n   ggtitle(\"Estimated Number Who Oppose Saied's 2021 Coup\",\n          subtitle=\"Based on Statistical Analysis of Randomized Response Question\") +\n  annotate(\"text\",\n           x=.52,y=4,label=paste0(\"Most likely number: \",round(mean(agg_est$est_adj),\n                                                               3)*100,\"%\"),\n           hjust=0)\n\n\n\n\n\n\n\nggsave(\"num_oppose.png\",width=5,height=3)\n\n{quantile(agg_est$est_adj, c(0.05,.5,.95))}\nA difference of approximately 1 percent between the adjusted estimate and the naive number:\n\nquantile(apply(get_latent_draws, 1, mean),c(0.05,.5,.95))\n\n       5%       50%       95% \n0.4351862 0.5194791 0.6011222 \n\n\nNext we can aggregate these results to look at gender, age and governorate in terms of relative levels of Saied support. Given that there is substantial uncertainty due to the sensitive question design, these results are somewhat noisy and should be interpreted with caution.\n\nGender\nWhen we aggregate to the level of gender, we see that men are approximately 5% more likely than women to report opposition President Kais Saied.\n\n# merge in census data\nplot_data_gender &lt;- tunisia_pred %&gt;% \n  group_by(gender,draw) %&gt;% \n  mutate(prop_gender=pop/sum(pop)) %&gt;% \n  summarize(est_gender=sum(estimate*prop_gender)) %&gt;% \n  group_by(gender) %&gt;% \n  summarize(median_gender=median(est_gender),\n            low_gender=quantile(est_gender,.05),\n            high_gender=quantile(est_gender,.95))\n\nselect(plot_data_gender,\n       y=\"median_gender\",\n       x=\"gender\",\n       high_y=\"high_gender\",\n       low_y=\"low_gender\") %&gt;% \n  write_csv(\"plot_data_gender.csv\")\n\n  plot_data_gender %&gt;% \n  ggplot(aes(y=median_gender,\n             x=gender)) +\n  geom_pointrange(aes(ymin=low_gender,\n                      ymax=high_gender)) +\n  scale_y_continuous(labels=scales::percent) +\n    labs(\"% Opposing Saied\") +\n  theme_tufte()\n\n\n\n\n\n\n\n\n\n\nAge\nThe plot below shows aggregated opposition by age. Approximately 60 percent of the youngest age category (18 to 19 year olds) oppose President Saied while only 40 percent of the oldest age category (80 and over) oppose President Saied. This pattern is noticeably stronger than that for gender, though there is still considerable uncertainty.\n\nplot_data_age &lt;- tunisia_pred %&gt;% \n  group_by(age_cat,draw) %&gt;% \n  mutate(prop_age_cat=pop/sum(pop)) %&gt;% \n  summarize(est_age_cat=sum(estimate*prop_age_cat)) %&gt;% \n  group_by(age_cat) %&gt;% \n  summarize(median_age_cat=median(est_age_cat),\n            low_age_cat=quantile(est_age_cat,.05),\n            high_age_cat=quantile(est_age_cat,.95)) %&gt;% \n  mutate(age_cat=recode(age_cat, `80 year and more`=\"80+\")) \n  \n  select(plot_data_age,\n       y=\"median_age_cat\",\n       x=\"age_cat\",\n       high_y=\"high_age_cat\",\n       low_y=\"low_age_cat\") %&gt;% \n  write_csv(\"plot_data_age.csv\")\n\n  plot_data_age %&gt;% \n  ggplot(aes(y=median_age_cat,\n             x=age_cat)) +\n  geom_pointrange(aes(ymin=low_age_cat,\n                      ymax=high_age_cat)) +\n    scale_y_continuous(labels=scales::percent) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2))+\n  labs(y=\"% Opposing Saied\",x=\"\") +\n  ggtitle(\"Number Opposing Saied by Age Category\",\n         subtitle=\"Encrypted (Randomized) Response Question\") +\n   theme_tufte()\n\n\n\n\n\n\n\nggsave(\"age_cat.png\",width=5,height=3)\n\n\n\nRegion\nFinally, the plot below shows predicted opposition by region. In general, more rural districts like Kebili, Beja and Kairouan report more opposition, but these differences are quite noisy. Differences between regions are not especially pronounced in the data.\n\nplot_data_region &lt;- tunisia_pred %&gt;% \n  group_by(gov,draw) %&gt;% \n  mutate(prop_gov=pop/sum(pop)) %&gt;% \n  summarize(est_gov=sum(estimate*prop_gov)) %&gt;% \n  group_by(gov) %&gt;% \n  summarize(median_gov=median(est_gov),\n            low_gov=quantile(est_gov,.05),\n            high_gov=quantile(est_gov,.95))\n\n\nselect(plot_data_region,\n       x=\"median_gov\",\n       y=\"gov\",\n       high_x=\"high_gov\",\n       low_x=\"low_gov\") %&gt;% \n  write_csv(\"plot_data_region.csv\")\n\nplot_data_region %&gt;% \n  ggplot(aes(y=median_gov,\n             x=reorder(gov,median_gov))) +\n  coord_flip() +\n    labs(\"% Opposing Saied\") +\n  geom_pointrange(aes(ymin=low_gov,\n                      ymax=high_gov)) + theme_tufte()"
  },
  {
    "objectID": "posts/kais_saied_results/index.html#simulation-comparison-univariate-vs.-regression",
    "href": "posts/kais_saied_results/index.html#simulation-comparison-univariate-vs.-regression",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Simulation Comparison: Univariate Vs. Regression",
    "text": "Simulation Comparison: Univariate Vs. Regression\nIn this simulation we test the coverage and unbiasedness of the estimators, including our preferred Bayesian specification. The simulation shows that the estimators perform quite well at recovering the sensitive trait, and our Bayesian models have excellent coverage (the uncertainty they report is reasonable). We’ll use the regression spec as the DGP along with our observed parameters for this study.\n\nlibrary(parallel)\n\n# confusion matrix\n\nP &lt;- getPW(\"Warner\",.25)\n\n# assume N = 500, small sensitivity of 0.05\n\ntheta &lt;- 0.3\nN &lt;- 800\nbias &lt;- 0.1\n\nsims &lt;- 500\n\nif(run_sim) {\n  \n  over_sims &lt;- lapply(1:sims, function(s) {\n  \n  # assign half to treatment, half to control\n  \n  treatment &lt;- as.numeric(runif(N)&gt;0.5)\n  \n  obs_response &lt;- ifelse(treatment==1,\n                         as.numeric((P[2,1] * (1 - theta) + P[2,2]*theta)&gt;runif(N)),\n                         as.numeric((theta - bias)&gt;runif(N)))\n  \n  out_data &lt;- tibble(y = obs_response,\n                     treatment=treatment)\n  \n  # define custom family as being upper and lower bounded for bias\n  \n  family_sens_reg &lt;- custom_family(\"sens_reg\",\n                                 dpars=c(\"mu\",\"bias\"),\n                                 links=c(\"logit\",\"logit\"),\n                                 type=\"int\",\n                                 lb=c(NA,0),\n                                 ub=c(NA,1),\n                                 vars=c(\"P\",\"vint1[n]\"))\n  \n  # estimate model with brms\n\n  est_mod_brms &lt;- brm(y | vint(treatment) ~ 1,\n                      family=family_sens_reg,\n                      stanvars=all_stanvars,\n                      data=out_data,\n                      #prior=prior(normal(0,10),class=\"bias\"),\n                      prior=prior(beta(1,1),class=\"bias\") +\n                        prior(normal(-0.84,10),class=\"Intercept\"),\n                      chains=1,\n                      cores=1,\n                      iter=1000,\n            backend = \"cmdstanr\")\n  \n  get_est_brms &lt;- posterior::summarise_draws(as_draws(est_mod_brms, \"b_Intercept\"),\n                                             estimate=~median(plogis(.x)),\n                                             high=~quantile(plogis(.x),.95),\n                                             low=~quantile(plogis(.x),.05)) %&gt;% \n    mutate(param=\"mu\") \n  \n  # calculate bias transformed\n  \n  bias_est &lt;- as_draws_matrix(est_mod_brms, \"bias\")\n  \n  mu_trans &lt;- posterior_epred(est_mod_brms) %&gt;% apply(1,median)\n\n  bias_trans_est &lt;- tibble(`95%`=quantile(bias_est * mu_trans,.95),\n                         estimate=median(bias_est * mu_trans),\n                         `5%`=quantile(bias_est * mu_trans,.05)) %&gt;% \n    mutate(param=\"bias\")\n  \n  get_est_brms &lt;- bind_rows(get_est_brms, bias_trans_est) %&gt;% \n    mutate(model=\"Bayes BRMS\")\n  \n  # compare with RRreg\n  \n  est_freq &lt;- RRuni(response=obs_response[treatment==1],\n                  p=.25,\n                  model=\"Crosswise\",MLest = TRUE)\n  \n  this_sum &lt;- summary(est_freq)\n  \n  # now do cheap Bayes\n  \n  lambda &lt;- mean(obs_response[treatment==1])\n  \n  sim_data2 &lt;- list(success_cm=(sum(treatment))*((lambda - .75)/(-.5)),\n                     fail_cm= (sum(treatment))*(1-((lambda - .75)/-.5)),\n                     success_d=sum(obs_response[treatment==0]),\n                     fail_d=sum(1 - obs_response[treatment==0]))\n\n  fit_simple &lt;- sen_mod$sample(chains=1, cores=1, data=sim_data2,refresh=0)\n  \n  simple_sum &lt;- fit_simple$summary()\n  \n  mu &lt;- try(tibble(estimates=c(simple_sum$median[simple_sum$variable==\"pi_hat\"],\n                     this_sum$coefficients[1,1],\n                     get_est_brms$estimate[get_est_brms$param==\"mu\"]),\n         q5=c(simple_sum$q5[simple_sum$variable==\"pi_hat\"],\n                     this_sum$coefficients[1,1] - 1.96*this_sum$coefficients[1,2],\n              get_est_brms$`5%`[get_est_brms$param==\"mu\"]),\n         q95=c(simple_sum$q95[simple_sum$variable==\"pi_hat\"],\n                     this_sum$coefficients[1,1] + 1.96*this_sum$coefficients[1,2],\n               get_est_brms$`95%`[get_est_brms$param==\"mu\"]),\n         models=c(\"RRreg\",\"Bayes Simple\",\"Bayes BRMS\"),\n         param=\"mu\") %&gt;% \n    mutate(cov=theta &gt; q5 & theta &lt; q95,\n           sim=s))\n  \n  bias_est &lt;- try(filter(get_est_brms, param==\"bias\") %&gt;% \n    select(estimates=\"estimate\",\n           q5=\"5%\",\n           q95=\"95%\") %&gt;% \n    mutate(cov=bias &gt; q5 & bias &lt; q95,\n           sim=s,\n           param=\"bias\",\n           models=\"brms\"))\n  \n  try(bind_rows(mu, bias_est))\n   \n  \n}) %&gt;% bind_rows\n  \n  saveRDS(over_sims, \"data/over_sims.rds\")\n  \n} else {\n  \n  over_sims &lt;- readRDS(\"data/over_sims.rds\")\n  \n}\n\n\n\nover_sims %&gt;% group_by(models,param) %&gt;% \n  summarize(mean_cov=mean(cov)) %&gt;% \n  knitr::kable(caption=\"Coverage\")\n\n\nCoverage\n\n\nmodels\nparam\nmean_cov\n\n\n\n\nBayes BRMS\nmu\n0.906\n\n\nBayes Simple\nmu\n0.956\n\n\nRRreg\nmu\n0.568\n\n\nbrms\nbias\n0.908\n\n\n\n\nover_sims %&gt;% group_by(models,param) %&gt;% \n  summarize(spread_CIs=sum(q95 - q5)) %&gt;% \n  knitr::kable(caption=\"Total Variance\")\n\n\nTotal Variance\n\n\nmodels\nparam\nspread_CIs\n\n\n\n\nBayes BRMS\nmu\n71.44307\n\n\nBayes Simple\nmu\n96.11747\n\n\nRRreg\nmu\n37.30712\n\n\nbrms\nbias\n73.93706\n\n\n\n\n\nNow let’s check bias.\n\n#RMSE\n\nover_sims %&gt;% group_by(models,param) %&gt;% \n  summarize(mean_rmse=switch(unique(param),\n                             mu=sqrt(mean((estimates - theta)^2)),\n                             bias=sqrt(mean((estimates - bias)^2)))) %&gt;% \n  knitr::kable(caption = \"RMSE\")\n\n\nRMSE\n\n\nmodels\nparam\nmean_rmse\n\n\n\n\nBayes BRMS\nmu\n0.0419459\n\n\nBayes Simple\nmu\n0.0485183\n\n\nRRreg\nmu\n0.0487269\n\n\nbrms\nbias\n0.0459833\n\n\n\n\nover_sims %&gt;% group_by(models,param) %&gt;% \n  summarize(abs_bias=switch(unique(param),\n                             mu=mean(abs(estimates - theta)),\n                             bias=mean(abs(estimates - bias)))) %&gt;% \n  knitr::kable(caption = \"Mean Absolute Bias\")\n\n\nMean Absolute Bias\n\n\nmodels\nparam\nabs_bias\n\n\n\n\nBayes BRMS\nmu\n0.0341473\n\n\nBayes Simple\nmu\n0.0388322\n\n\nRRreg\nmu\n0.0390247\n\n\nbrms\nbias\n0.0377768"
  },
  {
    "objectID": "posts/frac_logit/index.html",
    "href": "posts/frac_logit/index.html",
    "title": "Fixing Fractional Logit?",
    "section": "",
    "text": "This post focuses on one of the more curious models in contemporary statistics, a specification for proportions that is either called fractional logit or quasi-Binomial. An earlier version of this blog post had a much more negative take on the fractional logit specification. After dialogue with people on Twitter (who knew it could be useful??), I have revised this blog post to take into account other perspectives. As such, this blog post is now framed to be more open-ended. I still think there are issues with the utility of the fractional logit model, but understood within the motivation behind it, it does have a use. The question is when you would want to use fractional logit as opposed to a full-fledged stqtistical distribution like the continuous Bernoulli, ordered beta or zero-or-one inflated beta regression models.\nTo sum up my take for those who don’t want to wade through the analysis, I think that fractional logit is probably best applied to situations with big data where computational issues are likely to arise but the correct form of uncertainty is less of an issue. In these situations, fractional logit is an alternative to OLS that respects the bounds of proporitional/fractional outcomes, and is significantly easier to fit than the other alternatives discussed here. In addition, having a large dataset means that inefficiency or an incorrect form for the uncertainty of fractional logit estimates is unlikely to affect decision-making or inference.\nWhere I see less utility for fractional logit is where continuous Bernoulli but more so the beta regression models are valid alternatives. In these cases, fractional logit is supposed to guard against mis-specification worries, but it is not entirely clear how strong that worry can/should be given the specific domain of bounded continuous variables–i.e., there are only so many alternatives. In addition, it is difficult to characterize the performance of fractional logit vis-a-vis other models, which also makes it difficult to know when the specification issue might become more important than other facets of statistical distributions like fit and interpretability.\nIn this revised blog post I include the code and examples from the original post along with additional analyses that get into some of the Twitter feedback along with some of my previous evaluations of fractional logit. My hope is that it communicates honestly the state of research about these models and helps people make an informed choice.\nIf you want the code for this post, you can see the full Rmarkdown file here."
  },
  {
    "objectID": "posts/frac_logit/index.html#the-problem-what-a-proportion-is",
    "href": "posts/frac_logit/index.html#the-problem-what-a-proportion-is",
    "title": "Fixing Fractional Logit?",
    "section": "The Problem: What a Proportion Is",
    "text": "The Problem: What a Proportion Is\nWe can first start with the definition of the specification that is primarily used for fractional logit (abbreviated FL from here on), which comes from a 1996 paper by Papke and Wooldridge. Given a linear model \\(X\\beta\\), a link function \\(g(\\cdot)\\) (logit), and an outcome \\(Y_i\\) that can take on any value between 0 and 1, we have a likelihood as follows:\n\\[\nL(Y_i) = g(x_i\\beta)^{Y_i}(1-g(x_i\\beta))^{(1 - Y_i)}\n\\]\nThis is not the only likelihood or estimation possible for FL. Technically, the model is defined as the following:\n\\[\nE(Y|X) = g(X\\beta)\n\\]\nIn other words, it only has to make an assumption about how we determine the expected value (average response) of the outcome \\(Y\\). That assumption is that the expected value is equal to a linear model (\\(X\\beta\\)) scaled by the logit function to map on to a proportion (strictly between 0 and 1). I’ll discuss more below what this means in practice. For my purposes, I’ll stick with the likelihood above as that is what has been primmarily used to get estimates of the regression coefficients \\(\\beta\\).\nTo get the intuition behind the likelihood, the definition is in fact the same for the Bernoulli distribution. The only difference is that instead of \\(y_i\\) being a binary variable with values of 0 or 1, \\(y_i\\) could be anything from 0 to 1, including 0.2, 0.4, etc. Inserting these continuous values into a discrete distribution is the clever method that Papke and Wooldridge came up with and labeled fractional for a fraction of 0 and 1. They also showed that it was possible to find a maximum to the likelihood function and obtain estimates for the regression coefficients \\(\\beta\\) on the logit scale.\nThis model represented a substantial step forward from what was available at the time, mainly OLS. There were plenty of GLMs but not for a fractional or proportional response. No one was using beta regression as that model would not be proposed until 2004. For these reasons, fractional logit became very popular and has almost 5,000 citations according to Google Scholar.\nHowever, this might lead to some head-scratching–sticking continuous data in a discrete distribution is, at a minimum, kind of odd. I was critical in my prior version of this blog post because this seemed to me be an objectively worse specification than using a fully formed statistical distribution. However, in my Twitter conversation with Jeffrey Wooldridge and others, the response essentially was, “you can’t fix it because it’s already broke.” In other words, the fractional logit’s lack of connection to a specific distribution is seen as a way of guarding against mis-specification. Or as Prof. Wooldridge put it,\n\n\nSimulation studies aren't even needed. A theorem is a theorem. FL is consistent whenever E(Y|X) is correct. MLE never is in this context. The only issue is efficiency loss compared with MLE when D(Y|X) is fully specified. But that's moving the goalpost for the FL estimator.— Jeffrey Wooldridge (@jmwooldridge) February 20, 2023\n\n\n\nFL in the tweet refers to fractional logit, and \\(E(Y|X)\\) is the expected value of a proportional outcome (0 to 1 inclusive). What Prof. Wooldridge is saying is that the FL estimator is consistent, or will on average be correct, whenever the simple linear model \\(g(X\\beta)\\) is equal to the average value of the outcome. In other words, the statistical distribution doesn’t matter if all we care about is the expected, or average, value of the response.\nTo explain what is going on, it’s helpful to know what the logit function \\(g(\\cdot)\\) does. The logit function scales any number (large or small, negative or positive) to a number strictly within 0 and 1. As a result, saying that the average value is equal to \\(g(X\\beta)\\) is really just saying that the average value is equal to the linear model, which is of course what people expect when running a regression. As such, the model captures a critical part of regression model: how are the covariates on average related to the outcome/response?\nWhat is a bit odd is that fractional logit has a relationship for the expected value \\(E(Y|X)\\) but not for the distribution of \\(Y\\). As another tweeter put it,\n\n\n@rmkubinec To resolve the debate, it’s crucial to clarify the estimand:(i) D(Y|X), D is the dist; or(ii) E(Y|X)For (i), it’s necessary to correctly specify D(Y|X), In this case, CB only works for CB, ordBeta only works for ordBeta, etc.For (ii), it’s not necessary to…1/3— Lihua Lei (@lihua_lei_stat) February 21, 2023\n\n\n\nHere Prof. Lei’s comment is about separating the average/expected value of the response \\(E(Y|X)\\) from the distribution of \\(Y\\), denoted \\(D(Y|X)\\). CB stands for continuous Bernoulli and ordbeta for ordered beta regression, two other models for proportional outcomes I will explain later. Prof. Lei’s point is that if we don’t care about the distribution of \\(Y\\), we can content ourselves with the expected value, which fractional logit can estimate regardless of what \\(D(Y|X)\\) is."
  },
  {
    "objectID": "posts/frac_logit/index.html#ok-so-what",
    "href": "posts/frac_logit/index.html#ok-so-what",
    "title": "Fixing Fractional Logit?",
    "section": "OK So What?",
    "text": "OK So What?\nThe reader at this point might think, OK, so I’m estimating the expected value of the response with fractional logit, not the full distribution. What’s going to happen to me, the statistical gods will strike me with lightning?1\nThe main drawback of not having a distribution is that we don’t know how certain we can be about the average value \\(E(Y|X)\\) because we don’t ever know the true (or so-called population) value. In frequentist statistics, which is the underlying basis for the paper cited above, the population value is the value we would obtain if we sampled \\(Y\\) over and over again and kept taking averages for \\(E(Y|X)\\) and if we kept averaging all of those averages we would know the “real” \\(E(Y|X)\\). But, in real life, we don’t know that value, just an estimate of \\(E(Y|X)\\), which we can denote \\(\\widehat{E(Y|X)}\\). If our estimator is consistent, as FL is, then we know that \\(\\widehat{E(Y|X)}\\) is an unbiased estimate for \\(E(Y|X)\\)--but only if we were to take many samples \\(\\widehat{E(Y|X)}\\) and average them together. In most cases we have just one dataset, and we want to know how much we can learn about \\(E(Y|X)\\) from our regression coefficients that make up \\(\\widehat{E(Y|X)}\\).2\nFor these reasons, if you read the Papke and Wooldridge paper, you’d find that it spends relatively little time on the specification of model/likelihood and instead most of its time on determining standard errors (and confidence intervals) for the regression coefficients that estimate \\(\\widehat{E(Y|X)}\\). You can derive confidence intervals using maximum likelihood (MLE), but that would treat the FL specification as essentially being a part of the Bernoulli distribution, which it isn’t. As Papke and Wooldridge note, this naive estimate of the uncertainty will be over-confident. For these reasons, they propose a standard error correction that inflates standard errors in a similar manner to the so-called “sandwich” estimator (see the R package sandwich). These corrected standard errors, Papke and Wooldrige argue, are asympotically correct for any distribution \\(D(Y|X)\\), meaning that if we kept drawing samples from any distribution and recorded the average value \\(\\widehat{E(Y|X)}\\) using FL, and then took the average of those samples, it would equal the same expected value as if we also calculated those average values using the appropriate model for \\(D(Y|X)\\) like beta regression. This feature of the distribution is, in other words, what Prof. Lei sees as the big feature of FL.\nI can see the appeal of this trait of the model, but it is important to note that in statistics as in all of life, there is no such thing as a free lunch. The so-called bias-variance trade-off ensures that is almost never going to happen. I see the drawbacks as the following:\n\nWe have limited insight into how the standard error correction works. Papke and Wooldridge’s claim is that it is consistent, which would only hold across repeated samples, or what is known as asymptotically. We don’t know how well it would work in finite samples (as in the data we actually have), especially in terms of efficiency loss or how much we have to over-estimate uncertainty to avoid false positives. It’s hard to test this as we only have a set number of possibilities that we are aware of for \\(D(Y|X)\\), and how exactly FL does compared to each \\(D(Y|X)\\) may vary (i.e. beta regression versus continuous Bernoulli versus zero-one-inflated beta versus some model we aren’t even aware of yet).\nWe have a consistent estimate for \\(E(Y|X)\\), but this applies only to the regression coefficients and the uncertainty of those coefficients. If we wanted to calculate some non-linear combination of coefficients, we would have to adjust that uncertainty as well. We might not be able to use bootstrapping given that the sampling model is “wrong” (though I didn’t see any discussion of this in the paper). As such, it will be more tricky to use the estimates beyond making inferences, i.e. sign + significance.\n\nTo give an example, we cannot make model predictions that include 0s and 1s. The logit function only produces numbers strictly between 0 and 1. This makes sense if you think about it—how could the outcome on average be equal to exactly 1 or 0? The probability of any particular value of a continuous random variable is always 0. If we sample from our estimate \\(\\widehat{E(Y|X)}\\), we can only get draws for averages, not for individual variates. In other words, we might know that the average probability that white-collar workers turn out to vote in Minnesota is 33%, but we won’t know if that’s because the probability is close to 0 for most Minnesotans and close to 1 for a few or if the majority of Minnesotans have a roughly 20 - 40% chance of turning out to vote. Both of these distributions would have the same expected or average value of turning out to vote.\n\n\nI am particularly concerned about #1. The standard error correction could be huge; there is nothing that constrains the variance of the estimates. In my paper on ordered beta regression, I simulated data and compared fractional logit with beta regression, OLS, and other alternatives, and its performance was hard to predict. On the one hand, it was closer to the correct value for \\(E(Y|X)\\) than OLS, which would fit with Papke and Wooldridge’s arguments. The estimates were also over-confident in terms of uncertainty, which again matches their work given that I did not adjust the confidence intervals (it is not clear how that would exactly be done in a Bayesian framework). But, in the empirical example, the FL estimates had uncertainty that was a couple of orders of magnitude greater than any of the other models (i.e., really wide CIs).\nThe question about the utility of FL seems to depend on how much we should be concerned about the possibility of mis-specification (we have the wrong distribution for \\(Y\\)) as opposed to inflating our uncertainty unnecessarily (and restricting what we can use our models for). Again, very similar to the bias-variance tradeoff."
  },
  {
    "objectID": "posts/frac_logit/index.html#what-are-the-alternatives",
    "href": "posts/frac_logit/index.html#what-are-the-alternatives",
    "title": "Fixing Fractional Logit?",
    "section": "What Are the Alternatives?",
    "text": "What Are the Alternatives?\nOne way of framing this is what are the possibilities for \\(D(Y|X)\\) and how divergent are they? This is where it gets somewhat difficult as \\(Y\\) is a bounded outcome, and so, at least in my opinion, our alternative models seem somewhat limited.\nThe most direct comparison, and the specification that prompted this paper, is what has become known as the continuous Bernoulli distribution, which was derived by people working in the machine learning literature. Gabriel Loaiza-Ganem and John Cunningham looked at variational auto-encoders, which are models for pixels that make up images. Apparently people using these models had been employing something like fractional logit (for prediction, not inference), and they were likewise concerned about the fact that this specification was not a true statistical distribution. They went as far as identifying what the normalization constant is required to add to the fractional logit model to make it a true distribution, which turns out to be the following for a given value of the linear predictor \\(g(X\\beta)\\), which I denote as \\(\\mu\\) (see their eqn (7)):\n\\[\nC(\\mu) = \\begin{cases}\n  \\frac{2 \\text{tanh}^{-1}(1 - 2\\mu)}{1 - 2\\mu}  & \\text{if } \\mu \\ne 0.5 \\\\\n  2 & \\text{otherwise}\n\\end{cases}\n\\]\nThis thing, to be honest, is kind of ugly, and has a fixed point at 0.5 or 50%, meaning that there is a point where the value of the outcome and the combined value of the covariates must be equal to 0.5. The authors decided to name this distribution, which could be thought of as fractional logit with the minimal number of changes to make it a full distribution, as the continuous Bernoulli distribution. I am cutting and pasting their Figure 1 to show what the distribution looks like:\n\n\n\nFigure 1 from Loaiza-Ganem and Cunningham (2019)\n\n\nEssentially, the distribution allows for mildly sloping lines across 0 to 1 that can be either upward or downward. The value of the normalizing constant \\(C(\\mu)\\) in the leftward panel is much bigger towards the boundaries of the distribution, which intuitively makes sense. As the distribution moves towards extreme values, the denominator has to change to take into account the non-linearity in the outcome.\nFurthermore, they show in the paper that what they call the lack of normalization has a clear impact on performance. They use an image-learning task and examine how continuous Bernoulli without normalization (i.e., vanilla fractional logit) compares to continuous Bernoulli with normalization. Again, I’ll copy and paste their key result here:\n\nIn this figure, CB represents continuous Bernoulli and B is the fractional logit. As can be seen, the images are much sharper with continuous Bernoulli (normalized) than fractional logit. The authors point out that this is likely due to the normalizing constant becoming so large towards the extremes of the distribution: the un-normalized distribution has a hard time knowing where the boundaries are.\nThis is an intriguing result, though it is important to note that the original FL model says almost nothing about predictive validity, but rather about making inferences about regression coefficients. Machine learning has a different set of goalposts, so it might not be surprising that FL just doesn’t cut it. In either case, we do end up with a new distribution for \\(Y\\) that very close to FL. I wrote this blog post in part to highlight this new model. Whether CB is a true alternative or fix for FL is difficult to say as FL estimates are supposed to average across all possible distributions (for good or ill). However, what can be said is that it appears to the specification that makes the minimum number of necessary statements about the distribution of \\(Y\\) while providing an estimate for the average value \\(E(Y|X)\\) as well."
  },
  {
    "objectID": "posts/frac_logit/index.html#example",
    "href": "posts/frac_logit/index.html#example",
    "title": "Fixing Fractional Logit?",
    "section": "Example",
    "text": "Example\nThis distribution is not available currently in R, though it can be implemented fairly straightforwardly in Stan. It is also available in tensorflow in Python, but as I’m not primarily a Python user, I’ll stick with R. I produce code below that can fit this model with the R package brms as a custom family, and in the future I plan to add support for it to ordbetareg. I still think ordered beta regression makes more sense as a default, especially with the issues with the normalizing constant in the continuous Bernoulli, but it is great to have this model as another robust alternative for bounded continuous variables.\nTo demonstrate how to fit continuous Bernoulli, I first generate data using the rordbeta function in ordbetareg that will create proportion data from 0 to 1 inclusive. I’ll add a covariate X to predict the mean of the distribution on the logit scale (which continuous Bernoulli also uses) with a coefficient of 2.5:\n\nlibrary(ordbetareg)\n\nN &lt;- 500\nX &lt;- runif(n=N)\nY &lt;- rordbeta(n=N, mu = plogis(-2 + 2.5*X),cutpoints=c(-3,3))\n\nhist(Y)\n\n\n\n\n\n\n\n\nThe outcome has a distinct U-shape and 70 discrete responses (0 or 1).\nThe code below defines the custom family for brms to work (the model will soon be available in my package ordbetareg):\n\nc_bernoulli &lt;- custom_family(\"c_bernoulli\",\n                             dpars=\"mu\",\n                             links=\"logit\",\n                             lb=0,ub=1,\n                             type=\"real\")\n\n# define log density function\n# some code from spinkey https://discourse.mc-stan.org/t/continuous-bernoulli/26886\n\nstan_funs &lt;- \"\n\n  //normalization constant\n  real c_norm(real mu) {\n  \n    if(mu==0.5) {\n    \n      return log(2);\n        \n    } else {\n    \n      real const = (log(2 - 2*mu) - log(2*mu))/(2 * (1 - 2*mu));\n      return(log(const));\n                \n    }\n  \n  }\n  \n  // log PDF for continuous Bernoulli\n  real c_bernoulli_lpdf(real y, real mu) {\n  \n    // unnormalized density\n    \n    real lp = y * log(mu) + (1 - y) * log1m(mu);\n    \n    // normalized density\n   \n    lp += c_norm(mu);\n      \n    return lp;\n    \n  }\"\n\nstanvars &lt;- stanvar(scode = stan_funs, block = \"functions\")\n\n# posterior predictions\n\nposterior_predict_c_bernoulli &lt;- function(i, prep, ...) {\n  \n  # need inverse CDF function for continuous bernoulli\n  \n  inv_cdf &lt;- function(i=NULL,mu=NULL,u=NULL) {\n    \n    mu &lt;- mu[i]\n    u &lt;- u[i]\n    \n    if(mu==0.5) {\n      \n      out &lt;- u\n      \n    } else {\n      \n      out &lt;- (log(u * (2 * mu - 1) + 1 - mu) - log(1 - mu))/(log(mu) - (log(1-mu)))\n      \n    }\n    \n    return(out)\n    \n  }\n  \n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  u &lt;- runif(n=length(mu))\n\n  sapply(1:length(mu),inv_cdf,mu,u)\n  \n}\n\nposterior_epred_c_bernoulli &lt;- function(prep) {\n  \n  # expected value\n  mu &lt;- brms::get_dpar(prep, \"mu\")\n  \n  t(apply(mu, 1, function(r) {\n    \n    sapply(r, function(mu_i) {\n      \n      if(mu_i==0.5) {\n      \n      return(0.5)\n      \n    } else {\n      \n      (mu_i / (2 * mu_i - 1)) + (1 / (2*atanh(1 - 2*mu_i)))\n      \n      }\n      \n    })\n      \n  }))\n  \n  \n}\n\nfit_c_bern &lt;- brm(\n  Y ~ X, data = tibble(Y=Y, X=X),\n  family = c_bernoulli, stanvars = stanvars, backend=\"cmdstanr\",\n  refresh=0\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 1.0 seconds.\nChain 2 finished in 0.9 seconds.\nChain 3 finished in 0.9 seconds.\nChain 4 finished in 0.9 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.9 seconds.\nTotal execution time: 4.1 seconds.\n\nlibrary(marginaleffects)\n\navg_slopes(fit_c_bern)\n\n\n Term Estimate 2.5 % 97.5 %\n    X     0.51 0.443  0.568\n\nColumns: term, estimate, conf.low, conf.high \n\n\nThe avg_slopes function from the marginaleffects package gives the marginal effect of the parameter back-transformed to 0/1. The continuous Bernoulli estimates a very big marginal effect for X (it is not the same as the true coefficient because that was on the logit scale, not the true scale). We can plot the posterior predictive distribution:\n\npp_check(fit_c_bern)\n\n\n\n\n\n\n\n\nIt’s a bit off, but pretty close, and we probably shouldn’t expect it to be perfect given that the data was generated from the ordered beta distribution, not the continuous Bernoulli.\nWe can then compare those results to the ordered beta regression in ordbetareg:\n\nfit_ordbeta &lt;- ordbetareg(\n  Y ~ X, data = tibble(Y=Y, X=X),\n  backend=\"cmdstanr\",\n  refresh=0\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 2.1 seconds.\nChain 2 finished in 2.4 seconds.\nChain 3 finished in 2.3 seconds.\nChain 4 finished in 2.2 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.3 seconds.\nTotal execution time: 9.5 seconds.\n\navg_slopes(fit_ordbeta)\n\n\n Term Estimate 2.5 % 97.5 %\n    X    0.489 0.409  0.561\n\nColumns: term, estimate, conf.low, conf.high \n\npp_check(fit_ordbeta)\n\n\n\n\n\n\n\n\nThe marginal effect of X is fairly close to the continuous Bernoulli’s, and the posterior distribution is much closer to the true distribution. But again, we generated the data from the ordered beta distribution so it’s not surprising that the continuous Bernoulli estimate is different. It might take some time to figure out exactly where and when the distributions will differ. For this relatively simple example, the marginal effects of both distributions are fairly close."
  },
  {
    "objectID": "posts/frac_logit/index.html#so-what-do-we-do",
    "href": "posts/frac_logit/index.html#so-what-do-we-do",
    "title": "Fixing Fractional Logit?",
    "section": "So What Do We Do?",
    "text": "So What Do We Do?\nI think there are still some unanswered questions—which of these distributions will FL’s estimates match most closely? Notably, both CB and ordered beta have a defined expected value for \\(Y\\) that is very close to the logit function plus covariates, i.e. \\(g(X\\beta)\\). CB and FL are probably not going to produce estimates that are very different–the main difference is that CB’s estimates will be valid based on the uncertainty of the distribution, while FL’s estimates will be over-confident without using standard error corrections. Because ordered beta regression is making a much more nuanced statement about the distribution for \\(Y\\), its estimates of uncertainty could diverge even further from FL’s in finite samples.\nHowever, the main criteria we would want to know for FL is hard to define–how concerned are we about getting the wrong model? CB is an intriguing new model, but it is quite simple, and it’s not clear when it would be preferred over something more complicated yet also more robust like beta regression. After all, CB has the strange property of fixing the expected value of the outcome at 0.5. All models have uses, and I am sure this one does, but it would seem to best apply in situations where the limited possibilities for uncertainty match the research question closely.\nThat is why, in my opinion, FL makes the most sense when we have a lot of data. As the amount of data increases, confidence intervals will shrink regardless of the inflation factor we use. Furthermore, nuances in our estimation of uncertainty may be less noticeable as uncertainty disappears. FL has a noted advantage in that it is the easiest model to estimate–it is quite literally a standard Bernoulli (logit) model. CB is close but certainly has additional complications, while the beta regression has a lot more computation necessary. If computation is a limitation, FL strikes me as a useful alternative to OLS as it will produce estimates that are valid for a bounded dependent variable.\nI do think that further research would be useful to clarify these questions, especially comparing FL’s uncertainty under different distributions \\(D(Y|X)\\). These would be helpful for knowing when mis-specification of \\(D(Y|X)\\) is a real problem and when we might want to use something like FL that inflates uncertainty. My sense that beta regression (standard or ordered beta or zero-inflated beta) is still the best generic model for bounded continuous data as it fits these outcomes well and is a full-featured statistical distribution."
  },
  {
    "objectID": "posts/frac_logit/index.html#footnotes",
    "href": "posts/frac_logit/index.html#footnotes",
    "title": "Fixing Fractional Logit?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith some positive probability, yes.↩︎\nWith this statement I am slipping dangerously close to Bayesian inference, but well, probably a good thing if I do :).↩︎"
  },
  {
    "objectID": "posts/logs/index.html",
    "href": "posts/logs/index.html",
    "title": "What’s Logs Got to Do With It?",
    "section": "",
    "text": "Twitter (or what’s left of it) was recently ablaze with a discussion of two smart working papers, one by Jiafeng Chen and Jonathan Roth and the other by John Mullahy and Edward Norton. In different ways, they argue against the use of non-linear transformations in applied modeling, especially logarithms (logs). I had been meaning to discuss the subject myself; I’ve been critical of the logarithmic transformation previously because it has been difficult to put a clear meaning on the transformation in a linear model. Furthermore, the inability of logs to include 0 (log of 0 is undefined or negative infinity, whichever is smaller) was a warning flag that there was a mismatch between what people wanted the math to do, and what it actually did.\nWhat I’m going to show in this post is that the ordered beta regression model can also address issues with logs (and the related inverse hyperbolic sine transformation) because it can produce estimates (including ATEs) that are based on proportions, and thus naturally scale-free. When the scale of the outcome is an issue, the ordered beta regression can help address that problem by estimating regression coefficients or treatment effects that do not vary with scale and also include 0s.\nThe two papers presented above approach the log issue in different ways, which makes them both fascinating when read together. Mullahy and Norton take a more traditional econometric approach, discussing how the log and hyperbolic sine transformations are heavily influenced by the constant chosen to either “bump” zeros to 1s for the log or to scale the hyperbolic sine. Chen and Roth use different notation to discuss how the logarithmic transformation affects estimation of an average treatment effect (ATE) using the potential outcomes framework in the causal inference literature.\nBoth sets of authors point out that these transformations end up changing either linear regression coefficients or estimated ATEs in ways that are not immediately obvious. Using economic theory, Mullahy and Norton discuss how adding 1 to any zero value in a model in order to use logs results in the model weighting estimates by the proportion of 0s and non-0 values in the data, or what they call the “intensive” and “extensive” margin. The regression coefficient will vary depending on whether 0s are replaced by 1, 0.9, 0.8, 0.7… etc. There simply isn’t one good value that can be added to the data to allow a logarithm to be well-defined.\nChen and Roth make the point that the estimate of an ATE can vary with different scaling functions such as logarithms. As they put it,\nIn other words, once you transform the outcome using one of these functions, we no longer know for sure that we will get the same ATE if we change the units, i.e. from dollars to cents, when using one of these common transformations. Chen and Roth’s suggestion, which is similar as well to Mullahy and Norton, is to consider using models that incorporate 0s (two-part models, also known as hurdles/zero-inflated models) or simple GLMs like the Poisson regression, which has had an amazing resurgence on Twitter at the moment. (No idea, though, if Poisson’s momentum will last through the 2024 election.)"
  },
  {
    "objectID": "posts/logs/index.html#simulation",
    "href": "posts/logs/index.html#simulation",
    "title": "What’s Logs Got to Do With It?",
    "section": "Simulation",
    "text": "Simulation\nI next use simulations to discuss this issue to show practically what the authors are talking about and also demonstrate how ordbetareg can estimate scale-free ATEs.\nI simulate data that matches what the articles discuss: highly skewed outcomes with zeroes. To do so I’ll simulate a two-part or hurdle model in which the ATE influences the first part (the probability of a 0) and the second part (a conventional log-normal distribution strictly greater than 0). The ATE will influence both the probability of 0s, which at baseline is 10% of total observations, and the mean of the non-0 (log-normal) distribution. I will assume a true ATE of +1 on the log/logit scale. I can then simulate the potential outcomes, Y1 and Y0, as independent distributions with a difference of the ATE.\n\nate &lt;- 1\nN &lt;- 500\nY0 &lt;- ifelse(runif(N)&lt;0.9,rlnorm(N),0)\nY1 &lt;- ifelse(runif(N)&lt;plogis(qlogis(0.9) + ate),\n             rlnorm(N,meanlog=ate),\n             0)\n\nWe can take a quick glance at the distributions:\n\nhist(Y1)\n\n\n\n\n\n\n\nhist(Y0)\n\n\n\n\n\n\n\n\nThe proportion of zeroes in Y0 is 0.086 and the proportion of zeroes in Y1 is 0.034.\nWe can then do some conventional comparisons of the treatment and control group, a t-test and linear model fit:\n\nt.test(Y1,Y0)\n\n\n    Welch Two Sample t-test\n\ndata:  Y1 and Y0\nt = 11.45, df = 639.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.423978 3.427505\nsample estimates:\nmean of x mean of y \n 4.330918  1.405176 \n\nt_data &lt;- tibble(Y=c(Y0,Y1),T=c(rep(0,N),rep(1,N)))\nmod1 &lt;- lm(Y ~ T,data=t_data)\nsummary(mod1)\n\n\nCall:\nlm(formula = Y ~ T, data = t_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.331 -1.839 -0.795  0.423 48.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.4052     0.1807   7.777 1.84e-14 ***\nT             2.9257     0.2555  11.450  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.04 on 998 degrees of freedom\nMultiple R-squared:  0.1161,    Adjusted R-squared:  0.1152 \nF-statistic: 131.1 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThe linear model and the t-test do not return the true ATE as that was simulated on the log scale. However, if we take the log of the estimate, it is pretty close – log of 2.926 is 1.074. The simulation is not perfect because the ATE will vary across the two parts due to scaling effects, but it will work for our purposes.\nAs the papers pointed out, the ATE could change if we use nonlinear transformations such as log with an adjustment such as 1 or 0.1 to replace the zeroes. We will test this by fitting two models, one with an outcome of \\(log(Y+1)\\) and the other with \\(log(Y+0.01)\\). We can then make a table of the model results with the modelsummary package:\n\nt_data &lt;- mutate(t_data,\n                 Y_1=Y + 1,\n                 Y_01=Y + 0.01)\n\ny_mod1 &lt;- lm(log(Y_1) ~ T,data=t_data)\ny_mod2 &lt;- lm(log(Y_01) ~ T, data=t_data)\n\nmodelsummary(models=list(`log(Y + 1)`=y_mod1, \n                         `log(Y + 0.01)`=y_mod2))\n\n \n\n  \n    \n    \n    tinytable_bmsn8qr24098pqqlr6tc\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                log(Y + 1)\n                log(Y + 0.01)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.706   \n                  -0.430   \n                \n                \n                             \n                  (0.029) \n                  (0.067)  \n                \n                \n                  T          \n                  0.650   \n                  1.236    \n                \n                \n                             \n                  (0.041) \n                  (0.095)  \n                \n                \n                  Num.Obs.   \n                  1000    \n                  1000     \n                \n                \n                  R2         \n                  0.200   \n                  0.146    \n                \n                \n                  R2 Adj.    \n                  0.200   \n                  0.145    \n                \n                \n                  AIC        \n                  4042.5  \n                  4028.7   \n                \n                \n                  BIC        \n                  4057.2  \n                  4043.4   \n                \n                \n                  Log.Lik.   \n                  -987.324\n                  -1822.961\n                \n                \n                  F          \n                  250.073 \n                  169.999  \n                \n                \n                  RMSE       \n                  0.65    \n                  1.50     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can see that the treatment effects are quite different. To get them back on the same scale, we can try a reverse transform (exponential the coefficient):\n\nexp(coef(y_mod1)[2]) - 1\n\n        T \n0.9159342 \n\nexp(coef(y_mod2)[2]) - .01\n\n       T \n3.433134 \n\n\nWe can see that the reverse transform doesn’t improve matters. In either case we end up with a very different coefficient. If we were to interpret the outcome as dollars, such as a salary, then the ATE in the first model would be 0.92 and the ATE in the second would be 3.28. As the papers suggest, changing that constant results in changes in the ATE, and so we need to be careful what constant we choose.\nTo see if units matters as the papers also propose, we will divide the outcome by 100 and compare to the original outcome while using the same transformation of \\(log(Y + 1)\\):\n\nt_data &lt;- mutate(t_data,\n                 Y_1_rescale=(Y/100)+1)\n\ny_mod1_rescale &lt;- lm(log(Y_1_rescale) ~ T,data=t_data)\n\nmodelsummary(models=list(`log(Y + 1) Original`=y_mod1, \n                         `log(Y + 1) Rescaled`=y_mod1_rescale))\n\n \n\n  \n    \n    \n    tinytable_l0d795q37l9gp233dj1o\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                log(Y + 1) Original\n                log(Y + 1) Rescaled\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.706   \n                  0.014   \n                \n                \n                             \n                  (0.029) \n                  (0.002) \n                \n                \n                  T          \n                  0.650   \n                  0.027   \n                \n                \n                             \n                  (0.041) \n                  (0.002) \n                \n                \n                  Num.Obs.   \n                  1000    \n                  1000    \n                \n                \n                  R2         \n                  0.200   \n                  0.125   \n                \n                \n                  R2 Adj.    \n                  0.200   \n                  0.124   \n                \n                \n                  AIC        \n                  4042.5  \n                  -3735.4 \n                \n                \n                  BIC        \n                  4057.2  \n                  -3720.7 \n                \n                \n                  Log.Lik.   \n                  -987.324\n                  1898.213\n                \n                \n                  F          \n                  250.073 \n                  142.951 \n                \n                \n                  RMSE       \n                  0.65    \n                  0.04    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAgain, we can see that rescaling \\(Y\\) and then applying the \\(log(Y+1)\\) transformation results in a remarkably different ATE. Because dividing by a constant doesn’t affect the proportion of zeroes but does affect the non-zero values, the ATE will consequently change when using this transformation."
  },
  {
    "objectID": "posts/logs/index.html#ordbetareg-to-the-rescue",
    "href": "posts/logs/index.html#ordbetareg-to-the-rescue",
    "title": "What’s Logs Got to Do With It?",
    "section": "ordbetareg to the Rescue",
    "text": "ordbetareg to the Rescue\nWhile the authors propose some solutions to this problem, such as using a two-part model, I want to show that the ordered beta regression can estimate scale-free ATEs if the outcome is normalized; that is, rescaled to between 0 and 1. Here we will let 0 be equal to 0 while 1 will be the sum of the outcome, i.e., the total income of the sample. We can rescale so that each observation in our data is equal to the proportion of the total. It corresponds to the following estimand, which I’ll call \\(ATE_n\\) for normalized:\n\\[\nATE_n = E[\\frac{Y1}{Y1+Y0} - \\frac{Y0}{Y1+Y0}]\n\\]\nCrucially, if we scale the outcome by any value \\(c\\), \\(c\\) will cancel as it will be in both the top and bottom of the fraction. We could estimate this quantity with either OLS or a t-test, but the error term is likely to be misleading as it won’t respect the bounds of the outcome (i.e. 0 or 1). This is where ordered beta regression can help. I’ll estimate two models, one with the original outcome and the second with the original outcome multiplied by a 100 to test for scaling effects. I will specify the true_bounds parameter to be 0 and 1 as by definition there will not be any values at the upper bound of 1.\n\nt_data &lt;- mutate(t_data,\n                 Y_n=Y/(sum(Y)),\n                 Y_n_rescale=(Y/100)/(sum(Y/100)))\n\ny_n_mod &lt;- ordbetareg(Y_n ~ T,data=t_data,\n                      chains=1,iter=1000,\n                      true_bounds=c(0,1),refresh=0,\n                      backend=\"cmdstanr\")\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 2.9 seconds.\n\ny_n_rescale &lt;- ordbetareg(Y_n_rescale ~ T,data=t_data,\n                          chains=1,iter=1000,\n                          true_bounds=c(0,1),refresh=0,\n                          backend=\"cmdstanr\")\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 3.0 seconds.\n\nmodelsummary(models=list(`Normalized Y Original`=y_n_mod, \n                         `Normalized Y Rescaled`=y_n_rescale),\n             statistic=\"conf.int\")\n\n \n\n  \n    \n    \n    tinytable_uo2jivdxd8o2jm8hepeo\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Normalized Y Original\n                Normalized Y Rescaled\n              \n        \n        \n        \n                \n                  b_Intercept\n                  -7.108            \n                  -7.114            \n                \n                \n                             \n                  [-7.196, -7.011]  \n                  [-7.210, -7.012]  \n                \n                \n                  b_T        \n                  0.608             \n                  0.612             \n                \n                \n                             \n                  [0.507, 0.707]    \n                  [0.515, 0.716]    \n                \n                \n                  phi        \n                  810.062           \n                  808.721           \n                \n                \n                             \n                  [732.567, 888.675]\n                  [714.157, 890.808]\n                \n                \n                  Num.Obs.   \n                  1000              \n                  1000              \n                \n                \n                  R2         \n                  0.090             \n                  0.091             \n                \n                \n                  ELPD       \n                  5335.8            \n                  5335.9            \n                \n                \n                  ELPD s.e.  \n                  74.6              \n                  74.7              \n                \n                \n                  LOOIC      \n                  -10671.5          \n                  -10671.8          \n                \n                \n                  LOOIC s.e. \n                  149.3             \n                  149.3             \n                \n                \n                  WAIC       \n                  -10671.6          \n                  -10671.9          \n                \n                \n                  RMSE       \n                  0.00              \n                  0.00              \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAs can be seen, both estimates are very close. We can convert these treatment effects to marginal effects using the marginaleffects package:\n\nmargin_Y &lt;- marginaleffects(y_n_mod, variables=\"T\")\nsummary(margin_Y)\n\n\n Term          Contrast Estimate    2.5 %  97.5 %\n    T mean(1) - mean(0) 0.000887 0.000624 0.00196\n\nColumns: term, contrast, estimate, conf.low, conf.high \n\n\nThe treatment effect here is miniscule as it represents the proportion of income in the sample accruing to an individual unit. However, we can transform this back to the original scale by simply multiplying the estimated marginal effect for each posterior draw by the sum of Y1 and Y0:\n\ntreatment &lt;- posteriordraws(margin_Y) %&gt;% \n  distinct(draw) %&gt;% \n  mutate(T=draw * sum(t_data$Y))\n\n# calculate a summary/point estimate\nsummary(treatment$T)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.585   2.190   2.542   2.812   3.052   9.119 \n\n# plot full distribution/uncertainty\nhist(treatment$T)\n\n\n\n\n\n\n\n\nDoing so gives us a treatment effect of approximately $2.7 dollars on the original scale–or, almost exactly \\(log 1\\) as per our original true ATE. Importantly, the estimated uncertainty of this ATE, which we can visualize by plotting all the posterior draws as a histogram, reflects the skew of the underlying variable, which in this case we would want to capture.\nAs the package designer Vincent Arel-Bundock pointed out, I can also do the code above using the marginaleffects package directly with the comparisons function:\n\ntreatment2 &lt;- avg_comparisons(y_n_mod,\n                          variables=\"T\",\n                          transform=function(x) x * sum(t_data$Y))\n\nprint(paste0(\"The treatment effect estimate is \",\n             round(treatment2$estimate,3),\n             \" with a lower bound of \",\n             round(treatment2$conf.low,3), \n             \" and an upper bound of \",\n             round(treatment2$conf.high,3),\".\"))\n\n[1] \"The treatment effect estimate is 2.543 with a lower bound of 1.788 and an upper bound of 5.631.\""
  },
  {
    "objectID": "posts/logs/index.html#conclusion",
    "href": "posts/logs/index.html#conclusion",
    "title": "What’s Logs Got to Do With It?",
    "section": "Conclusion",
    "text": "Conclusion\nFor these reasons, ordbetareg can be used to produce ATEs that are scale-free yet can be still be back-transformed to the original scale by rescaling the marginal effects as produced by the marginaleffects package. If the aim of the analysis is to estimate an ATE that is comparable across samples and income distributions, yet also doesn’t lose its relation to the original data, then ordbetareg can provide readily comparable estimates."
  },
  {
    "objectID": "posts/tunisia_polls/index.html",
    "href": "posts/tunisia_polls/index.html",
    "title": "Simulating Turnout in Tunisia’s Constitutional Referendum",
    "section": "",
    "text": "I am writing this post in response to questions about estimating turnout for Tunisia’s constitutional referendum today. Turnout is an important aspect to this referendum because high turnout would signal higher legitimacy for President Kais Saied’s dramatic changes to the Tunisia’s democracy. His proposed constitution would replace Tunisia’s democratic institutions with a new dictatorship that would concentrate power in the president’s hands.\nBecause of Saied’s interference in the election commission, we are much less sure about the accuracy of results from the official election commission, the ISIE. Saied has also banned foreign election observers from arriving, leaving only one local NGO, Mourakiboun, observing polling stations. Mourakiboun has done significant work in prior elections, but reportedly faces a shortfall of local participants because of low interest in the referendum.\nMourakiboun uses a method known as parallel vote tabulation (PVT) to estimate turnout. It is a method used by NGOs like NDI to provide an independent estimate of turnout. According to NDI, to estimate turnout through observers without bias, Mourakiboun has to select a random sample of polling stations and then accurately record all votes from these polling stations. I will use some code-based simulations to see how well this method will work depending on levels of turnout. If turnout is low, there is likely to be much more sampling uncertainty because there are fewer voters at polling stations to base estimates on. I will also examine what happens if Mourakiboun does not select polling stations at random, which is likely to happen for logistical reasons. For example, Mourakiboun may put more observers at stations with higher turnout in order to save volunteer hours.\nThe results of the simulation show that PVT/Mourakiboun turnout estimates are probably going to be less precise at low levels of turnout compared to high levels of turnout. In addition, when turnout is low and selection of polling stations is somewhat biased, such as selecting higher-turnout polling stations, the inaccuracies in the estimates get much worse. In other words, if Mourakiboun is using an imperfect methodology and turnout is low, their estimates will be farther off than if turnout in the referendum was high. If 50% of Tunisians voted, this wouldn’t be as much of a problem, but if 10% vote, it will be much harder to get an accurate estimate using PVT via polling station counts.\nWithout digging into the methods below, the results can be summarized as:\nIf turnout is large, these biases have a much smaller role, but when turnout is small, as is likely to be the case with the constitutional referendum, the biases can have a much bigger impact on the final estimates, making Mourakiboun’s methods more likely to fail."
  },
  {
    "objectID": "posts/tunisia_polls/index.html#simulation",
    "href": "posts/tunisia_polls/index.html#simulation",
    "title": "Simulating Turnout in Tunisia’s Constitutional Referendum",
    "section": "Simulation",
    "text": "Simulation\nFirst, we randomly create polling stations roughly corresponding to Tunisia’s population of 7 million registered voters and 4,500 polling stations:\n\nN &lt;- 8900000\n\nstations &lt;- 11000\n\nvote_assign &lt;- sample(1:stations,N,replace=T,\n                      prob=sample(1:3,stations,replace=T))\n\nsample_size &lt;- 1000\n\nWhat we will do is test how accurate samples will be depending on total turnout and the quality of samples. To do so, I’ll first vary true turnout from 5% up to 50% of the population. We’ll assume as well that turnout varies by polling station, which is much more accurate than assuming a constant rate of turnout for all polling stations.\nWe’ll assume that the stations can vary in size up to a 3-fold magnitude, i.e., some stations may be 3 times smaller than other stations. For our sample, we have a polling station as large as 1329 and one as small as 331. We’ll then assume, for now, that Mourakiboun selects a random sample of 50 polling stations and it records all votes at these stations. We’ll repeat this experiment 1,000 times and record what estimated turnout Mourakiboun would report and how that compares to real turnout:\n\n# loop over turnout, sample polls, estimate turnout\n\nover_turnout &lt;- parallel::mclapply(seq(.05,.5,by=.1), function(t) {\n  \n    # polling station varying turnout rates\n  \n    station_rates &lt;- rbeta(n=N,t*20,(1-t)*20)\n          \n    # randomly let voters decide to vote depending on true station-level turnout rate\n    \n    pop_turnout &lt;- lapply(1:stations, function(s) {\n                    tibble(turnout=rbinom(n=sum(vote_assign==s), size=1,prob = \n                                            station_rates[s]),\n                           station=s)\n                    }) %&gt;% bind_rows\n  \n    over_samples &lt;- lapply(1:1000, function(i) {\n      \n        # sample 100 random polling stations 1,000 times\n        sample_station &lt;- sample(1:stations, size=sample_size)\n        \n        turn_est &lt;- mean(pop_turnout$turnout[pop_turnout$station %in% sample_station])\n        \n        return(tibble(mean_est=turn_est,\n                      experiment=i))\n      \n    }) %&gt;% bind_rows %&gt;% \n      mutate(Turnout=t)\n    \n    over_samples\n  \n    },mc.cores=10) %&gt;% \n  bind_rows\n\nWe can now plot estimated versus actual turnout. If we have random samples and no problems with recording votes, this works fairly well. The dot shows the average of all samples, which is un-biased, and the vertical line shows where 95% of the samples fall, which is an estimate of potential sampling error. In other words, if Mourakiboun does everything right with a sample of 50 polling stations, they would expect this kind of error from random chance alone. If true turnout is 15%, they could see estimates that range from 12% to 17%.\n\nover_turnout %&gt;% \n  group_by(Turnout) %&gt;% \n  summarize(pop_est=mean(mean_est),\n            low_est=quantile(mean_est,.05),\n            high_est=quantile(mean_est, .95)) %&gt;% \n  ggplot(aes(y=pop_est,x=Turnout)) +\n  geom_pointrange(aes(ymin=low_est,\n                      ymax=high_est),size=.5,fatten=1) +\n  geom_abline(slope=1,intercept=0,linetype=2,colour=\"red\") +\n  theme_tufte() +\n  theme(text=element_text(family=\"\")) +\n  labs(y=\"Estimated Turnout\",x=\"True Turnout\",\n       caption=stringr::str_wrap(\"Comparison of Mourakiboun estimated (y axis) versus actual turnout (x axis). Red line shows where true and estimated values are equal. Based on random samples of 50 polling stations and assuming no problems with recording votes.\"))\n\n\n\n\n\n\n\n\nUnfortunately, this kind of accuracy only happens in a computer simulation. It is quite tricky to do a true random sample of polling stations because there may not be volunteers to cover all of the polling stations, as is the case with the current referendum. Let’s assume in the following simulation then that a polling station is more likely to be sampled if the true level of turnout is higher:\n\n# loop over turnout, sample polls, estimate turnout\n\nover_turnout_biased &lt;- parallel::mclapply(seq(.05,.5,by=.1), function(t) {\n  \n    # polling station varying turnout rates\n  \n    station_rates &lt;- rbeta(n=stations,t*20,(1-t)*20)\n          \n    # randomly let voters decide to vote depending on true station-level turnout rate\n    \n    pop_turnout &lt;- lapply(1:stations, function(s) {\n                    tibble(turnout=rbinom(n=sum(vote_assign==s), size=1,prob = \n                                            station_rates[s]),\n                           station=s)\n                    }) %&gt;% bind_rows\n  \n    over_samples &lt;- lapply(1:1000, function(i) {\n      \n        # sample 100 random polling stations 1,000 times\n        sample_station &lt;- sample(1:stations, size=50,prob=station_rates)\n        \n        turn_est &lt;- mean(pop_turnout$turnout[pop_turnout$station %in% sample_station])\n        \n        return(tibble(mean_est=turn_est,\n                      experiment=i))\n      \n    }) %&gt;% bind_rows %&gt;% \n      mutate(Turnout=t)\n    \n    over_samples\n  \n    },mc.cores=10) %&gt;% \n  bind_rows\n\nWe can now plot estimated versus actual turnout for this simulation where the polling stations were more likely to be sampled if they had higher turnout:\n\nover_turnout_biased %&gt;% \n  group_by(Turnout) %&gt;% \n  summarize(pop_est=mean(mean_est),\n            low_est=quantile(mean_est,.05),\n            high_est=quantile(mean_est, .95)) %&gt;% \n  ggplot(aes(y=pop_est,x=Turnout)) +\n  geom_pointrange(aes(ymin=low_est,\n                      ymax=high_est),size=.5,fatten=1) +\n  geom_abline(slope=1,intercept=0,linetype=2,colour=\"red\") +\n  theme_tufte() +\n  theme(text=element_text(family=\"\")) +\n  labs(y=\"Estimated Turnout\",x=\"True Turnout\",\n       caption=stringr::str_wrap(\"Comparison of Mourakiboun estimated (y axis) versus actual turnout (x axis). Red line shows where true and estimated values are equal. Based on biased samples of 50 polling stations with higher turnout stations more likely to be sampled. However, simulation assumes no problems with recording votes.\")) +\n  ylim(c(0,0.5)) +\n  xlim(c(0,0.5))\n\n\n\n\n\n\n\n\nWe see in the plot above that the black dot estimates are always higher than the red line showing the true values. Furthermore, the bias appears worse when turnout is smaller. The bias is also substantial - at low levels of turnout, such as at 5%, the estimated turnout can be twice as high.\nAs can be seen in this simulation study, it is quite possible for the method to work, but only if the polling stations are selected at random. I did not look into what would happen with other possible errors, such as being unable to accurately record all votes from a given polling station. For these reasons, while this method can certainly work, it is necessary to confirm what methodology was used to select the polling stations and also how strictly it was followed in implementation. There are many points at which this type of analysis could either intentionally or unintentionally result in over/under reporting of turnout.\nIf turnout is large, some of these biases have a minimal effect, but when turnout is small, as is likely to be the case with the constitutional referendum, they can have a much bigger impact on the final estimates."
  },
  {
    "objectID": "post/limited_dvs/index.html",
    "href": "post/limited_dvs/index.html",
    "title": "What To Do (And Not to Do) with Modeling Proportions/Fractional Outcomes",
    "section": "",
    "text": "Interested in more social science on contemporary issues? Check out my just-released book with Cambridge University Press and use discount code KUBINEC23 to get 20% off."
  },
  {
    "objectID": "post/limited_dvs/index.html#ordinary-least-squares-the-normal-distribution",
    "href": "post/limited_dvs/index.html#ordinary-least-squares-the-normal-distribution",
    "title": "What To Do (And Not to Do) with Modeling Proportions/Fractional Outcomes",
    "section": "Ordinary Least Squares (The Normal Distribution)",
    "text": "Ordinary Least Squares (The Normal Distribution)\nThe most common modeling technique for limited continuous variables is ordinary least squares (OLS) regression, in no small part because OLS is the default model for any kind of continuous outcome (and even some that are discrete). OLS does have remarkable properties, such as being the most efficient estimator for an unbounded continuous variable. The reason for this has to do with the unique properties of the Normal distribution. If all we know about a distribution is that it is continuous, then OLS is hard to beat.\nHowever, in this case we know more about the DV than that it is continuous. We know that the variable can have both an upper and lower bound. The Normal distribution, upon which OLS is based, does not have any bounds. In principle, any value on the real number line can come out of a Normal distribution–though with quite low probability.\nTo demonstrate this, I use R’s rnorm command to simulate data from a Normal distribution with a mean of 0.5 and a standard deviation (SD) of 0.05. I first draw about 1,000 random numbers from this distribution and plot them as a histogram:\n\nsim_num &lt;- rnorm(1000,0.5,0.1)\nhist(sim_num)\n\n\n\n\n\n\n\n\nThis plot shows that the distribution is centered around 0.5 with a maximum of 0.8349045 and a minimum of 0.129012. If this DV was bounded between 0 and 1, we would presumably have little to worry about observations exceeding the bounds. However, let’s do the same analysis, but this time sample 10,000,000 observations:\n\nsim_num_big &lt;- rnorm(10000000,0.5,0.1)\nhist(sim_num_big)\n\n\n\n\n\n\n\n\nAs can be seen, the bounds of the plot are now much wider. The max value from the draws is 1.0437146 and the min value is -0.0219126. What happened is that by getting way more draws, we allow for very low-probability events to occur. Even with a Normal distribution tightly centered around 0.5, there is always some chance that an observation will arise outside of the 0 to 1 bounds. The Normal distribution can’t handle bounds–it essentially ignores them because technically, any observation can happen with some probability, even if that probability is very small.\n\nBeta Regression\nWhile OLS cannot handle bounds, there is another distribution that is continuous and bounded above or below–the Beta distribution. The Beta distribution is related to the binomial distribution, and it can be thought of as a prior distribution for the probability of a successful trial in a binomial/Bernoulli distribution. The Beta distribution has two parameters, usually denoted \\(\\alpha\\) and \\(\\beta\\), which represent the number of prior successes and failures. For example, suppose we want to come up with our best guess of how Donald Trump might do in the 2024 presidential election given his performance in the 2020 election in terms of the number of electoral college votes he received (which are roughly proportional to the popular vote for president in the United States). In that case, we could use the Beta distribution to get a sense of our uncertainty in that statement by plugging in 232 for the number of electors Trump won in 2020 for \\(\\alpha\\) and 306 for the number of electors that Trump didn’t win for \\(\\beta\\)–i.e., all the electors Joe Biden won in 2020:\n\nelectors &lt;- rbeta(1000, 232, 306)\n\nhist(electors)\n\n\n\n\n\n\n\n\nWhat the plot above shows is the range of uncertainty around the proportion of electors that Trump could win in 2024 if we use his 2020 performance as our prior knowledge. We would expect, based on chance alone, for his 2024 total to vary between 40% of electors to 48% of electors. In other words, he would be likely to still lose, but he could get a lot closer to a majority of electoral college votes.\nIn a sense, the Beta distribution is a distribution of probabilities. The probability represents the chance of success in an independent trial, in this case winning an elector. If we increase the sample size, say by including how many electoral votes Trump won in 2016 (304 out of 531), our uncertainty would decrease:\n\nelectors &lt;- rbeta(1000, 304 + 232, 227 + 306)\n\nhist(electors)\n\n\n\n\n\n\n\n\nThe plausible range of percentages/probabilities is now within 48% to 52% as taking into account Trump’s 2016 results increases our sample size while also increasing our most likely estimate of his 2024 performance. Importantly, the Beta distribution respects bounds. Suppose, for example, that we only expect Trump to win 1 out of 538 electors. That would concentrate a substantial amount of plausible values at the lower end of the range of the variable:\n\nelectors &lt;- rbeta(1000, 1, 537)\n\nhist(electors)\n\n\n\n\n\n\n\n\nWhat we can see here is a unique feature of the Beta distribution: it respects upper and lower bounds. Here the Beta distribution says it is most likely Trump wins a very small number of electors – below 1% of them – but it is plausible that he still wins more than 1%. It is impossible, as it should be, for Trump to win less than 0% of electors. OLS, on the other hand, would allow for Trump to win negative electoral votes, which is nonsensical.\nThe Beta distribution also has a role in Bayesian inference by representing our prior belief in a probability or proportion. Suppose that is the night of the 2024 election, and we want to know how many additional electors Trump could win given how many we’ve observed him win so far. Of course, this is somewhat unrealistic, as the electors are clustered within states, but we’ll ignore that for the example. Suppose it is quite early, and so far we have seen Trump win 3 electors and lose 7. That leaves 528 electors “on the table.” We can calculate that using what is called the Beta-binomial distribution, which is where we multiply our Beta prior distribution with a binomial distribution for the remaining number of electors.\n\ntrump_post &lt;- extraDistr::rbbinom(n=1000,size=528,\n                                  alpha=3,beta=7)\n\nhist(trump_post)\n\n\n\n\n\n\n\n\nWe can see that our prior is quite vague–we’ve only observed 10 electors out of a possible 538. As a result, the Beta-binomial distribution is telling us that there could be a wide range of expected results for Trump from the remaining 528, though on the whole we expect him to lose.\nThis extended discussion was designed to show that in many ways, the Beta distribution is an intuitive way of representing a proportion, or really, a probability.\nHowever, there is a really big problem with the Beta distribution. Probabilities aren’t every equal to zero or one–that would be a situation of complete uncertainty, which would have no probability involved. Similarly, the Beta distribution can never produce values that are equal to 0 or 1.\nFor example, let’s say we think Trump could win .01 elector and lose 537.99 of them (hypothetically, of course). We would then get the following values for the Beta distribution:\n\nbounds &lt;- rbeta(10000,.01,537.99)\n\nmin(bounds)\n\n[1] 5.562685e-311\n\nmax(bounds)\n\n[1] 0.006495601\n\n\nWe see that our lower bound is tiny – a number that is very close to, but not exactly, zero. Just like the Normal distribution, the Beta distribution is continuous, so it can’t ever be equal to a single point. The Beta distribution can let probabilities get very small, but never actually zero.\nThis is a problem because what if we wanted to use it to model the proportion of people who support Trump in the population? We know that could actually be zero or one. This is the main limitation of what is known as Beta regression, where the Beta distribution is employed just like OLS–we have a model with covariates that has an effect defined over the range of the Beta distribution, i.e. strictly within upper and lower bounds.\n\n\nFractional Logit\nAnother way of dealing with the same problem is to use what is known as the fractional logit model. Popularized by the econometricians Papke and Wooldridge, the fractional logit model is known as a “quasi-likelihood” because it isn’t actually a statistical distribution. For that reason, it is a bit confusing to describe exactly what it is. In essence, we start with the Bernoulli distribution, which I will reproduce below:\n\\[\nP(X = 1)= p^x(1-p)^{1-x}\n\\] This distribution is quite simple. It says that the probability that X equals one is equal to the parameter \\(p\\) for probability. The variable X can only take a value of 0 or 1. If X is equal to 1, we get \\(p^x\\), and if X is equal to 0, we get \\((1-p)^{1-x}\\).\nThe fractional logit model inserts a continuous X instead of a binary one. For example, if X=0.6, and \\(p=0.4\\), then we get\n\\[\nP(X=0.6) = 0.4^{0.6}(0.6^{0.4}) =  0.47\n\\]\nThis formula essentially tries to force the Bernoulli distribution to handle continuous variables by simply plugging them in. The problem is, what we end up with is not actually a statistical distribution. This means I can’t generate data using a function like rnorm, and there also isn’t a PDF or a CDF. In other words, it’s a bit of a hack.1\nIt does “work” in the sense the probability of X increases as \\(p\\) is closest to X. For example, you can look at the probability of various values of \\(p\\) if X is equal to 0.3:\n\nX &lt;- 0.3\np &lt;- seq(0,1,by=0.001)\nfrac_logit &lt;- function(x,p) (p^x)*((1-p)^(1-x))\nplot(p,frac_logit(x=X,p=p))\n\n\n\n\n\n\n\n\nWe can see that the parameter \\(p\\) hits a maximum at 0.3, or the same value as X. This is a neat trick, but because we can’t sample from this distribution, it is tough to characterize how the distribution performs. While this model is quite practical, it was published long before people started using Beta regression, which I think is superior as it is an actual distribution. As I discuss later, in most situations, Beta regression is preferable–and if for some reason you can’t or won’t use Beta regression, probably best just to stick with OLS, which has known properties."
  },
  {
    "objectID": "post/limited_dvs/index.html#solutions-to-the-beta-distribution-problem",
    "href": "post/limited_dvs/index.html#solutions-to-the-beta-distribution-problem",
    "title": "What To Do (And Not to Do) with Modeling Proportions/Fractional Outcomes",
    "section": "Solutions to the Beta Distribution Problem",
    "text": "Solutions to the Beta Distribution Problem\nHaving now presented the various estimators, I’ll focus on fixes to the Beta distribution. As can be seen, the Beta distribution has some very desirable properties–it is bounded between 0 and 1, it is continuous and it can take on a variety of shapes. The only problem is the issue with the bounds–what if we have a variable that actually equals 0 or 1?\nOne solution proposed in the literature is to do a bit of “nudging.” This formula, first proposed by Smithson and Verkuilen (2006), is as follows:\n\\[\ny_j = \\frac{y_i(N-1) + 0.5}{N}\n\\]\nI do not recommend this formula. While it seems straightforward, it depends on \\(N\\), or sample size. This means that the amount of nudging that happens changes as the sample size changes. We can illustrate this issue by transforming a dataset of equally spaced values from 0 to 1 while we vary \\(N\\) in the transformation formula:\n\nN &lt;- seq(100,10000,by=10)\n\nmin_y_j &lt;- sapply(N, function(n) {\n  \n  y_i &lt;- seq(0,1,length.out=n)\n  y_j &lt;- (y_i*(n-1) + 0.5)/n\n  min(y_j)\n  \n})\n\nmax_y_j &lt;- sapply(N, function(n) {\n  \n  y_i &lt;- seq(0,1,length.out=n)\n  y_j &lt;- (y_i*(n-1) + 0.5)/n\n  max(y_j)\n  \n})\n\nplot(N,min_y_j)\n\n\n\n\n\n\n\nplot(N,max_y_j)\n\n\n\n\n\n\n\n\nAs can be seen, the value of the highest or lowest value in the distribution gets nudged closer and closer to 0 or 1 as \\(N\\) gets larger. This means that we get increasingly implausible outliers as \\(N\\) grows in a function that is highly non-linear. This will strongly effect estimates, as I show in my paper’s simulation. My main reason for bringing up this fix is to discourage its use given these dangerous side effects.\nHowever, there has been substantial developments in the past 10 years that provide much more satisfactory ways of helping the Beta distribution deal with bounds. The most well-known recent methodology to fix the Beta distribution’s issues are known as the zero-or-one and zero-and-one beta regression models (also called the ZOIB). These combo models add distinct sub-models for the discrete responses at 0 or 1. The zero-or-one can model either 0s or 1s plus a continuous Beta regression, but not both 0s and 1s at the same time, while the zero-and-one can model 0s, 1s, and everything in between with a Beta regression. Because it is a general solution, I’ll look at the zero-and-one formulation before turning to my model. I developed ordered beta regression specifically to deal with limitations in the ZOIB.\nWe can represent the ZOIB mathematically in terms of three distinct probabilities for a bounded response \\(y_i\\): \\(Pr(y_i=0)\\) for the responses that equal 0, \\(Pr(y_i=1)\\) for the responses that equal 1, and \\(Pr(y_i&gt;0 \\cap y_i&lt;1)\\) for the responses that are continuous (strictly between 0 and 1). In the formula below, I use \\(\\alpha\\) for \\(Pr(y_i=0)\\), \\(\\gamma\\) for \\(Pr(y_i=1)\\), and \\(\\delta\\) for \\(Pr(y_i&gt;0 \\cap y_i&lt;1)\\). The parameters \\(\\mu\\) and \\(\\phi\\) represent the mean and dispersion of the Beta regression that can be used for the continuous responses. As can be seen, the ZOIB can handle any type of response:\n\\[\nf(y_i|\\alpha,\\gamma,\\delta,\\mu,\\phi) = \\left\\{\\begin{array}{lr}\n\\alpha & \\text{if } y_i=0\\\\\n(1-\\alpha)\\gamma & \\text{if } y_i=1\\\\\n(1-\\alpha)(1-\\gamma)\\text{Beta}(\\mu,\\phi) & \\text{if } y_i \\in (0,1)\\\\\n\\end{array}\\right\\}\n\\]\nHowever, the issue with this model is that each sub-model is independent. The probability of a 0 can change independently of any other probability, and the value of a 1 can change independently of probability of the Beta regression model. This might seem fairly abstract, but it is important because each model sub-component has to have its own linear model. For example, suppose we have one predictor in our model, say GDP. We would end up with 3 separate regression coefficients of the effect of GDP on \\(y_i\\), one for 0s, one for 1s, and one for all the values between 0 and 1.\nNow, there may be situations where having that many coefficients is warranted. As I discuss in my paper, that could happen when you think that 0s and 1s are distinct processes from the continuous values. Suppose that 0 = no drug use, and then 0 - 1 is equal to at least some drug use. In that sense, we could use the model above to separately predict the decision to choose drugs from the decision of how many drugs to consume if someone becomes a drug user. For that reason, the ZOIBs are actually related to selection and hurdle models where someone selects into and out of the continuous responses.\nTo be clear, the ZOIB does solve the Beta regression problem–it can estimate a model with 0s and 1s and anything in between. It just does so with a lot of extra parameters, requiring the user to have 3 different linear models. If the scale is one construct, though, that can be overkill, such as with the electors example we started with. I developed the ordered beta regression model specifically for an experiment I ran with an outcome that represented the fraction invested in a given company. The difference between 0% invested and 1% invested was one of degree, not kind. Coefficients for the effect of covariates on investment that differed between 0% and 1% didn’t make sense.\nFor that reason, I derived the ordered beta regression model as a simpler alternative that still relies on Beta regression’s strong points. The formula for the ordered beta regression model is below, defined in terms of the three probabilities, \\(\\alpha\\) (0s), \\(\\delta\\) (1s), and \\(\\gamma\\) (continuous responses). The main difference is that I introduce cutpoints, \\(k_1\\) and \\(k_2\\), parameters drawn from the more widely known ordinal logit model. These cutpoints permit a single linear model, \\(X'\\beta\\), to predict all three parts of the outcome: 0s, 1s and anything in between:\n\\[\n\\left\\{\\begin{array}{lr}\n\\alpha  = 1 - g(X'\\beta - k_1)\\\\\n\\delta  = \\left[g(X'\\beta - k_1) - g(X'\\beta - k_2) \\right ] \\text{Beta}(g(X'\\beta),\\phi)\\\\\n\\gamma = g(X'\\beta - k_2)\\\\\n\\end{array}\\right\\}\n\\]\nThe intuition behind this model is the same as the ordinal logit model. Imagine we have a single scale, \\(y_i^*\\). This scale is continuous and unbounded, but we only observe \\(y_i\\), which is bounded between 0 and 1. The cutpoints determine where we observe values at the boundaries instead of the fully continuous \\(y_i^*\\). In other words, the cutpoints alert us to how different the bounds are from the other continuous values. If the cutpoint values are high in absolute terms, the amount of censoring is likewise high. If the cutpoints are equal to zero, then there is no censoring and we essentially end up with Beta regression without any boundary issues.\nSimple, easy, breezy. Because this model only has one set of coefficients for a given covariate, it is a drop-in replacement for OLS. It only has two additional parameters, which are the cutpoints.\nThe main difference between the ZOIB and the ordered beta regression model is how they treat the discrete values at the bounds. The ZOIB is premised on these values coming from a distinct sub-process, like a selection model. The ordered beta regression model allows for qualitative differences between the bounds and the continuous responses. The amount of difference is a function of the data and will vary from sample to sample."
  },
  {
    "objectID": "post/limited_dvs/index.html#modeling-exercise",
    "href": "post/limited_dvs/index.html#modeling-exercise",
    "title": "What To Do (And Not to Do) with Modeling Proportions/Fractional Outcomes",
    "section": "Modeling Exercise",
    "text": "Modeling Exercise\nNow that I have defined the models statistically, I will turn to an applied regression problem to illustrate their usage and practical significance. To do so I use data from the Varieties of Democracy (V-DEM) project, which records the proportion of the parliamentary seats in the lower chamber of a country that are directly elected, as opposed to being appointed. This value could be 0 (all seats are appointed) to 100, indicating that all seats are elected using the popular vote. I use the V-DEM Github R package to get their data, and subset it to the most recent year (2021):\n\ndata(vdem)\nvdem &lt;- vdem %&gt;% \n  filter(year==2021) %&gt;% \n  select(appoint_chamber=\"v2lgello\",\n         power_groups=\"v2pepwrsoc\",country_name)\n\nFor simplicity, we will use a single covariate, to what extent “power is distributed by social groups”, which could indicate that some seats are appointed or reserved to respect the rights of particular groups in a society. Our model looks at associations between the power of social groups and the share of appointed vs. elected seats in 2021 by comparing countries to each other. We can first look at the distribution of the share of elected seats in parliaments:\n\nhist(vdem$appoint_chamber)\n\n\n\n\n\n\n\n\nThe plot shows that most scores are equal to 100, indicating all seats are elected via popular vote. There are some legislatures, though, where everyone is appointed, and some where there are some who are appointed and elected. These countries include the following:\n\nfilter(vdem, appoint_chamber&gt;0, appoint_chamber&lt;100) %&gt;% \n  pull(country_name)\n\n [1] \"Egypt\"                \"Colombia\"             \"Bangladesh\"          \n [4] \"India\"                \"Kenya\"                \"Tanzania\"            \n [7] \"Uganda\"               \"Zambia\"               \"Zimbabwe\"            \n[10] \"Botswana\"             \"Burundi\"              \"Iran\"                \n[13] \"Iraq\"                 \"Qatar\"                \"Sierra Leone\"        \n[16] \"The Gambia\"           \"Kazakhstan\"           \"Rwanda\"              \n[19] \"Eswatini\"             \"Comoros\"              \"Guyana\"              \n[22] \"Hong Kong\"            \"Kuwait\"               \"Mauritius\"           \n[25] \"Singapore\"            \"United Arab Emirates\" \"Zanzibar\"            \n\n\nAs can be seen, there are 26 countries that have these mixed legislatures. Some of these countries, such as India and Kenya, are known to have distinct ethnic minorities that might want to have protected representation.\nThe covariate that might predict this variation, the power of social groups, is a continuous measure with significant dispersion:\n\nhist(vdem$power_groups)\n\n\n\n\n\n\n\n\nTo test this question, I will fit models for each of the types of statistical distributions I have presented, including OLS, fractional logit, re-scaled Beta regression (normalizing to between 0 and 1), ZOIB, and ordered beta regression. I will first rescale the outcome to be between 0 and 1 instead of 0 and 100, as V-DEM coded it. This is necessary for some of the R packages, but does not affect the substantive results. In fact, any bounded variable can be rescaled to 0 and 1 using a simple formula.\n\nvdem$appoint_chamber &lt;- vdem$appoint_chamber/100\n\n\nOLS\nFirst we can run OLS using the base R command lm. I include a quadratic effect to see if powerful social groups are associated with some, but not all, elected seats:\n\nlm_est &lt;- lm(appoint_chamber ~ power_groups + I(power_groups^2),data=vdem)\nsummary(lm_est)\n\n\nCall:\nlm(formula = appoint_chamber ~ power_groups + I(power_groups^2), \n    data = vdem)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.98167  0.01211  0.02436  0.04901  0.38783 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.912202   0.020567  44.353  &lt; 2e-16 ***\npower_groups       0.077946   0.014646   5.322 3.23e-07 ***\nI(power_groups^2) -0.019882   0.008681  -2.290   0.0232 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1879 on 169 degrees of freedom\n  (7 observations deleted due to missingness)\nMultiple R-squared:  0.1437,    Adjusted R-squared:  0.1336 \nF-statistic: 14.18 on 2 and 169 DF,  p-value: 2.023e-06\n\n\nOur intuitions could be partly confirmed: as social groups become more powerful, seats are more likely to be elected, but only up to a point. Above that point, they in fact become more likely to be appointed. To figure out what the relationship looks like, we can use lm’s predict function for varying values of power_groups, and plot the result:\n\nex_data &lt;- seq(min(vdem$power_groups),\n               max(vdem$power_groups),length.out=100)\n\nlm_pred &lt;- predict(lm_est, se.fit=T,\n                             newdata=tibble(power_groups=ex_data))\n\nggplot(mapping=aes(y=lm_pred$fit,x=ex_data)) +\n  geom_ribbon(aes(ymin=lm_pred$fit + 1.96*lm_pred$se.fit,\n                  ymax=lm_pred$fit - 1.96*lm_pred$se.fit),fill=\"blue\",\n              alpha=0.5) +\n  geom_hline(yintercept=1,linetype=2) +\n  geom_line() +\n    scale_y_continuous(labels=scales::percent_format()) +\n  labs(y=\"% Seats Elected\",\n       x=\"Power of Social Groups\") +\n  theme_tufte2\n\n\n\n\nOLS Predicted Values of Elected Lower Chamber Seats Given Power of Social Groups\n\n\n\n\nThe plot shows that when the power of social groups is equal to +2, most seats are elected, but when the power of social groups is equal to +3, fewer than all seats are elected. So in other words, when social groups are the most powerful, at least some seats are appointed. When social groups have little if any power, all seats are likely to be appointed.\nOne issue is that this prediction is close to the maximum of the scale: 0.9885877. While OLS doesn’t predict out of bounds in this example, the confidence interval of the prediction is likely to exceed zero. We can test that by examining the maximum of the confidence interval:\n\nmax(lm_pred$fit + 2*lm_pred$se.fit)\n\n[1] 1.095932\n\n\nWe can see that high end of the high end 5% to 95% confidence interval reaches 1.09. In other words, OLS predicts that when social groups are moderately powerful, 109% of seats will be elected. This possibility, of course, is ridiculous, and makes no sense given our scale. Again, OLS can’t respect bounds, which means that we can get intervals that can’t be mapped on to the underlying scale in a meaningful sense.\n\n\nFractional Logit\nTo respect the bounds, we’ll first use fractional logit, the simplest method although, as I mentioned earlier, a bit of a hack. We can estimate this model with a base R command glm and the quasibinomial family argument:\n\nfrac_logit_est &lt;- glm(appoint_chamber ~ power_groups + I(power_groups^2),data=vdem,\n                  family = quasibinomial)\nsummary(frac_logit_est)\n\n\nCall:\nglm(formula = appoint_chamber ~ power_groups + I(power_groups^2), \n    family = quasibinomial, data = vdem)\n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       2.452975   0.350523   6.998 5.82e-11 ***\npower_groups      0.904496   0.215441   4.198 4.34e-05 ***\nI(power_groups^2) 0.004464   0.166477   0.027    0.979    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasibinomial family taken to be 0.5591972)\n\n    Null deviance: 60.415  on 171  degrees of freedom\nResidual deviance: 48.359  on 169  degrees of freedom\n  (7 observations deleted due to missingness)\nAIC: NA\n\nNumber of Fisher Scoring iterations: 7\n\n\nInterestingly, we do not see the same relationship. Fractional logit does not find a very strong quadratic effect, and instead only a linear effect of the power of social groups on the share of elected seats. We can replicate our prediction to see what the estimates look like:\n\npred_power_groups_frac &lt;- predict(frac_logit_est,type=\"response\",se.fit = T,\n                                  newdata=tibble(power_groups=ex_data))\n\nggplot(mapping=aes(y=pred_power_groups_frac$fit,x=ex_data)) +\n  geom_ribbon(aes(ymin=pred_power_groups_frac$fit + 1.96*pred_power_groups_frac$se.fit,\n                  ymax=pred_power_groups_frac$fit - 1.96*pred_power_groups_frac$se.fit),fill=\"blue\",\n              alpha=0.5) +\n  geom_hline(yintercept=1,linetype=2) +\n  geom_line() +\n    scale_y_continuous(labels=scales::percent_format()) +\n  labs(y=\"% Seats Elected\",\n       x=\"Power of Social Groups\") +\n  theme_tufte2\n\n\n\n\nFractional Logit Predicted Values of Elected Lower Chamber Seats Given Power of Social Groups\n\n\n\n\nAs can be seen, this model is not identical to OLS. However, it does have the property that it will stay within the bounds, as can be seen with the curvature to the prediction. As the prediction gets closer to the boundary, it shrinks in size. This is a feature of the logit link function used in the model. We can also check for the maximum value of the confidence interval:\n\nmax(pred_power_groups_frac$fit + 2*pred_power_groups_frac$se.fit)\n\n[1] 1.012364\n\n\nThe max confidence interval isn’t exactly one, but very close. The small increase above 1 is due to the limits of the Normal approximation used to calculate the confidence interval. Compared to OLS, the fractional logit does respect the boundaries. However, compared to the beta regression variants that we turn to next, it is not as powerful a tool. There is no easy way, for example, to know why the estimated coefficients are so different than OLS because we cannot simulate data from the fractional logit “quasi-likelihood” as we can with the other distributions.\n\n\n(Vanilla) Beta Regression\nNext I turn to the Beta regression, which employs the same distribution shown earlier but with a linear model to predict the average value of the outcome. There is some work that needs to be done to use the Beta distribution as a regression model, but I’ll refer the reader to other sources for that info. We can use the R package gam to do the modeling using maximum likelihood with a family from the mgcv package.\nUnfortunately, we can’t fit the model without adjusting the data as we have observations at the bounds, i.e. we have legislatures with either 0% elected seats or 100% elected seats. We’ll have to use the transformation mentioned earlier to model the data. We could also just exclude observations at the bounds, but of course that would also introduce bias unless the observations at the bounds were randomly assigned to those values (highly unlikely).\n\nvdem_trans &lt;- mutate(vdem, appoint_chamber = (appoint_chamber*(n()-1) + 0.5)/n())\n\nfit_beta_reg &lt;- gam(appoint_chamber ~ power_groups + I(power_groups^2),data=vdem_trans,\n                    family=betar(link=\"logit\"))\n\nsummary(fit_beta_reg)\n\n\nFamily: Beta regression(1.414) \nLink function: logit \n\nFormula:\nappoint_chamber ~ power_groups + I(power_groups^2)\n\nParametric coefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)        1.65637    0.12239  13.534  &lt; 2e-16 ***\npower_groups       0.26403    0.08988   2.937  0.00331 ** \nI(power_groups^2) -0.06480    0.05225  -1.240  0.21486    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nR-sq.(adj) =  0.102   Deviance explained = 53.6%\n-REML = -423.08  Scale est. = 1         n = 172\n\n\nInterestingly, we do now see some similarities to OLS in terms of a marginally declining effect of power_groups at very high levels of power_groups. We can also look at predicted values, which of course will be in the transformed (nudged) outcome scale:\n\npred_beta_reg &lt;- predict(fit_beta_reg,type=\"response\",se.fit=T,\n                                  newdata=tibble(power_groups=ex_data))\n\nggplot(mapping=aes(y=pred_beta_reg$fit,x=ex_data)) +\n  geom_ribbon(aes(ymin=pred_beta_reg$fit + 1.96*pred_beta_reg$se.fit,\n                  ymax=pred_beta_reg$fit - 1.96*pred_beta_reg$se.fit),fill=\"blue\",\n              alpha=0.5) +\n  geom_hline(yintercept=1,linetype=2) +\n  geom_line() +\n    scale_y_continuous(labels=scales::percent_format()) +\n  labs(y=\"% Seats Elected\",\n       x=\"Power of Social Groups\") +\n  theme_tufte2\n\n\n\n\nBeta Regression Predicted Values of Elected Lower Chamber Seats Given Power of Social Groups\n\n\n\n\nWhat should be clear is that the nudging does have an effect–the outcome doesn’t come all that close to 100%. Because \\(N\\) is small, the nudging will be farther from the boundary. Again, the transformation formula is not benign, and will influence estimates. To “fix” this problem, I will turn next to models that make use of the beta regression’s features without ad hoc solutions to the boundaries.\n\n\nZOIB\nFollowing our earlier presentation, I will now estimate the ZOIB model. While there are a couple of R packages available, I will use one of the best, which is known as brms. brms is a powerful Bayesian regression modeling package that can fit a variety of models. The ZOIB variants are complicated enough that Bayesian inference is necessary to fit them properly.\nThe syntax for the model is relatively straightforward. However, as it is a Bayesian model, it will take a significantly longer amount of time to estimate. Given our dataset, it won’t be a big difference, but with larger datasets the time can add up, especially given the ZOIB’s relative complexity. Because the ZOIB has three sub-models, we have to include two additional formulas for \\(zoi\\) (zero inflation) and \\(coi\\) (one inflation). We wrap the formulas in the bf() function because we have multiple formulas for one model:\n\nfit_zoib_est &lt;- brm(bf(appoint_chamber ~ power_groups + I(power_groups^2),\n                    zoi ~ power_groups + I(power_groups^2),\n                    coi ~ power_groups + I(power_groups^2)),\n                    data=vdem,family=zero_one_inflated_beta(),\n                    chains=1,iter = 2000,refresh=0)\n\n\nsummary(fit_zoib_est)\n\n Family: zero_one_inflated_beta \n  Links: mu = logit; phi = identity; zoi = logit; coi = logit \nFormula: appoint_chamber ~ power_groups + I(power_groups^2) \n         zoi ~ power_groups + I(power_groups^2)\n         coi ~ power_groups + I(power_groups^2)\n   Data: vdem (Number of observations: 172) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept               1.54      0.29     0.96     2.11 1.00     1129      832\nzoi_Intercept           0.87      0.29     0.29     1.43 1.00     1531      745\ncoi_Intercept           3.15      0.69     1.96     4.70 1.00     1380      744\npower_groups            0.13      0.21    -0.33     0.50 1.00     1107      763\nIpower_groupsE2        -0.12      0.16    -0.43     0.20 1.00      762      802\nzoi_power_groups        0.25      0.26    -0.28     0.72 1.00     1169      762\nzoi_Ipower_groupsE2     0.50      0.20     0.12     0.94 1.00      829      547\ncoi_power_groups        1.11      0.35     0.51     1.88 1.00      717      552\ncoi_Ipower_groupsE2    -0.09      0.26    -0.57     0.42 1.01     1136      719\n\nFurther Distributional Parameters:\n    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi     4.80      1.38     2.53     7.91 1.00      821      507\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nThe model for continuous values is reported without a prefix (power_groups) while the discrete 0/1 models have a zoi and coi prefix. The models for continuous values match what we had with OLS, but are way less precise. The sub-models for discrete values show purely positive coefficients. For the coi model, which predicts 1s, this would match the continuous values (positive relationship). For the 0s zoi model, though, it would be the opposite direction: more powerful social groups are associated with all appointed seats in the legislature.\nThese results are interesting but quite complicated. If we want to get one overall effect, we actually have to go a second step and calculate predictions that average over all 3 submodels. We can do that with the conditional_effects function from brms:\n\nzoib_pred &lt;- conditional_effects(fit_zoib_est)[[1]]\n\nzoib_pred %&gt;% \n  ggplot(aes(y=estimate__,x=power_groups)) +\n  geom_ribbon(aes(ymin=lower__,\n                  ymax=upper__),fill=\"blue\",\n              alpha=0.5) +\n  geom_hline(yintercept=1,linetype=2) +\n  geom_line() +\n  theme_tufte2 +\n  scale_y_continuous(labels=scales::percent_format()) +\n  labs(y=\"% Seats Elected\",\n       x=\"Power of Social Groups\")\n\n\n\n\nOLS Predicted Values of Elected Lower Chamber Seats Given Power of Social Groups\n\n\n\n\nWe can see in this plot that increasing levels of social group power are associated with greater numbers of appointed seats up until fairly high levels of group power. At the highest levels, there does not seem to be much of a relationship. As a result, it would seem that the ZOIB model has a different spin than OLS, showing really strong associations at lower levels of social group power but little association at high levels of group power.\nOf course, a big complication is that we can’t get a single effect from the model’s coefficients, only by averaging over them and creating marginal effects. To address this issue, I next use ordered beta regression.\n\n\nOrdered Beta\nTo fit the ordered beta regression model, I use the ordbetareg package. This package is a relatively simple which front-end to brms, with a simpler syntax because there is only one linear model. All of the other many features of brms, such as mixed/random effects, dynamic models and latent variables, can all be used with ordbetareg.\n\nfit_ord_est &lt;- ordbetareg(appoint_chamber ~ power_groups + I(power_groups^2),\n                          data=vdem,\n                          chains=1,iter=2000,refresh=0)\n\nWe can see from the coefficients below that ordered beta regression is close to ZOIB in not finding a quadratic effect of power_groups on appoint_chamber:\n\nsummary(fit_ord_est)\n\n Family: ord_beta_reg \n  Links: mu = identity; phi = identity; cutzero = identity; cutone = identity \nFormula: appoint_chamber ~ power_groups + I(power_groups^2) \n   Data: data (Number of observations: 172) \n  Draws: 1 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 1000\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept           1.31      0.23     0.87     1.75 1.00      839      743\npower_groups        0.47      0.14     0.19     0.74 1.00      792      585\nIpower_groupsE2     0.08      0.11    -0.11     0.30 1.00      882      598\n\nFurther Distributional Parameters:\n        Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nphi         4.93      1.21     2.90     7.60 1.00      838      814\ncutzero    -1.64      0.44    -2.54    -0.82 1.00      766      526\ncutone      0.64      0.19     0.24     1.01 1.00      893      629\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAlthough we know the relationship from the coefficients, we can also look at predicted values:\n\nord_pred &lt;- conditional_effects(fit_ord_est)[[1]]\n\nord_pred %&gt;% \n  ggplot(aes(y=estimate__,x=power_groups)) +\n  geom_ribbon(aes(ymin=lower__,\n                  ymax=upper__),fill=\"blue\",\n              alpha=0.5) +\n  geom_hline(yintercept=1,linetype=2) +\n  geom_line() +\n  theme_tufte2 +\n  scale_y_continuous(labels=scales::percent_format()) +\n  labs(y=\"% Seats Elected\",\n       x=\"Power of Social Groups\")\n\n\n\n\nOLS Predicted Values of Elected Lower Chamber Seats Given Power of Social Groups\n\n\n\n\nTo compare the two models directly, the plot below includes predicted values from both the ZOIB and ordered beta regression:\n\nbind_rows(ord_pred,zoib_pred,.id=\"Model\") %&gt;% \n  mutate(Model=factor(Model, labels=c(\"Ordered Beta\",\n                                      \"ZOIB\"))) %&gt;% \n  ggplot(aes(y=estimate__,x=power_groups)) +\n  geom_ribbon(aes(ymin=lower__,\n                  ymax=upper__,fill=Model),\n              alpha=0.5) +\n  geom_hline(yintercept=1,linetype=2) +\n  geom_line(aes(linetype=Model)) +\n  theme_tufte2 +\n    scale_fill_viridis_d() +\n  scale_y_continuous(labels=scales::percent_format()) +\n  labs(y=\"% Seats Elected\",\n       x=\"Power of Social Groups\")\n\n\n\n\nComparison of Ordered Beta and ZOIB Model Predictions of Elected Seats\n\n\n\n\nThe differences between these models are intriguing. First, the two models primarily diverge at predicting the probability of all seats being appointed, which is not surprising as that is where the ZOIB sub-model finds a contrasting relationship. In addition, the uncertainty intervals for ordered beta are noticeably smaller. This is a feature of ordered beta regression that I documented in my paper–because it uses fewer parameters and is less complicated, it will generally return more precise estimates. This increase in precision is essentially cost-less: by modeling the data more carefully, it is possible to have more power. While in this case the difference would not affect inference, it is of course quite possible that it could.\nNext I compare ordered beta regression to all of the other estimators one by one in the plot below. The ordered beta prediction is in the dark gray ribbon:\n\nlm_pred_df &lt;- tibble(estimate__=lm_pred$fit,\n                     power_groups=ex_data) %&gt;% \n              mutate(lower__=estimate__ - 1.96*lm_pred$se.fit,\n                     upper__=estimate__ + 1.96*lm_pred$se.fit)\n\nfrac_pred_df &lt;- tibble(estimate__=pred_power_groups_frac$fit,\n                     power_groups=ex_data) %&gt;% \n              mutate(lower__=estimate__ - 1.96*pred_power_groups_frac$se.fit,\n                     upper__=estimate__ + 1.96*pred_power_groups_frac$se.fit)\n\npred_beta_reg_df &lt;- tibble(estimate__=pred_beta_reg$fit,\n                     power_groups=ex_data) %&gt;% \n              mutate(lower__=estimate__ - 1.96*pred_beta_reg$se.fit,\n                     upper__=estimate__ + 1.96*pred_beta_reg$se.fit)\n\nord_pred_rep &lt;- lapply(1:4, function(i) ord_pred) %&gt;% \n                bind_rows(.id=\"Model\") %&gt;% \n  mutate(Model=factor(Model, labels=c(\"ZOIB\",\n                                      \"OLS\",\n                                      \"Fractional Logit\",\n                                      \"Transformed Beta\")))\n\nbind_rows(zoib_pred,\n          lm_pred_df,\n          frac_pred_df,\n          pred_beta_reg_df,\n          .id=\"Model\") %&gt;% \n  mutate(Model=factor(Model, labels=c(\"ZOIB\",\n                                      \"OLS\",\n                                      \"Fractional Logit\",\n                                      \"Transformed Beta\"))) %&gt;% \n  ggplot(aes(y=estimate__,x=power_groups)) +\n  geom_ribbon(data=ord_pred_rep,aes(ymin=lower__,\n                  ymax=upper__),fill=\"black\",\n              alpha=0.5) +\n  geom_line(data=ord_pred_rep) +\n  geom_ribbon(aes(ymin=lower__,\n                  ymax=upper__),fill=\"blue\",\n              alpha=0.5) +\n  geom_hline(yintercept=1,linetype=2) +\n  geom_line(aes(linetype=Model)) +\n  theme_tufte2 +\n  facet_wrap(~Model) +\n  scale_y_continuous(labels=scales::percent_format()) +\n  labs(y=\"% Seats Elected\",\n       x=\"Power of Social Groups\")\n\n\n\n\nComparison of Model Predictions of Elected Seats to Ordered Beta Predictions (Gray Ribbon)\n\n\n\n\nThis plot shows that most other estimators are less precise, especially at low levels of power_groups and number of elected seats. OLS is more precise at these low numbers of elected seats, but is way more imprecise at large numbers of elected seats, and of course predicts well outside the boundary. Fractional logit is far less precise at lower values of elected seats, but somewhat more precise at high values of elected seats. This peculiar behavior is likely due to the nature of the Bernoulli distribution: fractional logit will perform better when the data are almost discrete. The transformed beta regression is remarkably, and dangerously, different from ordered beta and the other estimators. This quite strong divergence is the reason why this transformation should not be used in practice."
  },
  {
    "objectID": "post/limited_dvs/index.html#footnotes",
    "href": "post/limited_dvs/index.html#footnotes",
    "title": "What To Do (And Not to Do) with Modeling Proportions/Fractional Outcomes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs Staffan Betner informed me, it is possible to convert the fractional logit specification into an actual distribution known as the continuous Bernoulli. However, to do so a normalizing function has to be added to the distribution which is ugly as sin, involving the inverse hyperbolic tan function. Generally speaking, if you have to use the inverse hyperbolic tan function to get your distribution square, you are headed towards a dark place. In any case, the continuous Bernoulli is a different specification even if it shares part of its formula with the simpler fractional logit model.↩︎"
  },
  {
    "objectID": "post/panel_dag_new/index.html",
    "href": "post/panel_dag_new/index.html",
    "title": "The Causal Representation of Panel Data: A Comment On Xu (2022)",
    "section": "",
    "text": "NB: An earlier version of this post critiqued Victor Chernozhukov’s approach to directed a-cyclic graphs and fixed effects, but made some critical errors in interpreting his approach. These errors were entirely mine, and I apologize to Victor for doing so.\nWith the explosion of work in the causal analysis of panel data, we have an ever-increasing array of estimators and techniques for working with this form of observational data. Yiqing Xu’s new review article, available here, provides a remarkably lucid approach to understanding these new approaches. In this blog post I focus on some areas that I think are unclear in current panel data analysis in a causal perspective, especially fixed effects and the dimensions of variance in panel data. People familiar with my prior work will probably not be surprised at this; I believe that understanding these dimensions is critical to correct inference with panel data.\nFor those reading this post from disciplines outside the social sciences, or who are unfamiliar with this rapidly changing literature, panel data refers to any dataset with multiple cases or units (such as persons or countries), and multiple observations for each case or unit over time. Traditionally, panel data has been modeled by including intercepts for cases and/or time points to represent the structure of the data. The method usually employed in political science and economics is to use independent intercepts for each time point and/or case, which are called “fixed effects”. In other fields, “varying” intercepts are included that are assumed to be Normally distributed, such as is common in hierarchical/multilevel models. Confusingly, these types of models are referred to as “random effects” in the economics literature. Xu’s paper provides a new framework for thinking about panel data in causal terms, and so is moving beyond these modeling strategies, although it necessarily begins with them.\nXu’s piece, as we would expect, starts with formulas based on the potential outcomes framework. I reproduce his equation 1 here:\n\\[\n\\{Y_{it}(0),Y_{it}(1)\\} \\perp D_{is} | \\textbf{X}_i^{1:T},\\alpha_i, \\textbf{f}^{1:T}, \\forall i, t, s.\n\\] This is considered to be the strict exogeneity assumption, or what Xu asserts “researchers usually invoke … when estimating a TWFE [two-way fixed effects] model.”1 Breaking this down, this equation says that treatment assignment \\(D_{is}\\) for a given unit \\(i\\) in time \\(s\\) is independent of the potential outcomes for that unit in that time period. As a result, if we only had observed outcomes (which of course is all we have), we can substitute either \\(Y_{it}(0)\\) or \\(Y_{it}(1)\\) depending on whether we observe \\(D_{is}=1\\) or \\(D_{is}=0\\) and we can still, at least theoretically, get an unbiased estimate of the treatment effect.\nThis does follow mathematically. But it is a somewhat strange way of presenting this problem because, properly speaking, that is counterfactual inference for a single unit at a single point in time. In other words, this is inherently unknowable as we can only observe one potential outcome for a single time point. The potential outcomes framework, of course, is based on using substitutes for missing potential outcomes. We use a control group to approximate \\(Y_{it}(0)\\) and a treatment group for \\(Y_{it}(1)\\) and we randomize treatment assignment to get an average treatment effect.\nWhile I know this seems nitpicky, I think it matters for where we begin when we discuss causal inference in panel data, especially when we start adding in fixed effects and other features of the data. To make matters a bit clearer, let’s return to the classic Rubin formulation for the ATE:\n\\[\n\\hat{ATE} = E[Y_i|D_i=1] - E[Y_i|D_i=0]\n\\] What isn’t commonly discussed when presenting this formula is that this represents cross-sectional inference. We have a cross-section of units \\(i\\) at a given time point \\(t\\). The time dimension is ignored because in the simple experiment we assign the treatment randomly at one point in time and then record the outcome.\nWith panel data things get interesting because we are adding in a new dimension of time. We observe now multiple observations for each unit \\(i\\). What this means is that we can define a new \\[\\hat{ATE_t}\\] for over-time inference for a given unit, such as unit \\(1\\):\n\\[\n\\hat{ATE_t} = E[Y_{1t}|D_{1t}=1] - E[Y_{1t}|D_{1t}=0]\n\\] In other words, we can now get an estimate of the average treatment effect solely by using repeated observations for a given unit \\(i\\) in which at some time points, unit \\(i\\) is in treatment, and in some time points, unit \\(i\\) is in control. However, there is a wee bit of a problem–using this formula naively would require that treatment assignment is independent of the potential outcomes, but we can’t do that kind of random assignment as it involves moving across time, not just space. We can randomize in each time point one after the other (i.e., sequentially), but that’s not the same thing as a cross-section randomization because randomization won’t happen simultaneously to all observations, introducing potential correlation with time as a confounder if the unit changes over time.\nMy point here is simply to say that with panel data we implicitly have two different \\(ATE\\)s, a cross-sectional \\(\\hat{ATE}_i\\) for each time point and an over-time \\(\\hat{ATE_t}\\) for each unit. For those of you familiar with my paper with Jon Kropko, this corresponds to our definition of an over-time regression coefficient and a cross-sectional (between-case) regression coefficient. What is important–and very helpful in understanding these estimands–is that all panel data estimands are combinations of these two basic dimensions of variation.\nThis notation should also make it clear what the challenges for inference are with panel data. While many researchers prefer over-time inference, it is clear that we can never obtain randomization of the same nature over time as we can with a cross-section. On the other hand, over-time inference is often considered to be better because it is thought that there is less heterogeneity in the same unit observed over time compared to a cross-section of different units at a given point in time.\nAs I discuss in my earlier blog post, this folk theorem isn’t necessarily true. It’s easy to imagine situations where over-time inference could be more faulty than a cross-section, especially if the time periods were quite long, such as a panel with hundreds of years. A cross-section would have less heterogeneity in a given year than the same country compared over 300 years apart.\nIn the limit, it’s of course straightforward to see when over-time inference should be most credible, and that is when \\(t\\) is of the shortest duration. If we can observed repeated observations of the same unit in and out of treatment status, and those time periods are very close together, then we would have much more reason to believe that the units are comparable and treatment status is ignorable.\nThis definition of over-time inference also makes it easier to see what one of the most popular assumptions, sequential ignorability, is all about. In Xu’s formulation, sequential ignorability is similar to his first formula except that he dropped the fixed effects:\n\\[\n\\{Y_{it}(0),Y_{it}(1)\\} \\perp D_{it} | \\textbf{X}_i^{1:T}, Y_i^{1:(t-1)} \\forall i, t.\n\\] I think that estimating an over-time \\(\\hat{ATE}_t\\) provides a straightforward way to understand this common assumption. We need sequential ignorability because time limits us; we can’t randomize simultaneously across time points without a time machine, and even Stanford’s vaunted methods program hasn’t invented one yet.2\nPut simply, we are taking an average of over-time observations for each unit \\(i\\). Of course, we need to think about the order in which we do the averaging. I think it would make the most sense to calculate an \\(\\hat{ATE}_t\\) for each unit \\(i\\), and then average those \\(\\hat{ATE}_t\\)s for all units \\(i\\) to get a more precise estimate of \\(\\hat{ATE}_t\\). Of course, we need sequential ignorability assumptions here to do the averaging and get an unbiased estimate."
  },
  {
    "objectID": "post/panel_dag_new/index.html#fixed-effects-and-dags",
    "href": "post/panel_dag_new/index.html#fixed-effects-and-dags",
    "title": "The Causal Representation of Panel Data: A Comment On Xu (2022)",
    "section": "Fixed Effects and DAGs",
    "text": "Fixed Effects and DAGs\nSo far, hopefully, so good. However, potential outcomes alone struggle to answer all causal queries, which is why DAGs have gotten so popular. These diagrams show causal arguments as a collection of variables with arrows pointing in the direction of causality. Intuitive yet very powerful, they can make much clearer what we mean by a causal query.\nThe DAGs are where the limitations of an analysis that ignores the dimensions of variation becomes the most clear. I reproduce below the DAG representing Xu’s “strict exogeneity” (i.e. two-way fixed effects (TWFE) estimation) equation:\n\nThis DAG has a lot going on. However, one thing it does not have is a clear indication of the dimensions of variation. We are told simply that this is “panel data.” Which is fine, but–what kind? Looking more closely, we see on the figure caption that “Subscript \\(i\\) for unit is omitted for simplicity.” This should be a good clue–we are looking at an over-time \\(\\hat{ATE_t}\\) for a single unit. This unit is observed in different time points, presumably with varying treatment status \\(D_t\\).\nHowever, now we have fixed effects as well in the diagram. For the uninformed, these represent dummy variables or varying intercepts for each time point \\(t\\) and unit \\(i\\) (assuming we have a two-way FE model). We are told that the dashed gray arrows represent unobserved relationships with unobserved variables, and the fixed effects \\(\\alpha\\) and \\(f_t\\) are unobserved variables. We only have one unit intercept \\(\\alpha\\) because we are only examining a single unit in the DAG.\nThis is where things start to get a bit tricky. What are fixed effects and how do we represent them in a causal diagram? Here we are told that they are unobserved confounders. This is a somewhat strange definition. A “fixed effect” is simply a dummy variable for either a case or a time point. It doesn’t in and of itself have any meaning in the sense of a causal factor like income or the presence of COVID-19. In other words, there is an ontological problem in putting fixed effects into a DAG like this because they are substantively different than the other variables, which are more properly real things.\nI think this is an important and largely overlooked question in Judea Pearl’s causal diagram analysis: what are the criteria for deciding what is a variable we can manipulate as opposed to just a coefficient from a model without any substantive meaning?\nI’ll make the claim that, in a causal diagram, every node on the graph has to be its own separate causal factor. For that to be true, it has to have some kind of independent existence in the “real world.” We could also call it a random variable–some trait we can observe that can take on multiple values. If we have a complete and accurate causal graph, we can then know with confidence which variables we need to measure to ensure so that our statistics have a causal interpretation.\nNow going back to Figure 2 above, it’s useful to think about the nodes on this graph and what they mean. The DAG is claiming that the fixed effects \\(\\alpha\\) and \\(f_t\\) intercepts confound the relationship between \\(D_{t}\\) and \\(Y_{t}\\), and interestingly, also cause \\(D_{t}\\) in some sense. Based on this DAG, we must then include these intercepts to identify the effect of \\(D_{t}\\). Problem solved! We can all now go get a beer and call it a day.\nWell… not so fast. Again, one element of causal analysis is that we are studying things “in the real world.” Each element of our graph must be some factor or force or element which we could (at least in theory) manipulate, or apply Pearl’s \\(do()\\) operator. Does including an intercept for each case meet that standard? I would say that in general, no. We can’t go and manipulate an intercept in the “real world.” Why do we use intercepts? Because we want to focus on a particular dimension of panel data, i.e. we want to estimate either \\(\\hat{ATE}_i\\) or \\(\\hat{ATE_t}\\) (or some combination thereof).\nTo illustrate this, I’m going to start with a DAG that is much easier to understand and that has an empirical meaning. In this DAG, the outcome \\(Y_{it}\\) represents the level of democracy in a country \\(i\\) in year \\(t\\). We want to know what the causal effect of the level of gross domestic product (\\(G_{it}\\)) is on a country’s democracy over time. Unfortunately, there are potential confounding variables in this relationship.\nIn the diagram I also put \\(F_i\\), which represents a country’s factor endowment, or types of soil and other natural resources. As Engerman and Sokoloff argue, it could be that the type of natural environment predisposed some countries to more repressive types of agriculture such as large slaveholder plantations, and slave-owning countries and regions were more resistant to democracy over time. We know that slave-owning areas also tend to be poorer over time as well as they are less likely to industrialize. As a consequence, we are concerned that the relationship between GDP and democracy will be confounded by the level of factor endowments. Note that for factor endowments I only include the subscript \\(i\\): a country’s factor endowments are fixed over time, unlike GDP, which can vary over time and also across countries at a single point in time.\n\n\n\n\n\n\n\n\n\nThis sort of setup is the typical justification for including fixed effects, or one intercept/dummy variable for each country \\(i\\): if we include \\(\\alpha_i\\) then it will adjust for \\(F_i\\) so long as the two are equivalent (\\(\\alpha_i \\equiv F_i\\)). This seems easy and straightforward: \\(\\alpha_i\\) only varies by country and \\(F_i\\) only varies by country. Bingo.\nWell, I hate to be the bearer of bad news, but including \\(\\alpha_i\\) as an adjustment variable to account for \\(F_i\\) doesn’t get you the same thing because \\(\\alpha_i\\) and \\(F_i\\) are not equivalent, logically or otherwise. \\(F_i\\) is a vector of factor endowments with a plausible range from zero to a very large number, depending on how exactly we want to measure endowments. \\(\\alpha_i\\), by contrast, is either a categorical variable where each value is the particular country in the data, or a matrix of dummy variables minus one country which serves as the reference category. Declaring these two equivalent is… odd.\nIn particular, something happens to the DAG above when we include \\(\\alpha_i\\) as an adjustment factor instead of just \\(F_i\\). The following DAG shows what happens:\n\n\n\n\n\n\n\n\n\nThe above formula is likely familiar to people who have studied panel data in economics or political science. For both the treatment GDP and the outcome of democracy, \\(Y_{it}\\), I subtracted away the average value for each case \\(i\\). This process is also called “de-meaning” (which no one has yet made clever puns about, unfortunately) as we convert the two variables into deviations away from their over-time mean. Why did this happen? Again, the \\(\\alpha_i\\) represent here an intercept for every country. If we have 100 countries, then we have a giant matrix of ninety-nine dummy variables. Each dummy variable will absorb everything that doesn’t vary over time in that country. Think of it like a tornado that sweeps through the country, leaving behind only time-varying debris.\nNow did the \\(\\alpha_i\\) get rid of the confounding variable \\(F_i\\)? Well, sort of. If you think about it, we couldn’t keep \\(F_i\\) in the DAG because there is only one value of factor endowment per country, so the average of a single number is…. that value. You subtract it away and you have zero. This neat math trick is what first motivated economists to think of fixed effects (intercepts for cases) as a cool way to get rid of confounders.\nHowever, tornadoes have side effects. We no longer have \\(Y_{it}\\) as our outcome, we have \\(Y_{it} - \\bar{Y_i}\\). In other words, we no longer have to worry about \\(F_i\\) or any other time-invariant confounder, but we’ve also changed the question we’re asking. We have never answered the question posed in the earlier DAG about what the effect of GDP on democracy is because… we now only have above or below average GDP regressed on above or below average democracy, or what we can also call \\(\\hat{ATE_t}\\).\nSo is \\(\\alpha_i\\) an adjustment set or control variable for factor endowments? No, it isn’t. It’s true that if you include it in the model, you won’t need to worry about factor endowments, but that’s as comforting as telling someone trying to find shelter in order to protect themselves from a tornado that they no longer need to worry about being late for the plane flight they were supposed to catch later that day. True, but largely irrelevant to the problem at hand.\nIf we want to know what the causal effect of \\(G_{it}\\) on \\(Y_{it}\\) is when accounting for factor endowments, then we need to include a measure of factor endowments or find a way of manipulating a country’s income in a way that is independent of other causal factors (i.e. random assignment). Including fixed effects or intercepts for countries/cases does something different; it isolates a dimension of variation in the outcome so that we can obtain an estimate of either \\(\\hat{ATE}_t\\) or \\(\\hat{ATE}_i\\). This is itself an important thing to do as the two subscripts of \\(Y_{it}\\) both matter but have different interpretations. We may be more interested, for example, in comparing countries at cross-sections for each year period. In that case, we could include intercepts for time points in the model. Or we might prefer to compare countries to themselves over time–in that case we would include the \\(\\alpha_i\\) as above."
  },
  {
    "objectID": "post/panel_dag_new/index.html#conclusion",
    "href": "post/panel_dag_new/index.html#conclusion",
    "title": "The Causal Representation of Panel Data: A Comment On Xu (2022)",
    "section": "Conclusion",
    "text": "Conclusion\nSo circling back to the original question… how should we represent fixed effects in a DAG? I don’t think they belong there. Rather, I think we should explicitly show the de-meaning process or notation that signifies it, such as \\(\\hat{ATE_t}\\) or \\(\\hat{ATE_i}\\). For example, we could break up Figure 2 into two different DAGS, one with \\(Y_{it} - \\bar{Y_i}\\) as the outcome, representing \\(\\hat{ATE}_t\\) and the other as \\(Y_{it} - \\bar{Y_t}\\) as the outcome, representing \\(\\hat{ATE_i}\\).\nI discussed this with Yiqing Xu, and he said that the issue can also be framed as having a DAG that also has functional form assumptions baked in. In other words, when we write a DAG, it is supposed to express how random variables are plausibly related to each other–not just specific models. When we start within the framework of a linear model, even one as basic as a panel data fixed effects model, it can be difficult to derive an intellectually-satisfying DAG. Instead, if possible, we want our DAG to represent our substantive knowledge that can subsume various specifications. Granted, this is really, really hard in very general terms, and we probably have more work to do, which is fine. Scholars needs jobs, after all.\nMaking these distinctions clear can also help clarify more complicated estimators, such as the somewhat infamous new DiD literature. The basic DiD estimand is simply the difference of two cross-section \\(ATE\\)s, or \\(\\hat{ATE}_{i2} - \\hat{ATE}_{i1}\\) for two time points 1 and 2. More complicated DiD estimators are at bottom other ways of doing these averages, either averaging for all cross-sections in a given time point or for all time points for a given unit. We can get creative by interacting and subtracting the dimensions from each other, but we still only have two of them.\nFinally, there is another way of baking the cake, and that is to try to explicitly estimate a latent confounder rather than rely on intercepts. This is the approach taken by so-called interactive fixed effects models, such as Xu’s other work known as generalized synthetic control, and the matrix completion methods, such as those by Victor Chernozhukov, Susan Athey and Guido Imbens, among others. In this case, the coefficient in a regression model can be given the interpretation of adjusting for latent confounders. The trick with these methods, of course, is how to select the correct form and dimension of this latent variable, and so I think there is more room for work in this area as well.\nIf you want to read more on the topic of panel data, I have both a prior blog post and a co-authored paper which I recommend you read at your leisure while avoiding tornadoes, miss-specified causal graphs and false equivalencies."
  },
  {
    "objectID": "post/panel_dag_new/index.html#footnotes",
    "href": "post/panel_dag_new/index.html#footnotes",
    "title": "The Causal Representation of Panel Data: A Comment On Xu (2022)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNB: The formula above does not have the true independence symbol as it is not in base latex for whatever reason.↩︎\nGranted, if they had one, they probably wouldn’t tell us about it.↩︎"
  },
  {
    "objectID": "post/logs/index.html",
    "href": "post/logs/index.html",
    "title": "What’s Logs Got to Do With It?",
    "section": "",
    "text": "Twitter (or what’s left of it) was recently ablaze with a discussion of two smart working papers, one by Jiafeng Chen and Jonathan Roth and the other by John Mullahy and Edward Norton. In different ways, they argue against the use of non-linear transformations in applied modeling, especially logarithms (logs). I had been meaning to discuss the subject myself; I’ve been critical of the logarithmic transformation previously because it has been difficult to put a clear meaning on the transformation in a linear model. Furthermore, the inability of logs to include 0 (log of 0 is undefined or negative infinity, whichever is smaller) was a warning flag that there was a mismatch between what people wanted the math to do, and what it actually did.\nWhat I’m going to show in this post is that the ordered beta regression model can also address issues with logs (and the related inverse hyperbolic sine transformation) because it can produce estimates (including ATEs) that are based on proportions, and thus naturally scale-free. When the scale of the outcome is an issue, the ordered beta regression can help address that problem by estimating regression coefficients or treatment effects that do not vary with scale and also include 0s.\nThe two papers presented above approach the log issue in different ways, which makes them both fascinating when read together. Mullahy and Norton take a more traditional econometric approach, discussing how the log and hyperbolic sine transformations are heavily influenced by the constant chosen to either “bump” zeros to 1s for the log or to scale the hyperbolic sine. Chen and Roth use different notation to discuss how the logarithmic transformation affects estimation of an average treatment effect (ATE) using the potential outcomes framework in the causal inference literature.\nBoth sets of authors point out that these transformations end up changing either linear regression coefficients or estimated ATEs in ways that are not immediately obvious. Using economic theory, Mullahy and Norton discuss how adding 1 to any zero value in a model in order to use logs results in the model weighting estimates by the proportion of 0s and non-0 values in the data, or what they call the “intensive” and “extensive” margin. The regression coefficient will vary depending on whether 0s are replaced by 1, 0.9, 0.8, 0.7… etc. There simply isn’t one good value that can be added to the data to allow a logarithm to be well-defined.\nChen and Roth make the point that the estimate of an ATE can vary with different scaling functions such as logarithms. As they put it,\nIn other words, once you transform the outcome using one of these functions, we no longer know for sure that we will get the same ATE if we change the units, i.e. from dollars to cents, when using one of these common transformations. Chen and Roth’s suggestion, which is similar as well to Mullahy and Norton, is to consider using models that incorporate 0s (two-part models, also known as hurdles/zero-inflated models) or simple GLMs like the Poisson regression, which has had an amazing resurgence on Twitter at the moment. (No idea, though, if Poisson’s momentum will last through the 2024 election.)"
  },
  {
    "objectID": "post/logs/index.html#simulation",
    "href": "post/logs/index.html#simulation",
    "title": "What’s Logs Got to Do With It?",
    "section": "Simulation",
    "text": "Simulation\nI next use simulations to discuss this issue to show practically what the authors are talking about and also demonstrate how ordbetareg can estimate scale-free ATEs.\nI simulate data that matches what the articles discuss: highly skewed outcomes with zeroes. To do so I’ll simulate a two-part or hurdle model in which the ATE influences the first part (the probability of a 0) and the second part (a conventional log-normal distribution strictly greater than 0). The ATE will influence both the probability of 0s, which at baseline is 10% of total observations, and the mean of the non-0 (log-normal) distribution. I will assume a true ATE of +1 on the log/logit scale. I can then simulate the potential outcomes, Y1 and Y0, as independent distributions with a difference of the ATE.\n\nate &lt;- 1\nN &lt;- 500\nY0 &lt;- ifelse(runif(N)&lt;0.9,rlnorm(N),0)\nY1 &lt;- ifelse(runif(N)&lt;plogis(qlogis(0.9) + ate),\n             rlnorm(N,meanlog=ate),\n             0)\n\nWe can take a quick glance at the distributions:\n\nhist(Y1)\n\n\n\n\n\n\n\nhist(Y0)\n\n\n\n\n\n\n\n\nThe proportion of zeroes in Y0 is 0.086 and the proportion of zeroes in Y1 is 0.034.\nWe can then do some conventional comparisons of the treatment and control group, a t-test and linear model fit:\n\nt.test(Y1,Y0)\n\n\n    Welch Two Sample t-test\n\ndata:  Y1 and Y0\nt = 11.45, df = 639.8, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 2.423978 3.427505\nsample estimates:\nmean of x mean of y \n 4.330918  1.405176 \n\nt_data &lt;- tibble(Y=c(Y0,Y1),T=c(rep(0,N),rep(1,N)))\nmod1 &lt;- lm(Y ~ T,data=t_data)\nsummary(mod1)\n\n\nCall:\nlm(formula = Y ~ T, data = t_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.331 -1.839 -0.795  0.423 48.338 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.4052     0.1807   7.777 1.84e-14 ***\nT             2.9257     0.2555  11.450  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.04 on 998 degrees of freedom\nMultiple R-squared:  0.1161,    Adjusted R-squared:  0.1152 \nF-statistic: 131.1 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\n\nThe linear model and the t-test do not return the true ATE as that was simulated on the log scale. However, if we take the log of the estimate, it is pretty close – log of 2.926 is 1.074. The simulation is not perfect because the ATE will vary across the two parts due to scaling effects, but it will work for our purposes.\nAs the papers pointed out, the ATE could change if we use nonlinear transformations such as log with an adjustment such as 1 or 0.1 to replace the zeroes. We will test this by fitting two models, one with an outcome of \\(log(Y+1)\\) and the other with \\(log(Y+0.01)\\). We can then make a table of the model results with the modelsummary package:\n\nt_data &lt;- mutate(t_data,\n                 Y_1=Y + 1,\n                 Y_01=Y + 0.01)\n\ny_mod1 &lt;- lm(log(Y_1) ~ T,data=t_data)\ny_mod2 &lt;- lm(log(Y_01) ~ T, data=t_data)\n\nmodelsummary(models=list(`log(Y + 1)`=y_mod1, \n                         `log(Y + 0.01)`=y_mod2))\n\n \n\n  \n    \n    \n    tinytable_bmsn8qr24098pqqlr6tc\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                log(Y + 1)\n                log(Y + 0.01)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.706   \n                  -0.430   \n                \n                \n                             \n                  (0.029) \n                  (0.067)  \n                \n                \n                  T          \n                  0.650   \n                  1.236    \n                \n                \n                             \n                  (0.041) \n                  (0.095)  \n                \n                \n                  Num.Obs.   \n                  1000    \n                  1000     \n                \n                \n                  R2         \n                  0.200   \n                  0.146    \n                \n                \n                  R2 Adj.    \n                  0.200   \n                  0.145    \n                \n                \n                  AIC        \n                  4042.5  \n                  4028.7   \n                \n                \n                  BIC        \n                  4057.2  \n                  4043.4   \n                \n                \n                  Log.Lik.   \n                  -987.324\n                  -1822.961\n                \n                \n                  F          \n                  250.073 \n                  169.999  \n                \n                \n                  RMSE       \n                  0.65    \n                  1.50     \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nWe can see that the treatment effects are quite different. To get them back on the same scale, we can try a reverse transform (exponential the coefficient):\n\nexp(coef(y_mod1)[2]) - 1\n\n        T \n0.9159342 \n\nexp(coef(y_mod2)[2]) - .01\n\n       T \n3.433134 \n\n\nWe can see that the reverse transform doesn’t improve matters. In either case we end up with a very different coefficient. If we were to interpret the outcome as dollars, such as a salary, then the ATE in the first model would be 0.92 and the ATE in the second would be 3.28. As the papers suggest, changing that constant results in changes in the ATE, and so we need to be careful what constant we choose.\nTo see if units matters as the papers also propose, we will divide the outcome by 100 and compare to the original outcome while using the same transformation of \\(log(Y + 1)\\):\n\nt_data &lt;- mutate(t_data,\n                 Y_1_rescale=(Y/100)+1)\n\ny_mod1_rescale &lt;- lm(log(Y_1_rescale) ~ T,data=t_data)\n\nmodelsummary(models=list(`log(Y + 1) Original`=y_mod1, \n                         `log(Y + 1) Rescaled`=y_mod1_rescale))\n\n \n\n  \n    \n    \n    tinytable_l0d795q37l9gp233dj1o\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                log(Y + 1) Original\n                log(Y + 1) Rescaled\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.706   \n                  0.014   \n                \n                \n                             \n                  (0.029) \n                  (0.002) \n                \n                \n                  T          \n                  0.650   \n                  0.027   \n                \n                \n                             \n                  (0.041) \n                  (0.002) \n                \n                \n                  Num.Obs.   \n                  1000    \n                  1000    \n                \n                \n                  R2         \n                  0.200   \n                  0.125   \n                \n                \n                  R2 Adj.    \n                  0.200   \n                  0.124   \n                \n                \n                  AIC        \n                  4042.5  \n                  -3735.4 \n                \n                \n                  BIC        \n                  4057.2  \n                  -3720.7 \n                \n                \n                  Log.Lik.   \n                  -987.324\n                  1898.213\n                \n                \n                  F          \n                  250.073 \n                  142.951 \n                \n                \n                  RMSE       \n                  0.65    \n                  0.04    \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAgain, we can see that rescaling \\(Y\\) and then applying the \\(log(Y+1)\\) transformation results in a remarkably different ATE. Because dividing by a constant doesn’t affect the proportion of zeroes but does affect the non-zero values, the ATE will consequently change when using this transformation."
  },
  {
    "objectID": "post/logs/index.html#ordbetareg-to-the-rescue",
    "href": "post/logs/index.html#ordbetareg-to-the-rescue",
    "title": "What’s Logs Got to Do With It?",
    "section": "ordbetareg to the Rescue",
    "text": "ordbetareg to the Rescue\nWhile the authors propose some solutions to this problem, such as using a two-part model, I want to show that the ordered beta regression can estimate scale-free ATEs if the outcome is normalized; that is, rescaled to between 0 and 1. Here we will let 0 be equal to 0 while 1 will be the sum of the outcome, i.e., the total income of the sample. We can rescale so that each observation in our data is equal to the proportion of the total. It corresponds to the following estimand, which I’ll call \\(ATE_n\\) for normalized:\n\\[\nATE_n = E[\\frac{Y1}{Y1+Y0} - \\frac{Y0}{Y1+Y0}]\n\\]\nCrucially, if we scale the outcome by any value \\(c\\), \\(c\\) will cancel as it will be in both the top and bottom of the fraction. We could estimate this quantity with either OLS or a t-test, but the error term is likely to be misleading as it won’t respect the bounds of the outcome (i.e. 0 or 1). This is where ordered beta regression can help. I’ll estimate two models, one with the original outcome and the second with the original outcome multiplied by a 100 to test for scaling effects. I will specify the true_bounds parameter to be 0 and 1 as by definition there will not be any values at the upper bound of 1.\n\nt_data &lt;- mutate(t_data,\n                 Y_n=Y/(sum(Y)),\n                 Y_n_rescale=(Y/100)/(sum(Y/100)))\n\ny_n_mod &lt;- ordbetareg(Y_n ~ T,data=t_data,\n                      chains=1,iter=1000,\n                      true_bounds=c(0,1),refresh=0,\n                      backend=\"cmdstanr\")\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 3.0 seconds.\n\ny_n_rescale &lt;- ordbetareg(Y_n_rescale ~ T,data=t_data,\n                          chains=1,iter=1000,\n                          true_bounds=c(0,1),refresh=0,\n                          backend=\"cmdstanr\")\n\nRunning MCMC with 1 chain...\n\nChain 1 finished in 3.0 seconds.\n\nmodelsummary(models=list(`Normalized Y Original`=y_n_mod, \n                         `Normalized Y Rescaled`=y_n_rescale),\n             statistic=\"conf.int\")\n\n \n\n  \n    \n    \n    tinytable_uo2jivdxd8o2jm8hepeo\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                 \n                Normalized Y Original\n                Normalized Y Rescaled\n              \n        \n        \n        \n                \n                  b_Intercept\n                  -7.108            \n                  -7.114            \n                \n                \n                             \n                  [-7.196, -7.011]  \n                  [-7.210, -7.012]  \n                \n                \n                  b_T        \n                  0.608             \n                  0.612             \n                \n                \n                             \n                  [0.507, 0.707]    \n                  [0.515, 0.716]    \n                \n                \n                  phi        \n                  810.062           \n                  808.721           \n                \n                \n                             \n                  [732.567, 888.675]\n                  [714.157, 890.808]\n                \n                \n                  Num.Obs.   \n                  1000              \n                  1000              \n                \n                \n                  R2         \n                  0.090             \n                  0.091             \n                \n                \n                  ELPD       \n                  5335.8            \n                  5335.9            \n                \n                \n                  ELPD s.e.  \n                  74.6              \n                  74.7              \n                \n                \n                  LOOIC      \n                  -10671.5          \n                  -10671.8          \n                \n                \n                  LOOIC s.e. \n                  149.3             \n                  149.3             \n                \n                \n                  WAIC       \n                  -10671.6          \n                  -10671.9          \n                \n                \n                  RMSE       \n                  0.00              \n                  0.00              \n                \n        \n      \n    \n\n    \n\n  \n\n\n\n\nAs can be seen, both estimates are very close. We can convert these treatment effects to marginal effects using the marginaleffects package:\n\nmargin_Y &lt;- marginaleffects(y_n_mod, variables=\"T\")\nsummary(margin_Y)\n\n\n Term          Contrast Estimate    2.5 %  97.5 %\n    T mean(1) - mean(0) 0.000887 0.000624 0.00196\n\nColumns: term, contrast, estimate, conf.low, conf.high \n\n\nThe treatment effect here is miniscule as it represents the proportion of income in the sample accruing to an individual unit. However, we can transform this back to the original scale by simply multiplying the estimated marginal effect for each posterior draw by the sum of Y1 and Y0:\n\ntreatment &lt;- posteriordraws(margin_Y) %&gt;% \n  distinct(draw) %&gt;% \n  mutate(T=draw * sum(t_data$Y))\n\n# calculate a summary/point estimate\nsummary(treatment$T)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.585   2.190   2.542   2.812   3.052   9.119 \n\n# plot full distribution/uncertainty\nhist(treatment$T)\n\n\n\n\n\n\n\n\nDoing so gives us a treatment effect of approximately $2.7 dollars on the original scale–or, almost exactly \\(log 1\\) as per our original true ATE. Importantly, the estimated uncertainty of this ATE, which we can visualize by plotting all the posterior draws as a histogram, reflects the skew of the underlying variable, which in this case we would want to capture.\nAs the package designer Vincent Arel-Bundock pointed out, I can also do the code above using the marginaleffects package directly with the comparisons function:\n\ntreatment2 &lt;- avg_comparisons(y_n_mod,\n                          variables=\"T\",\n                          transform=function(x) x * sum(t_data$Y))\n\nprint(paste0(\"The treatment effect estimate is \",\n             round(treatment2$estimate,3),\n             \" with a lower bound of \",\n             round(treatment2$conf.low,3), \n             \" and an upper bound of \",\n             round(treatment2$conf.high,3),\".\"))\n\n[1] \"The treatment effect estimate is 2.543 with a lower bound of 1.788 and an upper bound of 5.631.\""
  },
  {
    "objectID": "post/logs/index.html#conclusion",
    "href": "post/logs/index.html#conclusion",
    "title": "What’s Logs Got to Do With It?",
    "section": "Conclusion",
    "text": "Conclusion\nFor these reasons, ordbetareg can be used to produce ATEs that are scale-free yet can be still be back-transformed to the original scale by rescaling the marginal effects as produced by the marginaleffects package. If the aim of the analysis is to estimate an ATE that is comparable across samples and income distributions, yet also doesn’t lose its relation to the original data, then ordbetareg can provide readily comparable estimates."
  },
  {
    "objectID": "post/frac_logit/index.html",
    "href": "post/frac_logit/index.html",
    "title": "Fixing Fractional Logit?",
    "section": "",
    "text": "This post focuses on one of the more curious models in contemporary statistics, a specification for proportions that is either called fractional logit or quasi-Binomial. An earlier version of this blog post had a much more negative take on the fractional logit specification. After dialogue with people on Twitter (who knew it could be useful??), I have revised this blog post to take into account other perspectives. As such, this blog post is now framed to be more open-ended. I still think there are issues with the utility of the fractional logit model, but understood within the motivation behind it, it does have a use. The question is when you would want to use fractional logit as opposed to a full-fledged stqtistical distribution like the continuous Bernoulli, ordered beta or zero-or-one inflated beta regression models.\nTo sum up my take for those who don’t want to wade through the analysis, I think that fractional logit is probably best applied to situations with big data where computational issues are likely to arise but the correct form of uncertainty is less of an issue. In these situations, fractional logit is an alternative to OLS that respects the bounds of proporitional/fractional outcomes, and is significantly easier to fit than the other alternatives discussed here. In addition, having a large dataset means that inefficiency or an incorrect form for the uncertainty of fractional logit estimates is unlikely to affect decision-making or inference.\nWhere I see less utility for fractional logit is where continuous Bernoulli but more so the beta regression models are valid alternatives. In these cases, fractional logit is supposed to guard against mis-specification worries, but it is not entirely clear how strong that worry can/should be given the specific domain of bounded continuous variables–i.e., there are only so many alternatives. In addition, it is difficult to characterize the performance of fractional logit vis-a-vis other models, which also makes it difficult to know when the specification issue might become more important than other facets of statistical distributions like fit and interpretability.\nIn this revised blog post I include the code and examples from the original post along with additional analyses that get into some of the Twitter feedback along with some of my previous evaluations of fractional logit. My hope is that it communicates honestly the state of research about these models and helps people make an informed choice.\nIf you want the code for this post, you can see the full Rmarkdown file here."
  },
  {
    "objectID": "post/frac_logit/index.html#the-problem-what-a-proportion-is",
    "href": "post/frac_logit/index.html#the-problem-what-a-proportion-is",
    "title": "Fixing Fractional Logit?",
    "section": "The Problem: What a Proportion Is",
    "text": "The Problem: What a Proportion Is\nWe can first start with the definition of the specification that is primarily used for fractional logit (abbreviated FL from here on), which comes from a 1996 paper by Papke and Wooldridge. Given a linear model \\(X\\beta\\), a link function \\(g(\\cdot)\\) (logit), and an outcome \\(Y_i\\) that can take on any value between 0 and 1, we have a likelihood as follows:\n\\[\nL(Y_i) = g(x_i\\beta)^{Y_i}(1-g(x_i\\beta))^{(1 - Y_i)}\n\\]\nThis is not the only likelihood or estimation possible for FL. Technically, the model is defined as the following:\n\\[\nE(Y|X) = g(X\\beta)\n\\]\nIn other words, it only has to make an assumption about how we determine the expected value (average response) of the outcome \\(Y\\). That assumption is that the expected value is equal to a linear model (\\(X\\beta\\)) scaled by the logit function to map on to a proportion (strictly between 0 and 1). I’ll discuss more below what this means in practice. For my purposes, I’ll stick with the likelihood above as that is what has been primmarily used to get estimates of the regression coefficients \\(\\beta\\).\nTo get the intuition behind the likelihood, the definition is in fact the same for the Bernoulli distribution. The only difference is that instead of \\(y_i\\) being a binary variable with values of 0 or 1, \\(y_i\\) could be anything from 0 to 1, including 0.2, 0.4, etc. Inserting these continuous values into a discrete distribution is the clever method that Papke and Wooldridge came up with and labeled fractional for a fraction of 0 and 1. They also showed that it was possible to find a maximum to the likelihood function and obtain estimates for the regression coefficients \\(\\beta\\) on the logit scale.\nThis model represented a substantial step forward from what was available at the time, mainly OLS. There were plenty of GLMs but not for a fractional or proportional response. No one was using beta regression as that model would not be proposed until 2004. For these reasons, fractional logit became very popular and has almost 5,000 citations according to Google Scholar.\nHowever, this might lead to some head-scratching–sticking continuous data in a discrete distribution is, at a minimum, kind of odd. I was critical in my prior version of this blog post because this seemed to me be an objectively worse specification than using a fully formed statistical distribution. However, in my Twitter conversation with Jeffrey Wooldridge and others, the response essentially was, “you can’t fix it because it’s already broke.” In other words, the fractional logit’s lack of connection to a specific distribution is seen as a way of guarding against mis-specification. Or as Prof. Wooldridge put it,\n\n\nSimulation studies aren't even needed. A theorem is a theorem. FL is consistent whenever E(Y|X) is correct. MLE never is in this context. The only issue is efficiency loss compared with MLE when D(Y|X) is fully specified. But that's moving the goalpost for the FL estimator.— Jeffrey Wooldridge (@jmwooldridge) February 20, 2023\n\n\n\nFL in the tweet refers to fractional logit, and \\(E(Y|X)\\) is the expected value of a proportional outcome (0 to 1 inclusive). What Prof. Wooldridge is saying is that the FL estimator is consistent, or will on average be correct, whenever the simple linear model \\(g(X\\beta)\\) is equal to the average value of the outcome. In other words, the statistical distribution doesn’t matter if all we care about is the expected, or average, value of the response.\nTo explain what is going on, it’s helpful to know what the logit function \\(g(\\cdot)\\) does. The logit function scales any number (large or small, negative or positive) to a number strictly within 0 and 1. As a result, saying that the average value is equal to \\(g(X\\beta)\\) is really just saying that the average value is equal to the linear model, which is of course what people expect when running a regression. As such, the model captures a critical part of regression model: how are the covariates on average related to the outcome/response?\nWhat is a bit odd is that fractional logit has a relationship for the expected value \\(E(Y|X)\\) but not for the distribution of \\(Y\\). As another tweeter put it,\n\n\n@rmkubinec To resolve the debate, it’s crucial to clarify the estimand:(i) D(Y|X), D is the dist; or(ii) E(Y|X)For (i), it’s necessary to correctly specify D(Y|X), In this case, CB only works for CB, ordBeta only works for ordBeta, etc.For (ii), it’s not necessary to…1/3— Lihua Lei (@lihua_lei_stat) February 21, 2023\n\n\n\nHere Prof. Lei’s comment is about separating the average/expected value of the response \\(E(Y|X)\\) from the distribution of \\(Y\\), denoted \\(D(Y|X)\\). CB stands for continuous Bernoulli and ordbeta for ordered beta regression, two other models for proportional outcomes I will explain later. Prof. Lei’s point is that if we don’t care about the distribution of \\(Y\\), we can content ourselves with the expected value, which fractional logit can estimate regardless of what \\(D(Y|X)\\) is."
  },
  {
    "objectID": "post/frac_logit/index.html#ok-so-what",
    "href": "post/frac_logit/index.html#ok-so-what",
    "title": "Fixing Fractional Logit?",
    "section": "OK So What?",
    "text": "OK So What?\nThe reader at this point might think, OK, so I’m estimating the expected value of the response with fractional logit, not the full distribution. What’s going to happen to me, the statistical gods will strike me with lightning?1\nThe main drawback of not having a distribution is that we don’t know how certain we can be about the average value \\(E(Y|X)\\) because we don’t ever know the true (or so-called population) value. In frequentist statistics, which is the underlying basis for the paper cited above, the population value is the value we would obtain if we sampled \\(Y\\) over and over again and kept taking averages for \\(E(Y|X)\\) and if we kept averaging all of those averages we would know the “real” \\(E(Y|X)\\). But, in real life, we don’t know that value, just an estimate of \\(E(Y|X)\\), which we can denote \\(\\widehat{E(Y|X)}\\). If our estimator is consistent, as FL is, then we know that \\(\\widehat{E(Y|X)}\\) is an unbiased estimate for \\(E(Y|X)\\)–but only if we were to take many samples \\(\\widehat{E(Y|X)}\\) and average them together. In most cases we have just one dataset, and we want to know how much we can learn about \\(E(Y|X)\\) from our regression coefficients that make up \\(\\widehat{E(Y|X)}\\).2\nFor these reasons, if you read the Papke and Wooldridge paper, you’d find that it spends relatively little time on the specification of model/likelihood and instead most of its time on determining standard errors (and confidence intervals) for the regression coefficients that estimate \\(\\widehat{E(Y|X)}\\). You can derive confidence intervals using maximum likelihood (MLE), but that would treat the FL specification as essentially being a part of the Bernoulli distribution, which it isn’t. As Papke and Wooldridge note, this naive estimate of the uncertainty will be over-confident. For these reasons, they propose a standard error correction that inflates standard errors in a similar manner to the so-called “sandwich” estimator (see the R package sandwich). These corrected standard errors, Papke and Wooldrige argue, are asympotically correct for any distribution \\(D(Y|X)\\), meaning that if we kept drawing samples from any distribution and recorded the average value \\(\\widehat{E(Y|X)}\\) using FL, and then took the average of those samples, it would equal the same expected value as if we also calculated those average values using the appropriate model for \\(D(Y|X)\\) like beta regression. This feature of the distribution is, in other words, what Prof. Lei sees as the big feature of FL.\nI can see the appeal of this trait of the model, but it is important to note that in statistics as in all of life, there is no such thing as a free lunch. The so-called bias-variance trade-off ensures that is almost never going to happen. I see the drawbacks as the following:\n\nWe have limited insight into how the standard error correction works. Papke and Wooldridge’s claim is that it is consistent, which would only hold across repeated samples, or what is known as asymptotically. We don’t know how well it would work in finite samples (as in the data we actually have), especially in terms of efficiency loss or how much we have to over-estimate uncertainty to avoid false positives. It’s hard to test this as we only have a set number of possibilities that we are aware of for \\(D(Y|X)\\), and how exactly FL does compared to each \\(D(Y|X)\\) may vary (i.e. beta regression versus continuous Bernoulli versus zero-one-inflated beta versus some model we aren’t even aware of yet).\nWe have a consistent estimate for \\(E(Y|X)\\), but this applies only to the regression coefficients and the uncertainty of those coefficients. If we wanted to calculate some non-linear combination of coefficients, we would have to adjust that uncertainty as well. We might not be able to use bootstrapping given that the sampling model is “wrong” (though I didn’t see any discussion of this in the paper). As such, it will be more tricky to use the estimates beyond making inferences, i.e. sign + significance.\n\nTo give an example, we cannot make model predictions that include 0s and 1s. The logit function only produces numbers strictly between 0 and 1. This makes sense if you think about it—how could the outcome on average be equal to exactly 1 or 0? The probability of any particular value of a continuous random variable is always 0. If we sample from our estimate \\(\\widehat{E(Y|X)}\\), we can only get draws for averages, not for individual variates. In other words, we might know that the average probability that white-collar workers turn out to vote in Minnesota is 33%, but we won’t know if that’s because the probability is close to 0 for most Minnesotans and close to 1 for a few or if the majority of Minnesotans have a roughly 20 - 40% chance of turning out to vote. Both of these distributions would have the same expected or average value of turning out to vote.\n\n\nI am particularly concerned about #1. The standard error correction could be huge; there is nothing that constrains the variance of the estimates. In my paper on ordered beta regression, I simulated data and compared fractional logit with beta regression, OLS, and other alternatives, and its performance was hard to predict. On the one hand, it was closer to the correct value for \\(E(Y|X)\\) than OLS, which would fit with Papke and Wooldridge’s arguments. The estimates were also over-confident in terms of uncertainty, which again matches their work given that I did not adjust the confidence intervals (it is not clear how that would exactly be done in a Bayesian framework). But, in the empirical example, the FL estimates had uncertainty that was a couple of orders of magnitude greater than any of the other models (i.e., really wide CIs).\nThe question about the utility of FL seems to depend on how much we should be concerned about the possibility of mis-specification (we have the wrong distribution for \\(Y\\)) as opposed to inflating our uncertainty unnecessarily (and restricting what we can use our models for). Again, very similar to the bias-variance tradeoff."
  },
  {
    "objectID": "post/frac_logit/index.html#what-are-the-alternatives",
    "href": "post/frac_logit/index.html#what-are-the-alternatives",
    "title": "Fixing Fractional Logit?",
    "section": "What Are the Alternatives?",
    "text": "What Are the Alternatives?\nOne way of framing this is what are the possibilities for \\(D(Y|X)\\) and how divergent are they? This is where it gets somewhat difficult as \\(Y\\) is a bounded outcome, and so, at least in my opinion, our alternative models seem somewhat limited.\nThe most direct comparison, and the specification that prompted this paper, is what has become known as the continuous Bernoulli distribution, which was derived by people working in the machine learning literature. Gabriel Loaiza-Ganem and John Cunningham looked at variational auto-encoders, which are models for pixels that make up images. Apparently people using these models had been employing something like fractional logit (for prediction, not inference), and they were likewise concerned about the fact that this specification was not a true statistical distribution. They went as far as identifying what the normalization constant is required to add to the fractional logit model to make it a true distribution, which turns out to be the following for a given value of the linear predictor \\(g(X\\beta)\\), which I denote as \\(\\mu\\) (see their eqn (7)):\n\\[\nC(\\mu) = \\begin{cases}\n  \\frac{2 \\text{tanh}^{-1}(1 - 2\\mu)}{1 - 2\\mu}  & \\text{if } \\mu \\ne 0.5 \\\\\n  2 & \\text{otherwise}\n\\end{cases}\n\\]\nThis thing, to be honest, is kind of ugly, and has a fixed point at 0.5 or 50%, meaning that there is a point where the value of the outcome and the combined value of the covariates must be equal to 0.5. The authors decided to name this distribution, which could be thought of as fractional logit with the minimal number of changes to make it a full distribution, as the continuous Bernoulli distribution. I am cutting and pasting their Figure 1 to show what the distribution looks like:\n\n\n\nFigure 1 from Loaiza-Ganem and Cunningham (2019)\n\n\nEssentially, the distribution allows for mildly sloping lines across 0 to 1 that can be either upward or downward. The value of the normalizing constant \\(C(\\mu)\\) in the leftward panel is much bigger towards the boundaries of the distribution, which intuitively makes sense. As the distribution moves towards extreme values, the denominator has to change to take into account the non-linearity in the outcome.\nFurthermore, they show in the paper that what they call the lack of normalization has a clear impact on performance. They use an image-learning task and examine how continuous Bernoulli without normalization (i.e., vanilla fractional logit) compares to continuous Bernoulli with normalization. Again, I’ll copy and paste their key result here:\n\nIn this figure, CB represents continuous Bernoulli and B is the fractional logit. As can be seen, the images are much sharper with continuous Bernoulli (normalized) than fractional logit. The authors point out that this is likely due to the normalizing constant becoming so large towards the extremes of the distribution: the un-normalized distribution has a hard time knowing where the boundaries are.\nThis is an intriguing result, though it is important to note that the original FL model says almost nothing about predictive validity, but rather about making inferences about regression coefficients. Machine learning has a different set of goalposts, so it might not be surprising that FL just doesn’t cut it. In either case, we do end up with a new distribution for \\(Y\\) that very close to FL. I wrote this blog post in part to highlight this new model. Whether CB is a true alternative or fix for FL is difficult to say as FL estimates are supposed to average across all possible distributions (for good or ill). However, what can be said is that it appears to the specification that makes the minimum number of necessary statements about the distribution of \\(Y\\) while providing an estimate for the average value \\(E(Y|X)\\) as well."
  },
  {
    "objectID": "post/frac_logit/index.html#example",
    "href": "post/frac_logit/index.html#example",
    "title": "Fixing Fractional Logit?",
    "section": "Example",
    "text": "Example\nThis distribution is not available currently in R, though it can be implemented fairly straightforwardly in Stan. It is also available in tensorflow in Python, but as I’m not primarily a Python user, I’ll stick with R. I produce code below that can fit this model with the R package brms as a custom family, and in the future I plan to add support for it to ordbetareg. I still think ordered beta regression makes more sense as a default, especially with the issues with the normalizing constant in the continuous Bernoulli, but it is great to have this model as another robust alternative for bounded continuous variables.\nTo demonstrate how to fit continuous Bernoulli, I first generate data using the rordbeta function in ordbetareg that will create proportion data from 0 to 1 inclusive. I’ll add a covariate X to predict the mean of the distribution on the logit scale (which continuous Bernoulli also uses) with a coefficient of 2.5:\n\nlibrary(ordbetareg)\n\nN &lt;- 500\nX &lt;- runif(n=N)\nY &lt;- rordbeta(n=N, mu = plogis(-2 + 2.5*X),cutpoints=c(-3,3))\n\nhist(Y)\n\n\n\n\n\n\n\n\nThe outcome has a distinct U-shape and 70 discrete responses (0 or 1).\nThe code below defines the custom family for brms to work (the model will soon be available in my package ordbetareg):\n\nc_bernoulli &lt;- custom_family(\"c_bernoulli\",\n                             dpars=\"mu\",\n                             links=\"logit\",\n                             lb=0,ub=1,\n                             type=\"real\")\n\n# define log density function\n# some code from spinkey https://discourse.mc-stan.org/t/continuous-bernoulli/26886\n\nstan_funs &lt;- \"\n\n  //normalization constant\n  real c_norm(real mu) {\n  \n    if(mu==0.5) {\n    \n      return log(2);\n        \n    } else {\n    \n      real const = (log(2 - 2*mu) - log(2*mu))/(2 * (1 - 2*mu));\n      return(log(const));\n                \n    }\n  \n  }\n  \n  // log PDF for continuous Bernoulli\n  real c_bernoulli_lpdf(real y, real mu) {\n  \n    // unnormalized density\n    \n    real lp = y * log(mu) + (1 - y) * log1m(mu);\n    \n    // normalized density\n   \n    lp += c_norm(mu);\n      \n    return lp;\n    \n  }\"\n\nstanvars &lt;- stanvar(scode = stan_funs, block = \"functions\")\n\n# posterior predictions\n\nposterior_predict_c_bernoulli &lt;- function(i, prep, ...) {\n  \n  # need inverse CDF function for continuous bernoulli\n  \n  inv_cdf &lt;- function(i=NULL,mu=NULL,u=NULL) {\n    \n    mu &lt;- mu[i]\n    u &lt;- u[i]\n    \n    if(mu==0.5) {\n      \n      out &lt;- u\n      \n    } else {\n      \n      out &lt;- (log(u * (2 * mu - 1) + 1 - mu) - log(1 - mu))/(log(mu) - (log(1-mu)))\n      \n    }\n    \n    return(out)\n    \n  }\n  \n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  u &lt;- runif(n=length(mu))\n\n  sapply(1:length(mu),inv_cdf,mu,u)\n  \n}\n\nposterior_epred_c_bernoulli &lt;- function(prep) {\n  \n  # expected value\n  mu &lt;- brms::get_dpar(prep, \"mu\")\n  \n  t(apply(mu, 1, function(r) {\n    \n    sapply(r, function(mu_i) {\n      \n      if(mu_i==0.5) {\n      \n      return(0.5)\n      \n    } else {\n      \n      (mu_i / (2 * mu_i - 1)) + (1 / (2*atanh(1 - 2*mu_i)))\n      \n      }\n      \n    })\n      \n  }))\n  \n  \n}\n\nfit_c_bern &lt;- brm(\n  Y ~ X, data = tibble(Y=Y, X=X),\n  family = c_bernoulli, stanvars = stanvars, backend=\"cmdstanr\",\n  refresh=0\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 0.9 seconds.\nChain 2 finished in 0.9 seconds.\nChain 3 finished in 1.0 seconds.\nChain 4 finished in 1.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 1.0 seconds.\nTotal execution time: 4.1 seconds.\n\nlibrary(marginaleffects)\n\navg_slopes(fit_c_bern)\n\n\n Term Estimate 2.5 % 97.5 %\n    X     0.51 0.443  0.568\n\nColumns: term, estimate, conf.low, conf.high \n\n\nThe avg_slopes function from the marginaleffects package gives the marginal effect of the parameter back-transformed to 0/1. The continuous Bernoulli estimates a very big marginal effect for X (it is not the same as the true coefficient because that was on the logit scale, not the true scale). We can plot the posterior predictive distribution:\n\npp_check(fit_c_bern)\n\n\n\n\n\n\n\n\nIt’s a bit off, but pretty close, and we probably shouldn’t expect it to be perfect given that the data was generated from the ordered beta distribution, not the continuous Bernoulli.\nWe can then compare those results to the ordered beta regression in ordbetareg:\n\nfit_ordbeta &lt;- ordbetareg(\n  Y ~ X, data = tibble(Y=Y, X=X),\n  backend=\"cmdstanr\",\n  refresh=0\n)\n\nRunning MCMC with 4 sequential chains...\n\nChain 1 finished in 2.3 seconds.\nChain 2 finished in 2.4 seconds.\nChain 3 finished in 2.4 seconds.\nChain 4 finished in 2.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.4 seconds.\nTotal execution time: 9.7 seconds.\n\navg_slopes(fit_ordbeta)\n\n\n Term Estimate 2.5 % 97.5 %\n    X    0.489 0.409  0.561\n\nColumns: term, estimate, conf.low, conf.high \n\npp_check(fit_ordbeta)\n\n\n\n\n\n\n\n\nThe marginal effect of X is fairly close to the continuous Bernoulli’s, and the posterior distribution is much closer to the true distribution. But again, we generated the data from the ordered beta distribution so it’s not surprising that the continuous Bernoulli estimate is different. It might take some time to figure out exactly where and when the distributions will differ. For this relatively simple example, the marginal effects of both distributions are fairly close."
  },
  {
    "objectID": "post/frac_logit/index.html#so-what-do-we-do",
    "href": "post/frac_logit/index.html#so-what-do-we-do",
    "title": "Fixing Fractional Logit?",
    "section": "So What Do We Do?",
    "text": "So What Do We Do?\nI think there are still some unanswered questions—which of these distributions will FL’s estimates match most closely? Notably, both CB and ordered beta have a defined expected value for \\(Y\\) that is very close to the logit function plus covariates, i.e. \\(g(X\\beta)\\). CB and FL are probably not going to produce estimates that are very different–the main difference is that CB’s estimates will be valid based on the uncertainty of the distribution, while FL’s estimates will be over-confident without using standard error corrections. Because ordered beta regression is making a much more nuanced statement about the distribution for \\(Y\\), its estimates of uncertainty could diverge even further from FL’s in finite samples.\nHowever, the main criteria we would want to know for FL is hard to define–how concerned are we about getting the wrong model? CB is an intriguing new model, but it is quite simple, and it’s not clear when it would be preferred over something more complicated yet also more robust like beta regression. After all, CB has the strange property of fixing the expected value of the outcome at 0.5. All models have uses, and I am sure this one does, but it would seem to best apply in situations where the limited possibilities for uncertainty match the research question closely.\nThat is why, in my opinion, FL makes the most sense when we have a lot of data. As the amount of data increases, confidence intervals will shrink regardless of the inflation factor we use. Furthermore, nuances in our estimation of uncertainty may be less noticeable as uncertainty disappears. FL has a noted advantage in that it is the easiest model to estimate–it is quite literally a standard Bernoulli (logit) model. CB is close but certainly has additional complications, while the beta regression has a lot more computation necessary. If computation is a limitation, FL strikes me as a useful alternative to OLS as it will produce estimates that are valid for a bounded dependent variable.\nI do think that further research would be useful to clarify these questions, especially comparing FL’s uncertainty under different distributions \\(D(Y|X)\\). These would be helpful for knowing when mis-specification of \\(D(Y|X)\\) is a real problem and when we might want to use something like FL that inflates uncertainty. My sense that beta regression (standard or ordered beta or zero-inflated beta) is still the best generic model for bounded continuous data as it fits these outcomes well and is a full-featured statistical distribution."
  },
  {
    "objectID": "post/frac_logit/index.html#footnotes",
    "href": "post/frac_logit/index.html#footnotes",
    "title": "Fixing Fractional Logit?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWith some positive probability, yes.↩︎\nWith this statement I am slipping dangerously close to Bayesian inference, but well, probably a good thing if I do :).↩︎"
  },
  {
    "objectID": "post/vaccinepval/index.html",
    "href": "post/vaccinepval/index.html",
    "title": "A More Realistic P-Value for the Pfizer Vaccine Report",
    "section": "",
    "text": "I start with Eric Novik’s excellent blog post on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:\n\\[\nVE = 1 - \\frac{p_t}{p_c}\n\\]\nWhere \\(p_t\\) is the proportion of cases in the treatment (vaccinated) group with COVID-19 and \\(p_c\\) is the proportion of cases in the control (un-vaccinated) group). This is kind of an odd statistic as it would seem to make more sense to simply report the ratio rather than one minus the ratio, although subtracting one means a null ratio (difference of 1) equals zero. I am not sure a general audience is very aware of the distinction and what VE in fact means. In essence, if we assume the vaccinated group will have no more cases than the control group, this statistic will converge to 1 as \\(p_t\\) goes to zero, so VE of 100% would be a case in which there are no cases in the treatment group.\nAs Eric Novik shows, the VE is actually a calculated quantity and isn’t modeled directly. The clinical pre-registration says they will use a beta-binomial model. The put a prior on a transformation of the VE (vaccine efficacy) of \\(Beta(0.700102, 1)\\). We can get an idea of what that looks like by examining the distribution of these values:\n\nprior_VE &lt;- rbeta(10000,.700102,1)\n\nround(quantile((2*prior_VE - 1)/(prior_VE - 1),probs=seq(0,1,by=.05)),5)\n\n         0%          5%         10%         15%         20%         25% \n-4994.45563   -12.93514    -5.62962    -3.11045    -1.86012    -1.15668 \n        30%         35%         40%         45%         50%         55% \n   -0.63120    -0.28035    -0.00591     0.19966     0.36281     0.50447 \n        60%         65%         70%         75%         80%         85% \n    0.61128     0.69638     0.76767     0.83150     0.88354     0.92454 \n        90%         95%        100% \n    0.95870     0.98584     1.00000 \n\n\nThis prior puts significant mass at negative VEs which would suggest that the vaccine actually makes things worse for the treatment group than for the control group. While this would seem unrealistic, it also suggests that very high VEs, such as above 90%, are relatively unlikely. We can see that the prior suggests it is about as likely that the VE is negative (the vaccine causes the virus) as it is likely that the VE is greater than 80%.\nGiven this conservative but weakly informative prior, we can then calculate a p-value for what they pre-register in their study, which is the probability that the VE (vaccine efficacy) is greater than 30%. We can then fit a Bayesian beta-binomial model with these priors by modifying Eric Novik’s original code:\n\n# from Eric Novik, changed the prior per pre-reg\n\nvac_model &lt;- \"data {\n  int&lt;lower=1&gt; r_c; // num events, control\n  int&lt;lower=1&gt; r_t; // num events, treatment\n  int&lt;lower=1&gt; n_c; // num cases, control\n  int&lt;lower=1&gt; n_t; // num cases, treatment\n  array[2] real a;   // prior values for treatment effect\n  \n}\nparameters {\n  real&lt;lower=0, upper=1&gt; p_c; // binomial p for control\n  real&lt;lower=0, upper=1&gt; p_t; // binomial p for treatment \n}\ntransformed parameters {\n  real VE       = 1 - p_t / p_c;  // vaccine effectiveness\n}\nmodel {\n  (VE - 1)/(VE - 2) ~ beta(a[1], a[2]); // prior for treatment effect\n  r_c ~ binomial(n_c, p_c); // likelihood for control\n  r_t ~ binomial(n_t, p_t); // likelihood for treatment\n}\ngenerated quantities {\n  real effect   = p_t - p_c;      // treatment effect\n  real log_odds = log(p_t / (1 - p_t)) -\n                  log(p_c / (1 - p_c));\n}\"\n\nto_stan &lt;- cmdstan_model(write_stan_file(vac_model))\n\nn &lt;- 4.4e4  # number of volunteers\nr_c &lt;- 162  # number of events in control\nr_t &lt;- 8    # number of events in vaccine group\n\nstan_data &lt;- list(n=n,r_c=r_c,r_t=r_t,\n                  n_c=n/2,n_t=n/2,\n                  a=c(.700102,1))\n\nThen we can sample and plot the results. We’ll draw a lot of samples so we can get good estimates in the tails (i.e. very low p-values).\n\npfizer_est &lt;- to_stan$sample(data=stan_data,chains=1,iter_warmup=500,iter_sampling=100000,refresh=50000,show_messages=F)\n\ndraws &lt;- pfizer_est$draws() %&gt;% as_draws_df\n\nHere’s a plot of the VE as reported in the press release with the dotted line as the average estimate:\n\nmean_ve &lt;- mean(draws$VE)\n\ndraws %&gt;% \n  ggplot(aes(x=VE)) +\n  geom_density(fill=\"blue\",alpha=0.5)+\n  theme_tufte() +\n  ylab(\"\") +\n  scale_x_continuous(labels=scales::percent_format(accuracy = 1)) +\n  geom_vline(aes(xintercept=mean(VE)),linetype=2) +\n  annotate(\"text\",x=mean_ve,y=12,label=paste0(\"  VE = \",round(mean_ve*100,1),\"%\"))\n\n\n\n\n\n\n\n\nThis estimate matches the press release. So let’s calculate the p-value:\n\nmean(1 - as.numeric(draws$VE&gt;.3))\n\n[1] 0\n\n\nVirtually nil.\nHowever, we can also test some other thresholds, such as the FDA value of 50% VE:\n\nmean(1 - as.numeric(draws$VE&gt;.5))\n\n[1] 0\n\n\nStill vanishingly small. We can try something a bit closer, such as whether VE exceeds 90%:\n\nmean(1 - as.numeric(draws$VE&gt;.9))\n\n[1] 0.01883\n\n\nThis p-value is at least reportable at 0.02. As this is a Bayesian model, this can be interpreted directly that the probability that the vaccine efficacy is less than 90% is quite small, less than a 2% chance."
  },
  {
    "objectID": "post/energy_nationalism/index.html",
    "href": "post/energy_nationalism/index.html",
    "title": "Energy Nationalism To the Rescue",
    "section": "",
    "text": "The Russian invasion of the Ukraine has raised a new specter of energy nationalism. Although only the United Kingdom and the United States have gone as far as outright banning Russian oil and gas, other European economies, including Germany, are seriously discussing weaning off of their main energy supplier. This crisis offers politicians an important opportunity to reframe climate change policy by arguing for reduced dependence on foreign oil and pivoting to domestic renewable energy. My research into trade and business politics shows that building trade barriers against oil could cement a new and durable coalition in favor of higher gas prices uniting oil companies, conservationists and realpolitikers.\nThe idea of using trade measures to fight climate change is not new, though oddly enough, rarely implemented. European countries have proposed various sorts of carbon-weighted tariffs, and the World Trade Organization and the World Bank have long discussed liberalizing trade in environmental goods like solar panels. Yet climate change negotiations often take place in isolation from trade deals, leaving the two areas of policy largely unaffected by each other.\nA trade policy that bans foreign energy imports is a potential gold mine for climate action because a ban obscures who is responsible for the cost of higher fuel prices. Voters have proven to be averse to any policies that directly increase the cost of carbon pollution, such as Washington State’s carbon tax ballot initiative failure in 2018. As Matto Mildenberger has shown, it has been very difficult to push through pro-climate policies because they fracture coalitions on both sides of the aisle, bringing together erstwhile enemies like unions and corporations. A ban on foreign energy, on the other hand, seems like an “America first” policy that privileges the domestic economy at the expense of foreigners—even though it will result in higher gas prices in both the short and long term.\nIn the short term, bans of energy imports lead to a surge in higher oil and gas prices as suppliers react to changes in the market. Over the long term, a ban on foreign energy will have the net effect of any tariff: it will permanently raise the domestic price of the good, in this case oil and gas. For the United States, both renewable energy and oil companies stand to benefit, though oil companies will reap the lion’s share of the gains as consumers pay more at the pump. We know that higher gas prices both decrease energy usage and push car buyers towards electric vehicles, ultimately reducing carbon consumption for decades.\nBy favoring domestic energy suppliers, the U.S. can act tough on authoritarian regimes like Russia, raise the price of gasoline to reduce carbon pollution, and secure the support of oil companies who will profit. In other words, we could trap oil companies like the monkey in Aesop’s fable, its hand around a nut inside a jar that it simply will not let go of.\nWe know from recent trade wars that it won’t be easy for businesses hurt by higher fuel prices to mobilize together to defeat energy bans. As part of my research with the Princeton Trade Study on President Trump’s trade war with China, we implemented an online survey experiment of U.S. businesspeople in which we randomly provided some managers with information about how the trade war had affected their industry. We found that conservative-leaning managers thought the trade war helped their firm—even if our data showed it had not–while liberals believed the opposite. The complexities of the impact of tariffs on supply chains meant that businesses found it difficult to know who to blame for rising costs and whether to take politically risky action to oppose the trade war.\nEnergy bans are of course only one part of larger climate reform. But the power of trade restrictions to reshape economies in ways that are politically expedient shows an important way forward for feasible and immediate climate action."
  },
  {
    "objectID": "post/tunisia_polls/index.html",
    "href": "post/tunisia_polls/index.html",
    "title": "Simulating Turnout in Tunisia’s Constitutional Referendum",
    "section": "",
    "text": "I am writing this post in response to questions about estimating turnout for Tunisia’s constitutional referendum today. Turnout is an important aspect to this referendum because high turnout would signal higher legitimacy for President Kais Saied’s dramatic changes to the Tunisia’s democracy. His proposed constitution would replace Tunisia’s democratic institutions with a new dictatorship that would concentrate power in the president’s hands.\nBecause of Saied’s interference in the election commission, we are much less sure about the accuracy of results from the official election commission, the ISIE. Saied has also banned foreign election observers from arriving, leaving only one local NGO, Mourakiboun, observing polling stations. Mourakiboun has done significant work in prior elections, but reportedly faces a shortfall of local participants because of low interest in the referendum.\nMourakiboun uses a method known as parallel vote tabulation (PVT) to estimate turnout. It is a method used by NGOs like NDI to provide an independent estimate of turnout. According to NDI, to estimate turnout through observers without bias, Mourakiboun has to select a random sample of polling stations and then accurately record all votes from these polling stations. I will use some code-based simulations to see how well this method will work depending on levels of turnout. If turnout is low, there is likely to be much more sampling uncertainty because there are fewer voters at polling stations to base estimates on. I will also examine what happens if Mourakiboun does not select polling stations at random, which is likely to happen for logistical reasons. For example, Mourakiboun may put more observers at stations with higher turnout in order to save volunteer hours.\nThe results of the simulation show that PVT/Mourakiboun turnout estimates are probably going to be less precise at low levels of turnout compared to high levels of turnout. In addition, when turnout is low and selection of polling stations is somewhat biased, such as selecting higher-turnout polling stations, the inaccuracies in the estimates get much worse. In other words, if Mourakiboun is using an imperfect methodology and turnout is low, their estimates will be farther off than if turnout in the referendum was high. If 50% of Tunisians voted, this wouldn’t be as much of a problem, but if 10% vote, it will be much harder to get an accurate estimate using PVT via polling station counts.\nWithout digging into the methods below, the results can be summarized as:\nIf turnout is large, these biases have a much smaller role, but when turnout is small, as is likely to be the case with the constitutional referendum, the biases can have a much bigger impact on the final estimates, making Mourakiboun’s methods more likely to fail."
  },
  {
    "objectID": "post/tunisia_polls/index.html#simulation",
    "href": "post/tunisia_polls/index.html#simulation",
    "title": "Simulating Turnout in Tunisia’s Constitutional Referendum",
    "section": "Simulation",
    "text": "Simulation\nFirst, we randomly create polling stations roughly corresponding to Tunisia’s population of 7 million registered voters and 4,500 polling stations:\n\nN &lt;- 8900000\n\nstations &lt;- 11000\n\nvote_assign &lt;- sample(1:stations,N,replace=T,\n                      prob=sample(1:3,stations,replace=T))\n\nsample_size &lt;- 1000\n\nWhat we will do is test how accurate samples will be depending on total turnout and the quality of samples. To do so, I’ll first vary true turnout from 5% up to 50% of the population. We’ll assume as well that turnout varies by polling station, which is much more accurate than assuming a constant rate of turnout for all polling stations.\nWe’ll assume that the stations can vary in size up to a 3-fold magnitude, i.e., some stations may be 3 times smaller than other stations. For our sample, we have a polling station as large as 1329 and one as small as 331. We’ll then assume, for now, that Mourakiboun selects a random sample of 50 polling stations and it records all votes at these stations. We’ll repeat this experiment 1,000 times and record what estimated turnout Mourakiboun would report and how that compares to real turnout:\n\n# loop over turnout, sample polls, estimate turnout\n\nover_turnout &lt;- parallel::mclapply(seq(.05,.5,by=.1), function(t) {\n  \n    # polling station varying turnout rates\n  \n    station_rates &lt;- rbeta(n=N,t*20,(1-t)*20)\n          \n    # randomly let voters decide to vote depending on true station-level turnout rate\n    \n    pop_turnout &lt;- lapply(1:stations, function(s) {\n                    tibble(turnout=rbinom(n=sum(vote_assign==s), size=1,prob = \n                                            station_rates[s]),\n                           station=s)\n                    }) %&gt;% bind_rows\n  \n    over_samples &lt;- lapply(1:1000, function(i) {\n      \n        # sample 100 random polling stations 1,000 times\n        sample_station &lt;- sample(1:stations, size=sample_size)\n        \n        turn_est &lt;- mean(pop_turnout$turnout[pop_turnout$station %in% sample_station])\n        \n        return(tibble(mean_est=turn_est,\n                      experiment=i))\n      \n    }) %&gt;% bind_rows %&gt;% \n      mutate(Turnout=t)\n    \n    over_samples\n  \n    },mc.cores=10) %&gt;% \n  bind_rows\n\nWe can now plot estimated versus actual turnout. If we have random samples and no problems with recording votes, this works fairly well. The dot shows the average of all samples, which is un-biased, and the vertical line shows where 95% of the samples fall, which is an estimate of potential sampling error. In other words, if Mourakiboun does everything right with a sample of 50 polling stations, they would expect this kind of error from random chance alone. If true turnout is 15%, they could see estimates that range from 12% to 17%.\n\nover_turnout %&gt;% \n  group_by(Turnout) %&gt;% \n  summarize(pop_est=mean(mean_est),\n            low_est=quantile(mean_est,.05),\n            high_est=quantile(mean_est, .95)) %&gt;% \n  ggplot(aes(y=pop_est,x=Turnout)) +\n  geom_pointrange(aes(ymin=low_est,\n                      ymax=high_est),size=.5,fatten=1) +\n  geom_abline(slope=1,intercept=0,linetype=2,colour=\"red\") +\n  theme_tufte() +\n  theme(text=element_text(family=\"\")) +\n  labs(y=\"Estimated Turnout\",x=\"True Turnout\",\n       caption=stringr::str_wrap(\"Comparison of Mourakiboun estimated (y axis) versus actual turnout (x axis). Red line shows where true and estimated values are equal. Based on random samples of 50 polling stations and assuming no problems with recording votes.\"))\n\n\n\n\n\n\n\n\nUnfortunately, this kind of accuracy only happens in a computer simulation. It is quite tricky to do a true random sample of polling stations because there may not be volunteers to cover all of the polling stations, as is the case with the current referendum. Let’s assume in the following simulation then that a polling station is more likely to be sampled if the true level of turnout is higher:\n\n# loop over turnout, sample polls, estimate turnout\n\nover_turnout_biased &lt;- parallel::mclapply(seq(.05,.5,by=.1), function(t) {\n  \n    # polling station varying turnout rates\n  \n    station_rates &lt;- rbeta(n=stations,t*20,(1-t)*20)\n          \n    # randomly let voters decide to vote depending on true station-level turnout rate\n    \n    pop_turnout &lt;- lapply(1:stations, function(s) {\n                    tibble(turnout=rbinom(n=sum(vote_assign==s), size=1,prob = \n                                            station_rates[s]),\n                           station=s)\n                    }) %&gt;% bind_rows\n  \n    over_samples &lt;- lapply(1:1000, function(i) {\n      \n        # sample 100 random polling stations 1,000 times\n        sample_station &lt;- sample(1:stations, size=50,prob=station_rates)\n        \n        turn_est &lt;- mean(pop_turnout$turnout[pop_turnout$station %in% sample_station])\n        \n        return(tibble(mean_est=turn_est,\n                      experiment=i))\n      \n    }) %&gt;% bind_rows %&gt;% \n      mutate(Turnout=t)\n    \n    over_samples\n  \n    },mc.cores=10) %&gt;% \n  bind_rows\n\nWe can now plot estimated versus actual turnout for this simulation where the polling stations were more likely to be sampled if they had higher turnout:\n\nover_turnout_biased %&gt;% \n  group_by(Turnout) %&gt;% \n  summarize(pop_est=mean(mean_est),\n            low_est=quantile(mean_est,.05),\n            high_est=quantile(mean_est, .95)) %&gt;% \n  ggplot(aes(y=pop_est,x=Turnout)) +\n  geom_pointrange(aes(ymin=low_est,\n                      ymax=high_est),size=.5,fatten=1) +\n  geom_abline(slope=1,intercept=0,linetype=2,colour=\"red\") +\n  theme_tufte() +\n  theme(text=element_text(family=\"\")) +\n  labs(y=\"Estimated Turnout\",x=\"True Turnout\",\n       caption=stringr::str_wrap(\"Comparison of Mourakiboun estimated (y axis) versus actual turnout (x axis). Red line shows where true and estimated values are equal. Based on biased samples of 50 polling stations with higher turnout stations more likely to be sampled. However, simulation assumes no problems with recording votes.\")) +\n  ylim(c(0,0.5)) +\n  xlim(c(0,0.5))\n\n\n\n\n\n\n\n\nWe see in the plot above that the black dot estimates are always higher than the red line showing the true values. Furthermore, the bias appears worse when turnout is smaller. The bias is also substantial - at low levels of turnout, such as at 5%, the estimated turnout can be twice as high.\nAs can be seen in this simulation study, it is quite possible for the method to work, but only if the polling stations are selected at random. I did not look into what would happen with other possible errors, such as being unable to accurately record all votes from a given polling station. For these reasons, while this method can certainly work, it is necessary to confirm what methodology was used to select the polling stations and also how strictly it was followed in implementation. There are many points at which this type of analysis could either intentionally or unintentionally result in over/under reporting of turnout.\nIf turnout is large, some of these biases have a minimal effect, but when turnout is small, as is likely to be the case with the constitutional referendum, they can have a much bigger impact on the final estimates."
  },
  {
    "objectID": "post/conjoint/conjoint.html",
    "href": "post/conjoint/conjoint.html",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "",
    "text": "Conjoint survey experiments have become more popular in political science since the publication of Hainmueller, Hopkins and Yamamoto (2014). However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see SocArchiv Draft). I employ both traditional power measures and newer statistics from Gelman and Carlin (2014) reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).\nThe original Rmarkdown and saved simulation files can be downloaded from the site’s Github.\nThe packages required to run this simulation are listed in the code block below:\n\n#Required packages\nrequire(ggplot2)\nrequire(dplyr)\nrequire(tidyr)\nrequire(multiwayvcov)\nrequire(lmtest)\nrequire(stringr)\nrequire(kableExtra)\n\n# package MASS also used but not loaded\n\n# != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows.\n# to install parallelsugar:\n# install.packages('devtools')\n# library(devtools)\n# install_github('nathanvan/parallelsugar')\n\n# If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used\n\nif(.Platform$OS.type=='windows') {\n  parallelfunc &lt;- parallelsugar::mclapply_socket\n} else {\n  parallelfunc &lt;- parallel::mclapply\n}"
  },
  {
    "objectID": "post/conjoint/conjoint.html#background",
    "href": "post/conjoint/conjoint.html#background",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "",
    "text": "Conjoint survey experiments have become more popular in political science since the publication of Hainmueller, Hopkins and Yamamoto (2014). However, analysis of the statistical of power of conjoint experiments is difficult using standard parametric techniques because of the use of multiple treatments, interaction effects and paired vignettes. To that end, I have conducted the following simulation experiment to demonstrate the statistical properties of the conjoint experiment for my online survey experiment “Politically-Connected Firms and the Military-Clientelist Complex in North Africa” (see SocArchiv Draft). I employ both traditional power measures and newer statistics from Gelman and Carlin (2014) reflecting inferential errors that are particularly apt for experiments in the social sciences.This simulation also incorporates measurement error in the treatment variable by using a hierarchical distribution for the conjoint treatment effects (i.e., heterogeneous treatments).\nThe original Rmarkdown and saved simulation files can be downloaded from the site’s Github.\nThe packages required to run this simulation are listed in the code block below:\n\n#Required packages\nrequire(ggplot2)\nrequire(dplyr)\nrequire(tidyr)\nrequire(multiwayvcov)\nrequire(lmtest)\nrequire(stringr)\nrequire(kableExtra)\n\n# package MASS also used but not loaded\n\n# != Note this simulation uses a version of mclapply for windows. You must have R package parallelsugar installed to use it if you are running windows.\n# to install parallelsugar:\n# install.packages('devtools')\n# library(devtools)\n# install_github('nathanvan/parallelsugar')\n\n# If using Windows, parallelfunc comes from parallesugar, otherwise the standard mclapply is used\n\nif(.Platform$OS.type=='windows') {\n  parallelfunc &lt;- parallelsugar::mclapply_socket\n} else {\n  parallelfunc &lt;- parallel::mclapply\n}"
  },
  {
    "objectID": "post/conjoint/conjoint.html#simulation-set-up",
    "href": "post/conjoint/conjoint.html#simulation-set-up",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "Simulation Set-up",
    "text": "Simulation Set-up\nThe following parameters control the range of coefficients tested and the number of simulations. The survey experiment design employs vignettes in which appeals and the actors making appeals are allowed to vary between respondents. Any one vignette has one actor and one appeal. The probability of assignment is assumed to be a simple random fraction of the number of appeal-actor combinations (14). If run_sim is set to TRUE, the simulation is run, otherwise the simulation results are loaded from an RDS file and plotted. Running the simulation will take approximately 6 to 12 hours depending on the number of cores and speed of the CPU.\n\n#Actually run the simulation or just load the data and look at it?\nrun_sim &lt;- FALSE\n# Max number of respondents fixed at 2700\nnum_resp &lt;- 2700\n# Number of iterations (breaks in sample size)\nnum_breaks &lt;- 300\n# Number of simulations to run per iteration\nn_sims &lt;- 1000\n\nI then create a grid of all possible actor-appeal combinations as I am using simple randomization of profiles before presenting them to respondents. There are two vectors of treatments (actors and appeals) that each have 7 separate treatments for a total of 14 separate possible treatments.\n\n# Two treatment variables producing a cross-product of 7x7\ntreatments1 &lt;- c('military','MOI','president','MOJ','parliament','municipality','government')\ntreatments2 &lt;- c('exprop.firm','exprop.income','permit.reg','contracts.supply','permit.export','permit.import','reforms')\ntotal_treat &lt;- length(c(treatments1,treatments2))\ngrid_pair &lt;- as.matrix(expand.grid(treatments1,treatments2))\nprint(head(grid_pair))\n\n     Var1           Var2         \n[1,] \"military\"     \"exprop.firm\"\n[2,] \"MOI\"          \"exprop.firm\"\n[3,] \"president\"    \"exprop.firm\"\n[4,] \"MOJ\"          \"exprop.firm\"\n[5,] \"parliament\"   \"exprop.firm\"\n[6,] \"municipality\" \"exprop.firm\""
  },
  {
    "objectID": "post/conjoint/conjoint.html#simulation",
    "href": "post/conjoint/conjoint.html#simulation",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "Simulation",
    "text": "Simulation\nTo simulate the data, I first sample 14 coefficients \\(\\beta_j\\) (one for each treatment \\(J\\)) from a normal distribution with mean zero and standard deviation one. I then randomly sample from two profile combinations for each of the \\(I\\) respondents in accordance with simple random sampling. Two profile combinations, for a total of four tasks \\(T\\), are selected to reflect the fact that paired vignettes will be shown to each respondent as in the study design. I also sample a pre-treatment covariate \\(Z_I\\) that is a random binomial vector with probability of 0.2 (thus 20% of respondents will fall into this cell). A treatment interaction effect \\(\\beta_z\\) is sampled from a normal distribution with mean 0.5 and standard deviation of 0.3 to provide a sampling distribution for the true effect, instead of assuming that the true effect is a fixed population value. Adding a distribution for \\(\\beta_j\\) reflects additional uncertainty beyond standard sampling distribution uncertainty. In this case, it represents additional measurement error between the true concept and the indicators used in the survey design.\nI also post-stratify some estimates with a pre-treatment covariate \\(Q_I\\) from a binomial distribution of probability .5 that has a constant effect on \\(Y_{it}\\) of \\(+1\\) (representing a fixed effect).\nI then randomly sample a pair of outcomes, for a total of four tasks \\(T\\), for \\(Y_{it}\\) in the range of \\([1,10]\\) by drawing a number from a multivariate normal distribution. The mean \\(\\mu_{it}\\) of this normal distribution is equal to a linear model with an intercept of 5, the 14 dummy variables for treatment indicators \\(X_j\\) with associated coefficients \\(\\beta_j\\), the interaction \\(\\beta_z\\) between the pre-treatment covariate \\(Z_i\\) and \\(X_{ij}\\), and a post-stratification covariate \\(Q_i\\). To simplify matters, \\(Z_i\\) is not given its own constituent term as I am not interested in the unconditional effect of \\(Z_i\\) on \\(Y_{it}\\), only the effect of \\(X_{ijt}\\) on \\(Y_{it}\\) given \\(Z_{i}\\). Finally, I draw correlated errors from a multivariate normal distribution with mean of zero and length of 4 (equal to the number of tasks per respondent) to produce a \\(4 \\times 4\\) variance matrix \\(\\varSigma_i\\) with a diagonal of 4 and intra-respondent covariation of 1 (correlation of 0.5).\n\\[\n\\begin{aligned}\nX_{ITJ} &\\sim  \\mathrm{B} \\Big( \\frac{1}{J \\times 2} \\Big)\\\\\nB_{J} &\\sim  \\mathrm{N}(0,2)\\\\\n\\beta_z &\\sim \\mathrm{N}(0.5,0.3)\\\\\nZ_I &\\sim  \\mathrm{B}(0.2)\\\\\nQ_I &\\sim  \\mathrm{B}(0.5)\\\\\n\\mu_{it} &=  5 + \\sum_{j=1}^{J} \\sum_{t=1}^{T} \\beta_j * X_{itj} + \\beta_z * X_{it1} *Z_i + Q_i\\\\\nY_{it} &\\sim  \\mathrm{N}(\\mu_{it},\\varSigma_i)\n\\end{aligned}\n\\]\nThis process will produce some numbers outside the \\([1,10]\\) range; however, it is better to leave these values in as explicit truncation will violate the assumptions of the underlying causal model.\nI run 1000 simulations for each of 300 sequential sample sizes ranging from 100 to 2700. I then take the mean significant effect and report that as the likely significant effect size for that sample size. I also record the ratio of draws for which the effect is significant (the power). However, given that the true effect is not fixed, I interpret power as the ability detect a true effect greater than zero. I record both unadjusted p-values and p-values adjusted using the cluster.vcov function from the multiwayvcov package by clustering around respondent ID to reflect the pairing of vignettes. I also use separate results when post-stratifying on a pre-treatment covariate \\(Q_I\\).\nIn addition, I included M-errors (error of absolute magnitude of significant coefficients) and S-errors (incorrect sign of significant coefficients). M-errors provide an estimate of publication bias given that the \\(p=0.05\\) threshold is a hard boundary and will necessarily result in smaller effects being reported as statistically insignificant when in fact they are greater than zero. S-errors help determine the probability that an estimated effect is the correct sign even if it is significant. S-errors are particularly problematic in small samples when sampling error can produce large negative deviations that may be statistically significant.\n\nif(run_sim==TRUE) {\n  file.create('output_log.txt',showWarnings = FALSE)\n    \n    # Need to randomize over the simulations so that parallelization works correctly on windows\n    \n    sampled_seq &lt;- sample(seq(100,num_resp,length.out = num_breaks))\n    \n  all_sims &lt;- parallelfunc(sampled_seq,function(x) {\n  \n    out_probs &lt;- 1:n_sims\n    cat(paste0(\"Now running simulation on data sample size \",x),file='output_log.txt',sep='\\n',append=TRUE)\n  \n    out_data &lt;- lapply(1:n_sims, function(j) {\n      \n      total_probs &lt;- sapply(1:x,function(x) {\n        treat_rows &lt;- sample(1:nrow(grid_pair),4)\n        treatments_indiv &lt;- c(grid_pair[treat_rows,])\n        return(treatments_indiv)\n      })\n      \n      by_resp &lt;- t(total_probs)\n      by_resp &lt;- as_data_frame(by_resp)\n      names(by_resp) &lt;- c(paste0('actor.',1:4,\"_cluster\",c(1,1,2,2)),paste0('gift.',1:4,\"_cluster\",c(1,1,2,2)))\n      by_resp$respondent &lt;- paste0('Respondent_',1:nrow(by_resp))\n      by_resp &lt;- gather(by_resp,attribute,indicator,-respondent) %&gt;% separate(attribute,into=c('attribute','cluster'),sep='_') %&gt;% \n        separate(attribute,into=c('attribute','task')) %&gt;% spread(attribute,indicator)\n      \n      \n      # Assign true coefficients for treatments\n  \n      #Beta_js\n      \n      coefs &lt;- data_frame(coef_val=rnorm(n=length(c(treatments1,treatments2)),mean=0,sd=1),\n                          treat_label=c(treatments1,treatments2))\n      \n      # Create cluster covariance in the errors\n      \n      sigma_matrix &lt;- matrix(2,nrow=4,ncol=4)\n      diag(sigma_matrix) &lt;- 4\n  \n      # Add on the outcome as a normal draw, treatment coefficients, interaction coefficient, group errors/interaction by respondent\n      \n      by_resp &lt;- gather(by_resp,treatment,appeal_type,actor,gift) %&gt;% \n        left_join(coefs,by=c('appeal_type'='treat_label'))\n      \n      # Record interaction coefficient (true estimate of interest)\n      \n      true_effect &lt;- rnorm(n=1,mean=0.5,sd=0.3)\n      \n      by_resp &lt;- select(by_resp,-treatment) %&gt;% spread(appeal_type,coef_val) %&gt;%  group_by(respondent) %&gt;% mutate(error=MASS::mvrnorm(1,mu=rep(0,4),Sigma=sigma_matrix)) %&gt;% ungroup\n      \n      # interaction coefficient only in function if military==TRUE\n      \n      by_resp &lt;- mutate(by_resp,int_coef=true_effect*rbinom(n = n(),prob = 0.2,size=1),\n                        int_coef=if_else(military!=0,int_coef,0))\n      by_resp &lt;- lapply(by_resp, function(x) {\n        if(is.double(x)) {\n          x[is.na(x)] &lt;- 0\n        }\n        return(x)\n      }) %&gt;% as_data_frame\n      \n      # To make the outcome, need to turn the dataset long\n      # However, we now need to drop the reference categories\n      # Drop one dummy from actor/gift to prevent multicollinearity = reforms + government combination\n      \n      out_var &lt;- gather(by_resp,var_name,var_value,-respondent,-task,-cluster) %&gt;% \n         filter(!(var_name %in% c('reforms','government'))) %&gt;% \n        group_by(respondent,task) %&gt;% summarize(outcome=sum(var_value)+5)\n      \n      combined_data &lt;- left_join(out_var,by_resp,by=c('respondent','task'))\n      \n      \n      # Re-estimate with a blocking variable\n      \n  \n      combined_data$Q &lt;- c(rep(1,floor(nrow(combined_data)/2)),\n                           rep(0,ceiling(nrow(combined_data)/2)))\n      \n      combined_data$outcome &lt;- if_else(combined_data$Q==1,combined_data$outcome+1,\n                                       combined_data$outcome)\n      \n      # # Create data predictor matrix and estimate coefficients from the simulated dataset\n      # \n      to_lm &lt;- ungroup(combined_data) %&gt;% select(contracts.supply:reforms,int_coef,Q)\n      to_lm &lt;- mutate_all(to_lm,funs(if_else(.!=0,1,.))) %&gt;% mutate(outcome=combined_data$outcome)\n      \n      #No post-stratification\n      # I don't estimate a constituent term for int_coef because it is assumed to be zero\n      \n      results &lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +\n                      parliament + permit.export + permit.import + permit.reg + president + \n                      int_coef:military,data=to_lm)\n      \n      results_clust &lt;- cluster.vcov(results,cluster = combined_data$respondent)\n      pvals_adj &lt;- coeftest(results,vcov.=results_clust)[-1,4]&lt;0.05\n      pvals_orig &lt;- coeftest(results)[-1,4]&lt;0.05\n      \n      total_sig_orig &lt;- mean(pvals_orig)\n      total_sig_adj &lt;- mean(pvals_adj)\n      \n      int_sig_orig &lt;- pvals_orig['military:int_coef']\n      int_sig_adj &lt;- pvals_adj['military:int_coef']\n      \n      \n      # Now run the poststratification model\n      \n      results_ps &lt;- lm(outcome~contracts.supply + exprop.firm + exprop.income + military + MOI + MOJ + municipality +\n                      parliament + permit.export + permit.import + permit.reg + president + \n                      int_coef:military + Q,data=to_lm)\n      \n      results_clust &lt;- cluster.vcov(results,cluster = combined_data$respondent)\n      pvals_adj &lt;- coeftest(results_ps,vcov.=results_clust)[-1,4]&lt;0.05\n      pvals_orig &lt;- coeftest(results_ps)[-1,4]&lt;0.05\n      \n      total_sig_orig_blocker &lt;- mean(pvals_orig)\n      total_sig_adj_blocker &lt;- mean(pvals_adj)\n      \n      int_sig_orig_blocker &lt;- pvals_orig['military:int_coef']\n      int_sig_adj_blocker &lt;- pvals_adj['military:int_coef']\n      \n      out_results &lt;- data_frame(int_sig_adj,int_sig_orig,int_sig_adj_blocker,int_sig_orig_blocker,\n                                total_sig_adj,total_sig_orig,total_sig_adj_blocker,\n                                total_sig_orig_blocker,abs_true_effect=abs(true_effect),\n                                true_effect=true_effect,\n                                est_effect=coef(results)['military:int_coef'],\n                                est_effect_ps=coef(results)['military:int_coef'])\n    })\n    out_data &lt;- bind_rows(out_data)\n    \n    return(out_data)\n  },mc.cores=parallel::detectCores(),mc.preschedule=FALSE)\n  #save the data for inspection\n  \n  all_sims_data &lt;- bind_rows(all_sims) %&gt;% mutate(sample_size=rep(sampled_seq,each=n_sims),\n                                                  iter=rep(1:n_sims,times=num_breaks))\n\n}\n\nif(run_sim==TRUE) {\nsaveRDS(object = all_sims_data,file='all_sims_data.rds')\n} else {\n  all_sims_data &lt;- readRDS('all_sims_data.rds')\n}\n\nThis simulation yields a row with the significant effect of the interaction term for that simulation for a total of n_sims draws. From this raw data I am able to calculate all of the necessary statistics mentioned above.\n\n# add in different calculations\n\nall_sims_data &lt;- group_by(all_sims_data,sample_size)  %&gt;% mutate(sigeffVorig=ifelse(int_sig_orig,\n                                                                                     est_effect,\n                                                                                     NA),\nsigeffVadj=ifelse(int_sig_adj,est_effect,NA),\nsigeffVps_orig=ifelse(int_sig_orig_blocker,est_effect_ps,NA),\nsigeffVps_adj=ifelse(int_sig_adj_blocker,est_effect_ps,NA),\npowerVorig=int_sig_orig & (true_effect&gt;0),\npowerVadj=int_sig_adj & (true_effect&gt;0),\npowerVps_orig=int_sig_orig_blocker & (true_effect &gt; 0),\npowerVps_adj=int_sig_adj_blocker & (true_effect &gt; 0),\nSerrVorig=ifelse(int_sig_orig,1-(sign(est_effect)==sign(true_effect)),NA),\nSerrVadj=ifelse(int_sig_adj,1-(sign(est_effect)==sign(true_effect)),NA),\nSerrVps_orig=ifelse(int_sig_orig_blocker,\n                    1-(sign(est_effect_ps)==sign(true_effect)),NA),\nSerrVps_adj=ifelse(int_sig_adj_blocker,\n                   1-(sign(est_effect_ps)==sign(true_effect)),NA),\nMerrVorig=ifelse(int_sig_orig,abs(est_effect)/abs_true_effect,NA),\nMerrVadj=ifelse(int_sig_adj,abs(est_effect)/abs_true_effect,NA),\nMerrVps_orig=ifelse(int_sig_orig_blocker,abs(est_effect_ps)/abs_true_effect,NA),\nMerrVps_adj=ifelse(int_sig_adj_blocker,abs(est_effect_ps)/abs_true_effect,NA))\n\nlong_data &lt;- select(all_sims_data,matches('V|sample|iter')) %&gt;% gather(effect_type,result,-sample_size,-iter) %&gt;% separate(effect_type,into=c('estimate','estimation'),sep='V') %&gt;% \n  mutate(estimate=factor(estimate,levels=c('sigeff','power','Serr','Merr'),\n                         labels=c('Mean\\nSignificant\\nEffect',\n                                  'Mean\\nPower',\n                                  'S-Error\\nRate',\n                                  'M-Error\\nRate')),\n         estimation=factor(estimation,levels=c('adj','orig','ps_adj','ps_orig'),\n                           labels=c('No Post-Stratification\\nClustered Errors\\n',\n                                    'No Post-Stratification\\nUn-clustered Errors\\n',\n                                    'Post-Stratification\\nClustered Errors\\n',\n                                    'Post-Stratification\\nUn-clustered Errors\\n')))\n\nlong_data_treatment &lt;- select(all_sims_data,matches('total|iter|sample')) %&gt;% gather(effect_type,result,-sample_size,-iter) %&gt;%\nmutate(effect_type=factor(effect_type,levels=c('total_sig_adj',\n                                               'total_sig_orig',\n                                               'total_sig_adj_blocker',\n                                               'total_sig_orig_blocker'),\n                          labels=c('No Post-Stratification\\nClustered Errors\\n',\n                                   'No Post-Stratification\\nUn-clustered Errors\\n',\n                                   'Post-Stratification\\nClustered Errors\\n',\n                                   'Post-Stratification\\nUn-clustered Errors\\n')))\n\n\n\n# Plot a sample of the data (too big to display all of it)\n\nlong_data %&gt;% ungroup %&gt;% \n  slice(1:10) %&gt;% \n  select(-estimation) %&gt;% \n  mutate(estimate=str_replace(estimate,\"\\\\n\",\" \")) %&gt;% \n  knitr::kable(.) %&gt;% \n  kable_styling(font_size = 8)\n\n\n\n\nsample_size\niter\nestimate\nresult\n\n\n\n\n1334.783\n1\nMean Significant Effect\n0.6298429\n\n\n1334.783\n2\nMean Significant Effect\n0.3874088\n\n\n1334.783\n3\nMean Significant Effect\nNA\n\n\n1334.783\n4\nMean Significant Effect\n1.1438379\n\n\n1334.783\n5\nMean Significant Effect\n0.5653086\n\n\n1334.783\n6\nMean Significant Effect\n1.2689594\n\n\n1334.783\n7\nMean Significant Effect\nNA\n\n\n1334.783\n8\nMean Significant Effect\nNA\n\n\n1334.783\n9\nMean Significant Effect\nNA\n\n\n1334.783\n10\nMean Significant Effect\nNA"
  },
  {
    "objectID": "post/conjoint/conjoint.html#plotting",
    "href": "post/conjoint/conjoint.html#plotting",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "Plotting",
    "text": "Plotting\nI use the gam function in the ggplot2 package to plot a smoothed regression line of the simulation draws for each sample size.\nFirst we can look at the difference that clustered errors makes across the different statistics. The only noticeable differences are at sample sizes smaller than 500. Clustering on respondents tends to result in smaller average significant effects, but it also results in increases in sign errors. This finding differs from the literature that considers clustering important to control for intra-respondent correlation, which in this simulation was fixed at 0.5. At sample sizes larger than 500, there does not appear to be any difference between clustered and un-clustered estimates.\n\ng_title &lt;- guide_legend(title='')\nfilter(long_data,grepl('No Post',estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +\n  theme_minimal() + stat_smooth(colour='red') +\n  xlab('Sample Size') + ylab(\"\") +\n  facet_wrap(~estimate,scales='free') + theme(panel.grid.minor.y = element_blank(),\n                                              panel.grid.major.y = element_blank()) +\n  scale_color_brewer(palette='Accent') + guides(colour=g_title,linetype=g_title) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggsave('clust_err.png',units='in',width=6)\n\nNext I look at post-stratification as an option to improve the precision of estimates. For unclustered errors reported below, post-stratified estimates do have higher power and slightly lower average significant effects, and importantly, the post-stratified estimates worsen neither type S nor type M errors.\n\ng_title &lt;- guide_legend(title='')\nfilter(long_data,grepl('Un-clustered',estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +\n  theme_minimal() + stat_smooth(colour='red') +\n  xlab('Sample Size') + ylab(\"\") +\n  facet_wrap(~estimate,scales='free') + theme(panel.grid.minor.y = element_blank(),\n                                              panel.grid.major.y = element_blank()) +\n  scale_color_brewer(palette='Accent') + guides(colour=g_title,linetype=g_title) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggsave('post_unclust_err.png',units='in',width=6)\n\nPost-stratification appears to have a similar effect on clustered error estimations, although the differences are smaller. In smaller samples, post-stratified estimates do have smaller M-errors.\n\ng_title &lt;- guide_legend(title='')\nfilter(long_data,!grepl('Un-clustered',estimation)) %&gt;% ggplot(aes(y=result,x=sample_size,linetype=estimation)) +\n  theme_minimal() + stat_smooth(colour='red') +\n  xlab('Sample Size') + ylab(\"\") +\n  facet_wrap(~estimate,scales='free') + theme(panel.grid.minor.y = element_blank(),\n                                              panel.grid.major.y = element_blank()) +\n  scale_color_brewer(palette='Accent') + guides(colour=g_title,linetype=g_title) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggsave('post_clust_err.png',units='in',width=6)\n\nFinally, I also report average numbers of significant coefficients for the 14 treatments. Given that the 14 treatments were sampled from a normal distribution with prior density in the positive values with a meaan of 0.5, in expectation 95% of estimates should be statisticall significant. While that upper limit is reached only in high sample numbers, it looks like the ratio for treatment effects reaches an acceptable level of 70 percent at about 500 sample respondents. Also, post-stratifying un-clustered models results in effects that are reported as significant at much higher rates, as would follow from the previous results about post-stratification.\n\ng_title &lt;- guide_legend(title='')\nggplot(long_data_treatment,aes(y=result,x=sample_size,linetype=effect_type,colour=effect_type)) +\n  theme_minimal() + stat_smooth() +\n  xlab('Sample Size') + ylab(\"\") +\ntheme(panel.grid.minor.y = element_blank(),\n                                              panel.grid.major.y = element_blank()) +\n  scale_color_brewer(palette='Dark2') + guides(linetype=g_title,colour=g_title) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nggsave('all_treat_rate.png',units='in',width=6)"
  },
  {
    "objectID": "post/conjoint/conjoint.html#conclusion",
    "href": "post/conjoint/conjoint.html#conclusion",
    "title": "Simulating Conjoint Survey Experiments",
    "section": "Conclusion",
    "text": "Conclusion\nThis simulation study shows that a sample size of approximately 1,000 respondents is enough to obtain high power while also lowering both the S and M-error rates for treatment interaction effects in this conjoint experiment. The treatment effects themselves are generally of high quality once the sample size reaches 500 because the total number of respondents in each treatment cell is considerably higher than in an interaction. Post-stratification appears to be a useful strategy to increase precision without inducing S or M errors; at the very least, post-stratification does not appear to have any adverse effects on the estimation.\nOn the other hand, it appears that clustering errors increases the S-error rate at small sample sizes, a surprising finding considering that clustering methods are designed to inflate, not deflate, standard errors. Given that the S-error rate reveals the likelihood of making an error about the sign of the treatment effect, this is a potentially serious problem. For that reason I intend to report both clustered and un-clustered estimates in my analysis."
  },
  {
    "objectID": "post/kais_saied_results/index.html",
    "href": "post/kais_saied_results/index.html",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "",
    "text": "This Rmarkdown document contains code and data for the Kais Saied survey experiment by Robert Kubinec and Amr Yakout. This survey was fielded using an online panel in Tunisia during August of 2023. The survey experiment involved providing half the sample a direct question about whether they supported Kais Saied’s move to suspend parliament and centralize power in his hands, and the other half of the sample had a randomized response question designed to allow them to answer truthfully without being identified. This experiment was pre-registered and the pre-registration can be accessed from this link.\nThis document was drafted to allow people to verify our results using the raw data. The survey data included has been stripped of any identifying information about respondents, but it does include the actual answers from the survey so that all analyses can be reproduced. You can access the code file and the raw data from this Github repository: https://github.com/saudiwin/saudiwin.github.io/tree/sources/content/post (file is entitled kais_saied_results.Rmd and the data file is in the data/ subfolder). The survey data can be found in the data/ folder as kais_saied_survey.csv.\nThis Quarto document includes embedded R code that loads the survey data and estimates the Stan model for proportion of people who support Kais Saied, and compares it to the direct question to see if there is sensitive survey bias. At present there are a total of 913 completed responses.\nAll questions about the analysis should be directed to Robert Kubinec at rmk7@nyu.edu."
  },
  {
    "objectID": "post/kais_saied_results/index.html#robustness-check-time-of-completion",
    "href": "post/kais_saied_results/index.html#robustness-check-time-of-completion",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Robustness check: time of completion",
    "text": "Robustness check: time of completion\nIn this section I show some general statistics for the data. In particular, I look at how long it takes people to complete the survey. In the plot below I show the average duration in minutes over time for each survey response. The plot shows that most people took the survey in about 10 to 30 minutes:\n\nsurvey_data %&gt;% \n  filter(`Duration (in seconds)`&lt;10000) %&gt;% \n  mutate(Duration=`Duration (in seconds)`/60) %&gt;% \n  ggplot(aes(y=Duration,x=StartDate)) +\n  geom_point() +\n  scale_y_log10() +\n  geom_smooth()\n\n\n\n\n\n\n\n\nThe plot also shows that data collection started in late July and continued until mid-August. Data came in fairly regularly on a daily basis."
  },
  {
    "objectID": "post/kais_saied_results/index.html#survey-design",
    "href": "post/kais_saied_results/index.html#survey-design",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Survey Design",
    "text": "Survey Design\nIn the survey we implemented a form of a randomized response question that is designed to help people report answers on a survey that they may be embarrassed about or in danger if they report. The question was originally developed for private medical information like abortion or HIV infections, and has been extended to cover crime, drug use and corruption reporting. See Hout and Heijden (2002), Gingerich et al. (2016) and Blair, Imai, and Zhou (2015) for more information.\nThe theory behind a randomized response is to inject randomness into the survey question. Theoretically it is similar to approaches in cryptography that use random numbers to encode online data using keys. In our case, we used naturally ocurring randomness related to people’s birthdays. Because we did not ask for respondents’ birth dates, and the month in which one is born is essentially random, we can use that naturally occurring randomness to encode their responses.\nTo test for how much we could encourage people to respond truthfully, we randomly assigned each respondent to receive one of the two questions below:\nDirect Question\n\nDo you oppose President Kais Saied’s moves to change Tunisia’s constitution and close the parliament?\n\nYes\nNo\n\n\nRandomized Response\nWe understand that politics in Tunisia is sensitive right now. This question is worded so that you can tell us what you think but still protect your privacy. Because we don’t know when your mother was born, we also won’t know for sure your political opinion.\n\nMy mother’s birth date is in January, February or March.\nI oppose President Kais Saied’s moves to change Tunisia’s constitution and close the parliament.\n\nPlease pick the answer that best represents whether these statements are true of you:\n\nBoth statements are true OR neither is true.\nOne of the two statements is true.\n\nWhile explaining how this question works is beyond the scope of this note, we again refer to the linked paper above for the statistical mechanics. As long as the respondent reads and follows the instructions, they can report their opposition (or support) for Kais Saied’s power grab without us being able to know their true response. Essentially, we can’t separate their answer from whether their mother’s birthday is in a given month, and as a result, the individual answers are encoded. For any respondent, we have no idea whether they oppose Saied or not.\nHowever, the beauty of this method is that even though we don’t know any one person’s response, we can estimate how many in people in total support or oppose Saied. When we aggregate responses, we can take into account that on average \\(\\frac{1}{4}\\) of our respondents will have mothers who were born in those months. Using statistical models that we estimate below, we can find out how many people truly oppose Saied, and also how many people appear to be under-reporting their opposition relative to the direct question."
  },
  {
    "objectID": "post/kais_saied_results/index.html#sensitivity-estimates",
    "href": "post/kais_saied_results/index.html#sensitivity-estimates",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Sensitivity Estimates",
    "text": "Sensitivity Estimates\nIn this section we show several different ways that we calculate the true number of people who are opposed to Kais Saied. We first use a simple Bayesian calculation based on the beta distribution that was specified in the pre-registration linked above. We then look at other existing R packages that are used for randomized response questions and then we implement a custom Bayesian model that allows us to jointly model both the randomized response and the direct question. We also use this custom model to allow us to adjust for biases in using online panels that may not be fully representative.\n\n# proportion selecting birthday response\n\nlambda &lt;- table(survey_data$kais_rr)\nlambda &lt;- lambda[2]/sum(lambda)\n\nN &lt;- sum(as.numeric(!is.na(survey_data$kais_rr) & survey_data$FL_63_DO_saied_sensitive==1))\n\nto_stan_data &lt;- list(success_cm=N*((lambda - .75)/(-.5)),\n                     fail_cm= N*(1-((lambda - .75)/-.5)),\n                     success_d=sum(as.numeric(survey_data$kais_direct==\"Yes\" & survey_data$FL_63_DO_saied_nonsensitive==1),na.rm=T),\n                     fail_d=sum(as.numeric(survey_data$kais_direct==\"No\" & survey_data$FL_63_DO_saied_nonsensitive==1),na.rm=T))\n\nfit_mod &lt;- sen_mod$sample(chains=4, cores=4, data=to_stan_data,refresh=0)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\nfit_mod$summary()\n\n# A tibble: 5 × 10\n  variable          mean   median     sd    mad       q5      q95  rhat ess_bulk\n  &lt;chr&gt;            &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1 lp__          -853.    -852.    1.23   1.01   -855.    -851.     1.00    1998.\n2 pi_hat           0.507    0.507 0.0244 0.0250    0.468    0.548  1.00    3616.\n3 d_hat            0.298    0.298 0.0218 0.0218    0.262    0.334  1.00    3943.\n4 d_hat_binomi…    0.298    0.297 0.0212 0.0211    0.263    0.333  1.00    3934.\n5 estimand         0.210    0.210 0.0323 0.0319    0.157    0.264  1.00    3622.\n# ℹ 1 more variable: ess_tail &lt;dbl&gt;\n\n\nWe can plot the estimated bias as the difference between the average proportion of those responding affirmative to the direct question and the calculated level of opposition to Saied from the randomized response question:\n\nestimand &lt;- c(fit_mod$draws(\"estimand\"))\n\nmcmc_dens(fit_mod$draws(\"estimand\")) + \n  theme_tufte() +\n  labs(x=\"Percent Increase Over Direct Question\") +\n  scale_x_continuous(labels=scales::percent_format()) + \n  ggtitle(\"Difference Between Direct Question and Encrypted Question\",\n          subtitle=\"Opposition to Saied's Coup\")\n\n\n\n\n\n\n\nggsave(\"bias_calc.png\",width=5,height=4)\n\nOur basic model shows that approximately 21% of respondents reported anti-Saied preferences in the randomized response method versus the direct question. The 5% - 95% uncertainty interval is from 15.7 to 26.4 .\nThe full posterior density of the bias-corrected estimate is:\n\nmcmc_dens(fit_mod$draws(\"pi_hat\")) + theme_tufte()\n\n\n\n\n\n\n\n\nComparing the two estimates as intervals (pi_hat is the true level of opposition to Saied and d_hat is the proportion from the direct question):\n\nmcmc_intervals(fit_mod$draws(c(\"pi_hat\",\"d_hat\"))) + theme_tufte()"
  },
  {
    "objectID": "post/kais_saied_results/index.html#alternative-estimation-rrreg",
    "href": "post/kais_saied_results/index.html#alternative-estimation-rrreg",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Alternative Estimation: RRreg",
    "text": "Alternative Estimation: RRreg\nIn this section we use the R package RRreg to check these calculations using a more traditional means for estimating sensitive proportions.\n\nest_rreg &lt;- RRuni(response=as.numeric(comp_data_rr$kais_rr==\"Both statements are true OR neither is true.\" & comp_data_rr$FL_63_DO_saied_sensitive==1),\n                  p=.25,\n                  model=\"Crosswise\",MLest = TRUE)\n\nsummary(est_rreg)\n\nCrosswise Model with p = 0.25\nSample size: 425\n\n   Estimate   StdErr      z  Pr(&gt;|z|)    \npi 0.507059 0.048563 10.441 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ns1 &lt;- summary(est_rreg)\n\nLet’s compare these estimates with plots:\n\nest_beta &lt;- fit_mod$summary()\n\nest_data &lt;- tibble(estimate=c(s1$coefficients[,1],\n                              est_beta$median[est_beta$variable==\"pi_hat\"]),\n                   low=c(s1$coefficients[,1]-1.96*(s1$coefficients[,2]),\n                         est_beta$q5[est_beta$variable==\"pi_hat\"]),\n                   high=c(s1$coefficients[,1]+1.96*(s1$coefficients[,2]),\n                         est_beta$q95[est_beta$variable==\"pi_hat\"]),\n                   estimator=c(\"RRreg\",\"Bayes\"))\n\nest_data %&gt;% \n  ggplot(aes(y=estimate,x=estimator)) +\n  geom_linerange(aes(ymin=low,\n                     ymax=high)) +\n  geom_point() +\n   theme_tufte()\n\n\n\n\n\n\n\n\nWe can see that they are similar estimates, although the R package has higher uncertainty than the simple Bayesian estimate."
  },
  {
    "objectID": "post/kais_saied_results/index.html#alternative-estimator-rr",
    "href": "post/kais_saied_results/index.html#alternative-estimator-rr",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Alternative Estimator: rr",
    "text": "Alternative Estimator: rr\nWe can also use the R package rr which uses the EM algorithm as opposed to RRreg which uses more conventional optimization.\n\ncomp_data_rr$gender_two &lt;- factor(comp_data_rr$gender,\n                                  exclude=\"Other:\")\ncomp_data_rr &lt;- filter(comp_data_rr, !is.na(gender_two))\n# need to switch the outcome\ncomp_data_rr$kais_rr_num_switch &lt;- 1 - comp_data_rr$kais_rr_num\nrr_est &lt;- rrreg(kais_rr_num_switch ~ as.character(gender_two), \n                data=comp_data_rr,\n                      p=.75,\n                      design='mirrored')\n\n# predict sensitive trait\n\npred_rr &lt;- predict(rr_est,quasi.bayes = T)\nmean(pred_rr$est)\n\n[1] 0.5006138\n\nmean(pred_rr$est[comp_data_rr$gender_two==\"Male\"],na.rm=T)\n\n[1] 0.5191125\n\nmean(pred_rr$est[comp_data_rr$gender_two==\"Female\"],na.rm=T)\n\n[1] 0.4720857\n\npred_rr_mean &lt;- predict(rr_est,quasi.bayes = T,avg=T)\n\nThis number seems very close to our other estimators. Again, the uncertainty seems larger than the simple Bayesian method. Minimal gender differences over who is more or less likely to oppose Saied.\nWe can plot this estimate against the others:\n\nest_data &lt;- tibble(estimate=c(s1$coefficients[,1],\n                              est_beta$median[est_beta$variable==\"pi_hat\"],\n                              pred_rr_mean$est),\n                   low=c(s1$coefficients[,1]-1.96*(s1$coefficients[,2]),\n                         est_beta$q5[est_beta$variable==\"pi_hat\"],\n                         pred_rr_mean$ci.lower),\n                   high=c(s1$coefficients[,1]+1.96*(s1$coefficients[,2]),\n                         est_beta$q95[est_beta$variable==\"pi_hat\"],\n                         pred_rr_mean$ci.upper),\n                   estimator=c(\"RRreg\",\"Bayes\",\"EM\"))\n\nest_data %&gt;% \n  ggplot(aes(y=estimate,x=estimator)) +\n  geom_linerange(aes(ymin=low,\n                     ymax=high)) +\n  geom_point() + theme_tufte()"
  },
  {
    "objectID": "post/kais_saied_results/index.html#brms-model",
    "href": "post/kais_saied_results/index.html#brms-model",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "BRMS Model",
    "text": "BRMS Model\nFinally, we define a brms custom family for the randomized response model using the confusion matrix approach of Hout and Heijden (2002). This model is a variety of the Bernoulli distribution that takes into account the known probabilities of obtaining the true response. It jointly models both the randomized response model and the direct question, allowing us to estimate the bias directly rather than post-estimation. By implementing it in brms, we can make use of brms features like multilevel regression to allow us to do post-stratification adjustment of the survey responses with population data from Tunisia’s 2014 census. This is our preferred specification.\n\nlibrary(brms)\n\nstan_funs &lt;- '\n  real sens_reg_lpmf(int y, real mu, real bias, matrix P, int T) {\n  \n    // generalized RR model from van der Hout and van der Heijden (2002)\n    // also see R package RRreg\n    \n    real out;\n    // need to impose a constraint on bias where it cannot be larger than mu\n    real bias_trans = mu * bias;\n    \n    if(T==1) {\n    \n      // treatment distribution (crosswise model)\n      \n      if(y==1) {\n      \n        out = P[2,1] * (1 - mu) + P[2,2] * mu;\n      \n      } else if(y==0) {\n      \n       out = P[1,1]*(1-mu) + P[1,2]*mu;\n      \n      }\n\n    } else if (T==0) {\n    \n      // control = direct question\n    \n      if(y==1) {\n      \n        out = mu - bias_trans;\n      \n      } else if(y==0) {\n      \n        out = (1 - mu) + bias_trans;\n      \n      }\n    \n    }\n    \n    return log(out); \n\n  }\n  \n  int sens_reg_rng(real mu, real bias, matrix P, int T) {\n  \n    real bias_trans = mu*bias;\n  \n    if(T==1) {\n    \n      return bernoulli_rng(P[2,1] * (1 - mu) + P[2,2] * mu);\n      \n    } else {\n    \n      return bernoulli_rng(mu - bias_trans);\n    \n    }\n\n  }'\n\n# define custom family\n\nfamily_sens_reg &lt;- custom_family(\"sens_reg\",\n                                 dpars=c(\"mu\",\"bias\"),\n                                 links=c(\"logit\",\"logit\"),\n                                 type=\"int\",\n                                 lb=c(NA,NA),\n                                 ub=c(NA,NA),\n                                 vars=c(\"P\",\"vint1[n]\"))\n\n# define log-likelihood\n\nlog_lik_sens_reg &lt;- function(i, prep) {\n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  y &lt;- prep$data$Y[i]\n  treatment &lt;- prep$data$vint1[i]\n  bias &lt;- brms::get_dpar(prep, \"bias\", i = i)\n  \n  bias_trans &lt;- bias*mu\n  \n  if(treatment==1) {\n    \n    if(y==1) {\n    \n      return(log(P[2,1] * (1 - mu) + P[2,2] * mu))\n  \n    } else {\n      \n      return(log(P[1,1]*(1-mu) + P[1,2]*mu))\n    \n    }\n    \n  } else {\n    \n    if(y==1) {\n      \n      return(log(mu - bias_trans))\n      \n    } else {\n      \n      return(log((1 - mu) + bias_trans))\n      \n    }\n    \n  }\n  \n\n}\n\n# define posterior predictions\n\nposterior_predict_sens_reg &lt;- function(i, prep, ...) {\n  \n  mu &lt;- brms::get_dpar(prep, \"mu\", i = i)\n  bias &lt;- brms::get_dpar(prep, \"bias\", i = i)\n  y &lt;- prep$data$Y[i]\n  treatment &lt;- prep$data$vint1[i]\n  \n  bias_trans &lt;- mu*bias\n\n  if(treatment==1) {\n    \n    out &lt;- rbinom(n=length(mu),size=1,prob=P[2,1] * (1 - mu) + P[2,2] * mu)\n    \n  } else {\n    \n    out &lt;- rbinom(n=length(mu),size=1,prob=mu - bias_trans)\n    \n  }\n  \n  return(out)\n  \n}\n\n# define posterior expectation (equal to latent variable pi)\n\nposterior_epred_sens_reg &lt;- function(prep,...) {\n\n  mu &lt;- brms::get_dpar(prep, \"mu\")\n  bias &lt;- brms::get_dpar(prep, \"bias\")\n\n  mu\n\n}\n\nposterior_epred_bias_sens_reg &lt;- function(prep,...) {\n\n  mu &lt;- brms::get_dpar(prep, \"mu\")\n  bias &lt;- brms::get_dpar(prep, \"bias\")\n\n  bias*mu\n\n}\n\nP &lt;- getPW(\"Warner\",p=.25)\n\nall_stanvars &lt;- stanvar(x=P,block = \"data\") + \n  stanvar(scode=stan_funs,block=\"functions\")\n\nsurvey_data$age_cat_order &lt;- ordered(survey_data$age_cat)\n\nfit1 &lt;- brm(bf(kais_combined | vint(treatment) ~ gender*mo(age_cat_order) + (1|gov)), \n            data=survey_data,\n            family=family_sens_reg,\n            stanvars=all_stanvars,\n            prior=prior(beta(1,1),class=\"bias\") + \n              prior(normal(0,5), class=\"b\"),\n            chains=2,cores=2,control=list(max_treedepth=11,\n                                          adapt_delta=0.95),\n            backend = \"cmdstanr\")\n\nRunning MCMC with 2 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 44.7 seconds.\nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 61.7 seconds.\n\nBoth chains finished successfully.\nMean chain execution time: 53.2 seconds.\nTotal execution time: 61.8 seconds.\n\npp_check(fit1, type=\"bars\",ndraws=500)\n\n\n\n\n\n\n\nloo(fit1)\n\n\nComputed from 2000 by 880 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -576.8  8.9\np_loo        10.6  0.5\nlooic      1153.7 17.9\n------\nMonte Carlo SE of elpd_loo is 0.1.\n\nAll Pareto k estimates are good (k &lt; 0.5).\nSee help('pareto-k-diagnostic') for details.\n\ngov_cats &lt;- ranef(fit1,groups = \"gov\")$gov[,,1] %&gt;% as_tibble %&gt;% \n  mutate(level=row.names(ranef(fit1,groups = \"gov\")$gov[,,1]))\n\nPlot this additional type of estimation:\n\nget_latent_draws &lt;- posterior_epred(fit1)\n\nget_latent_est &lt;- tibble(median=median(get_latent_draws),\n                         high=quantile(apply(get_latent_draws, 1, median),.95),\n                         low=quantile(apply(get_latent_draws, 1, median),.05))\n\n# get estimate of bias\n\nprep_bias &lt;- prepare_predictions(fit1)\n\nbias_trans &lt;- posterior_epred_bias_sens_reg(prep_bias)\n\nbias_trans_est &lt;- tibble(bias_est_low=quantile(apply(bias_trans,1,median),.05),\n                         bias_est=median(apply(bias_trans,1,median)),\n                         bias_est_high=quantile(apply(bias_trans,1,median),.95))\n\nest_data &lt;- tibble(estimate=c(s1$coefficients[,1],\n                              est_beta$median[est_beta$variable==\"pi_hat\"],\n                              pred_rr_mean$est,\n                              get_latent_est$median),\n                   low=c(s1$coefficients[,1]-1.96*(s1$coefficients[,2]),\n                         est_beta$q5[est_beta$variable==\"pi_hat\"],\n                         pred_rr_mean$ci.lower,\n                         get_latent_est$low),\n                   high=c(s1$coefficients[,1]+1.96*(s1$coefficients[,2]),\n                         est_beta$q95[est_beta$variable==\"pi_hat\"],\n                         pred_rr_mean$ci.upper,\n                         get_latent_est$high),\n                   estimator=c(\"RRreg\",\"Bayes\",\"EM\",\"Bayes_Reg\"))\n\nest_data %&gt;% \n  ggplot(aes(y=estimate,x=estimator)) +\n  geom_linerange(aes(ymin=low,\n                     ymax=high)) +\n  geom_point() + theme_tufte()\n\n\n\n\n\n\n\nknitr::kable(bias_trans_est)\n\n\n\n\nbias_est_low\nbias_est\nbias_est_high\n\n\n\n\n0.1356792\n0.229125\n0.3196246\n\n\n\n\n\nAgain, we see that all of the estimates are quite similar. In the last section we compare them directly with a simulation, which shows that the two Bayesian estimators are both accurate and have good coverage, while RRreg is accurate but understates uncertainty in general."
  },
  {
    "objectID": "post/kais_saied_results/index.html#adjustment-with-mrp",
    "href": "post/kais_saied_results/index.html#adjustment-with-mrp",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Adjustment with MRP",
    "text": "Adjustment with MRP\nGiven that the data come from an online panel, we want to use Tunisian 2014 census data to adjust these findings by age, sex and governorate. This will account for a considerable (though not all) amount of sample selection bias due to the online survey frame. We will use random effects to model the influence of the sample frame on the outcome.\n\nhead(census)\n\n# A tibble: 6 × 6\n  gender gov     pop age_cat age_cat_order    prop\n  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;ord&gt;           &lt;dbl&gt;\n1 Male   Tunis 17446 15-19   15-19         0.00120\n2 Female Tunis 16984 15-19   15-19         0.00117\n3 Male   Tunis 54142 20-24   20-24         0.00373\n4 Female Tunis 51111 20-24   20-24         0.00352\n5 Male   Tunis 48773 25-29   25-29         0.00336\n6 Female Tunis 45668 25-29   25-29         0.00315\n\n\nWe will predict the sensitive trait for each census category, then stratify by summing over the proportion of population in each cell from the census data. We then plot this adjusted estimate.\n\n# do for each condition, then average\n\ncensus_treat &lt;- mutate(census, treatment=1)\ncensus_notreat &lt;- mutate(census, treatment=0)\n\npred_data &lt;- bind_rows(census_treat,\n                       census_notreat) %&gt;% \n  mutate(prop=prop/2)\n\ntunisia_pred &lt;- posterior_epred(fit1, newdata=pred_data) %&gt;% \n  as_tibble %&gt;% \n  mutate(draw=1:n()) %&gt;% \n  gather(key=\"key\",\n         value=\"estimate\",-draw) %&gt;% \n  mutate(draw=paste0(\"V\",draw)) %&gt;% \n  spread(key=\"draw\",value=\"estimate\") %&gt;% \n  mutate(key=as.numeric(stringr::str_remove(key, \"V\")))\n\ntunisia_pred &lt;- left_join(tunisia_pred, \n                          mutate(pred_data, key=1:n())) %&gt;% \n  left_join(census) %&gt;% \n  gather(key = \"draw\",\n         value= \"estimate\",\n         matches(\"V\",ignore.case=F))\n\n# aggregate to highest level\n\nagg_est &lt;- tunisia_pred %&gt;% \n  group_by(draw) %&gt;% \n  summarize(est_adj = sum(estimate * prop))\n\nagg_est %&gt;% \n  ggplot(aes(x=est_adj)) +\n  geom_density(fill=\"blue\",alpha=0.5,\n               colour=NA) + theme_tufte() +\n  labs(y=\"\",x=\"Percent Opposing Saied's Coup\") +\n  scale_x_continuous(labels=scales::percent_format()) +\n  geom_vline(aes(xintercept=mean(est_adj)),linetype=2) +\n   ggtitle(\"Estimated Number Who Oppose Saied's 2021 Coup\",\n          subtitle=\"Based on Statistical Analysis of Randomized Response Question\") +\n  annotate(\"text\",\n           x=.52,y=4,label=paste0(\"Most likely number: \",round(mean(agg_est$est_adj),\n                                                               3)*100,\"%\"),\n           hjust=0)\n\n\n\n\n\n\n\nggsave(\"num_oppose.png\",width=5,height=3)\n\n{quantile(agg_est$est_adj, c(0.05,.5,.95))}\nA difference of approximately 1 percent between the adjusted estimate and the naive number:\n\nquantile(apply(get_latent_draws, 1, mean),c(0.05,.5,.95))\n\n       5%       50%       95% \n0.4351862 0.5194791 0.6011222 \n\n\nNext we can aggregate these results to look at gender, age and governorate in terms of relative levels of Saied support. Given that there is substantial uncertainty due to the sensitive question design, these results are somewhat noisy and should be interpreted with caution.\n\nGender\nWhen we aggregate to the level of gender, we see that men are approximately 5% more likely than women to report opposition President Kais Saied.\n\n# merge in census data\nplot_data_gender &lt;- tunisia_pred %&gt;% \n  group_by(gender,draw) %&gt;% \n  mutate(prop_gender=pop/sum(pop)) %&gt;% \n  summarize(est_gender=sum(estimate*prop_gender)) %&gt;% \n  group_by(gender) %&gt;% \n  summarize(median_gender=median(est_gender),\n            low_gender=quantile(est_gender,.05),\n            high_gender=quantile(est_gender,.95))\n\nselect(plot_data_gender,\n       y=\"median_gender\",\n       x=\"gender\",\n       high_y=\"high_gender\",\n       low_y=\"low_gender\") %&gt;% \n  write_csv(\"plot_data_gender.csv\")\n\n  plot_data_gender %&gt;% \n  ggplot(aes(y=median_gender,\n             x=gender)) +\n  geom_pointrange(aes(ymin=low_gender,\n                      ymax=high_gender)) +\n  scale_y_continuous(labels=scales::percent) +\n    labs(\"% Opposing Saied\") +\n  theme_tufte()\n\n\n\n\n\n\n\n\n\n\nAge\nThe plot below shows aggregated opposition by age. Approximately 60 percent of the youngest age category (18 to 19 year olds) oppose President Saied while only 40 percent of the oldest age category (80 and over) oppose President Saied. This pattern is noticeably stronger than that for gender, though there is still considerable uncertainty.\n\nplot_data_age &lt;- tunisia_pred %&gt;% \n  group_by(age_cat,draw) %&gt;% \n  mutate(prop_age_cat=pop/sum(pop)) %&gt;% \n  summarize(est_age_cat=sum(estimate*prop_age_cat)) %&gt;% \n  group_by(age_cat) %&gt;% \n  summarize(median_age_cat=median(est_age_cat),\n            low_age_cat=quantile(est_age_cat,.05),\n            high_age_cat=quantile(est_age_cat,.95)) %&gt;% \n  mutate(age_cat=recode(age_cat, `80 year and more`=\"80+\")) \n  \n  select(plot_data_age,\n       y=\"median_age_cat\",\n       x=\"age_cat\",\n       high_y=\"high_age_cat\",\n       low_y=\"low_age_cat\") %&gt;% \n  write_csv(\"plot_data_age.csv\")\n\n  plot_data_age %&gt;% \n  ggplot(aes(y=median_age_cat,\n             x=age_cat)) +\n  geom_pointrange(aes(ymin=low_age_cat,\n                      ymax=high_age_cat)) +\n    scale_y_continuous(labels=scales::percent) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2))+\n  labs(y=\"% Opposing Saied\",x=\"\") +\n  ggtitle(\"Number Opposing Saied by Age Category\",\n         subtitle=\"Encrypted (Randomized) Response Question\") +\n   theme_tufte()\n\n\n\n\n\n\n\nggsave(\"age_cat.png\",width=5,height=3)\n\n\n\nRegion\nFinally, the plot below shows predicted opposition by region. In general, more rural districts like Kebili, Beja and Kairouan report more opposition, but these differences are quite noisy. Differences between regions are not especially pronounced in the data.\n\nplot_data_region &lt;- tunisia_pred %&gt;% \n  group_by(gov,draw) %&gt;% \n  mutate(prop_gov=pop/sum(pop)) %&gt;% \n  summarize(est_gov=sum(estimate*prop_gov)) %&gt;% \n  group_by(gov) %&gt;% \n  summarize(median_gov=median(est_gov),\n            low_gov=quantile(est_gov,.05),\n            high_gov=quantile(est_gov,.95))\n\n\nselect(plot_data_region,\n       x=\"median_gov\",\n       y=\"gov\",\n       high_x=\"high_gov\",\n       low_x=\"low_gov\") %&gt;% \n  write_csv(\"plot_data_region.csv\")\n\nplot_data_region %&gt;% \n  ggplot(aes(y=median_gov,\n             x=reorder(gov,median_gov))) +\n  coord_flip() +\n    labs(\"% Opposing Saied\") +\n  geom_pointrange(aes(ymin=low_gov,\n                      ymax=high_gov)) + theme_tufte()"
  },
  {
    "objectID": "post/kais_saied_results/index.html#simulation-comparison-univariate-vs.-regression",
    "href": "post/kais_saied_results/index.html#simulation-comparison-univariate-vs.-regression",
    "title": "What Do Tunisians Really Think About President Kais Saied?",
    "section": "Simulation Comparison: Univariate Vs. Regression",
    "text": "Simulation Comparison: Univariate Vs. Regression\nIn this simulation we test the coverage and unbiasedness of the estimators, including our preferred Bayesian specification. The simulation shows that the estimators perform quite well at recovering the sensitive trait, and our Bayesian models have excellent coverage (the uncertainty they report is reasonable). We’ll use the regression spec as the DGP along with our observed parameters for this study.\n\nlibrary(parallel)\n\n# confusion matrix\n\nP &lt;- getPW(\"Warner\",.25)\n\n# assume N = 500, small sensitivity of 0.05\n\ntheta &lt;- 0.3\nN &lt;- 800\nbias &lt;- 0.1\n\nsims &lt;- 500\n\nif(run_sim) {\n  \n  over_sims &lt;- lapply(1:sims, function(s) {\n  \n  # assign half to treatment, half to control\n  \n  treatment &lt;- as.numeric(runif(N)&gt;0.5)\n  \n  obs_response &lt;- ifelse(treatment==1,\n                         as.numeric((P[2,1] * (1 - theta) + P[2,2]*theta)&gt;runif(N)),\n                         as.numeric((theta - bias)&gt;runif(N)))\n  \n  out_data &lt;- tibble(y = obs_response,\n                     treatment=treatment)\n  \n  # define custom family as being upper and lower bounded for bias\n  \n  family_sens_reg &lt;- custom_family(\"sens_reg\",\n                                 dpars=c(\"mu\",\"bias\"),\n                                 links=c(\"logit\",\"logit\"),\n                                 type=\"int\",\n                                 lb=c(NA,0),\n                                 ub=c(NA,1),\n                                 vars=c(\"P\",\"vint1[n]\"))\n  \n  # estimate model with brms\n\n  est_mod_brms &lt;- brm(y | vint(treatment) ~ 1,\n                      family=family_sens_reg,\n                      stanvars=all_stanvars,\n                      data=out_data,\n                      #prior=prior(normal(0,10),class=\"bias\"),\n                      prior=prior(beta(1,1),class=\"bias\") +\n                        prior(normal(-0.84,10),class=\"Intercept\"),\n                      chains=1,\n                      cores=1,\n                      iter=1000,\n            backend = \"cmdstanr\")\n  \n  get_est_brms &lt;- posterior::summarise_draws(as_draws(est_mod_brms, \"b_Intercept\"),\n                                             estimate=~median(plogis(.x)),\n                                             high=~quantile(plogis(.x),.95),\n                                             low=~quantile(plogis(.x),.05)) %&gt;% \n    mutate(param=\"mu\") \n  \n  # calculate bias transformed\n  \n  bias_est &lt;- as_draws_matrix(est_mod_brms, \"bias\")\n  \n  mu_trans &lt;- posterior_epred(est_mod_brms) %&gt;% apply(1,median)\n\n  bias_trans_est &lt;- tibble(`95%`=quantile(bias_est * mu_trans,.95),\n                         estimate=median(bias_est * mu_trans),\n                         `5%`=quantile(bias_est * mu_trans,.05)) %&gt;% \n    mutate(param=\"bias\")\n  \n  get_est_brms &lt;- bind_rows(get_est_brms, bias_trans_est) %&gt;% \n    mutate(model=\"Bayes BRMS\")\n  \n  # compare with RRreg\n  \n  est_freq &lt;- RRuni(response=obs_response[treatment==1],\n                  p=.25,\n                  model=\"Crosswise\",MLest = TRUE)\n  \n  this_sum &lt;- summary(est_freq)\n  \n  # now do cheap Bayes\n  \n  lambda &lt;- mean(obs_response[treatment==1])\n  \n  sim_data2 &lt;- list(success_cm=(sum(treatment))*((lambda - .75)/(-.5)),\n                     fail_cm= (sum(treatment))*(1-((lambda - .75)/-.5)),\n                     success_d=sum(obs_response[treatment==0]),\n                     fail_d=sum(1 - obs_response[treatment==0]))\n\n  fit_simple &lt;- sen_mod$sample(chains=1, cores=1, data=sim_data2,refresh=0)\n  \n  simple_sum &lt;- fit_simple$summary()\n  \n  mu &lt;- try(tibble(estimates=c(simple_sum$median[simple_sum$variable==\"pi_hat\"],\n                     this_sum$coefficients[1,1],\n                     get_est_brms$estimate[get_est_brms$param==\"mu\"]),\n         q5=c(simple_sum$q5[simple_sum$variable==\"pi_hat\"],\n                     this_sum$coefficients[1,1] - 1.96*this_sum$coefficients[1,2],\n              get_est_brms$`5%`[get_est_brms$param==\"mu\"]),\n         q95=c(simple_sum$q95[simple_sum$variable==\"pi_hat\"],\n                     this_sum$coefficients[1,1] + 1.96*this_sum$coefficients[1,2],\n               get_est_brms$`95%`[get_est_brms$param==\"mu\"]),\n         models=c(\"RRreg\",\"Bayes Simple\",\"Bayes BRMS\"),\n         param=\"mu\") %&gt;% \n    mutate(cov=theta &gt; q5 & theta &lt; q95,\n           sim=s))\n  \n  bias_est &lt;- try(filter(get_est_brms, param==\"bias\") %&gt;% \n    select(estimates=\"estimate\",\n           q5=\"5%\",\n           q95=\"95%\") %&gt;% \n    mutate(cov=bias &gt; q5 & bias &lt; q95,\n           sim=s,\n           param=\"bias\",\n           models=\"brms\"))\n  \n  try(bind_rows(mu, bias_est))\n   \n  \n}) %&gt;% bind_rows\n  \n  saveRDS(over_sims, \"data/over_sims.rds\")\n  \n} else {\n  \n  over_sims &lt;- readRDS(\"data/over_sims.rds\")\n  \n}\n\n\n\nover_sims %&gt;% group_by(models,param) %&gt;% \n  summarize(mean_cov=mean(cov)) %&gt;% \n  knitr::kable(caption=\"Coverage\")\n\n\nCoverage\n\n\nmodels\nparam\nmean_cov\n\n\n\n\nBayes BRMS\nmu\n0.906\n\n\nBayes Simple\nmu\n0.956\n\n\nRRreg\nmu\n0.568\n\n\nbrms\nbias\n0.908\n\n\n\n\nover_sims %&gt;% group_by(models,param) %&gt;% \n  summarize(spread_CIs=sum(q95 - q5)) %&gt;% \n  knitr::kable(caption=\"Total Variance\")\n\n\nTotal Variance\n\n\nmodels\nparam\nspread_CIs\n\n\n\n\nBayes BRMS\nmu\n71.44307\n\n\nBayes Simple\nmu\n96.11747\n\n\nRRreg\nmu\n37.30712\n\n\nbrms\nbias\n73.93706\n\n\n\n\n\nNow let’s check bias.\n\n#RMSE\n\nover_sims %&gt;% group_by(models,param) %&gt;% \n  summarize(mean_rmse=switch(unique(param),\n                             mu=sqrt(mean((estimates - theta)^2)),\n                             bias=sqrt(mean((estimates - bias)^2)))) %&gt;% \n  knitr::kable(caption = \"RMSE\")\n\n\nRMSE\n\n\nmodels\nparam\nmean_rmse\n\n\n\n\nBayes BRMS\nmu\n0.0419459\n\n\nBayes Simple\nmu\n0.0485183\n\n\nRRreg\nmu\n0.0487269\n\n\nbrms\nbias\n0.0459833\n\n\n\n\nover_sims %&gt;% group_by(models,param) %&gt;% \n  summarize(abs_bias=switch(unique(param),\n                             mu=mean(abs(estimates - theta)),\n                             bias=mean(abs(estimates - bias)))) %&gt;% \n  knitr::kable(caption = \"Mean Absolute Bias\")\n\n\nMean Absolute Bias\n\n\nmodels\nparam\nabs_bias\n\n\n\n\nBayes BRMS\nmu\n0.0341473\n\n\nBayes Simple\nmu\n0.0388322\n\n\nRRreg\nmu\n0.0390247\n\n\nbrms\nbias\n0.0377768"
  },
  {
    "objectID": "post/tunisia_imf/index.html",
    "href": "post/tunisia_imf/index.html",
    "title": "New Survey Research on Tunisia and the IMF Deal",
    "section": "",
    "text": "Tunisia’s dictator, President Kais Saied, is pushing the country towards full-scale economic collapse by refusing to negotiate with the International Monetary Fund about a potential rescue deal. Neither the government nor the increasingly repressed opposition wants to take responsibility for the wrenching changes proposed by the IMF despite the offered loan package of $1.9 billion, such as privatization of state-owned enterprises and reducing public sector salaries. To gain insight into how ordinary Tunisians see this difficult situation, we implemented a national poll in July and August of 880 people using secure online recruitment tools. What we found is that the opinions of rank and file Tunisians are more diverse than their uncooperative leaders, but misunderstandings about the deal abound.\nTunisia is already in effective default as it has failed to pay suppliers of state-subsidized goods, including staples like sugar, flour and olive oil. Salaries to government employees have been delayed repeatedly. International default is looming with big loan repayments by the end of the year. For these reasons, an IMF deal that could avert an economic crisis is not an abstract outcome but rather a solution for a problem that all Tunisians face on a daily basis. Although President Saied’s ministers have negotiated with the IMF and advocated for the deal, he has argued against it in public because it would undermine the country’s sovereignty. As the country’s most popular political leader–albeit perhaps not quite as popular as he was a few years ago–his obstinacy is likely to undermine any negotiated compromise. Since he rejected an IMF deal last October, no further negotiations with the IMF have yet to produce any concrete results.\nOur online poll captures Tunisians’ mixed sentiments towards the IMF. On average, Tunisians give the IMF deal an approval rating of about 43 using a feeling thermometer scale of 1 to 100, indicating opposition but of a moderate kind. In addition, a significant number of Tunisians approve of the IMF deal very strongly. Approximately one in 10 Tunisians rates the deal at 80 or higher out of 100. Moreover, when we asked Tunisians in the poll to explain their support or opposition, we received a range of answers. Some of the rationales matched common refrains in the media, such as fears that the deal would increase poverty and others echoing President Saied’s concerns over the country relinquishing its sovereignty. A not insignificant number, though, expressed beliefs that the deal would be a needed support for the country’s dismal finances and could even improve the political situation.\nThese very mixed feelings could be driven by a lack of quality knowledge about the IMF deal despite its prominence in the media. Only 22 percent of Tunisians report that they understand the negotiations very well, even though 62 percent of the population reports that they hear about the deal at least once a week. When we asked Tunisians to choose the correct elements of the proposed deal from a list of options, they do select some of the core planks, such as privatization of state-owned enterprises and trimming public sector salaries. However, they also select policies that are not a part of the negotiations, especially reducing migration to European countries. While the IMF does not have any preferences about Tunisian immigration policies, the connection between Tunisia’s fiscal collapse and the European officials’ posturing about the “dangers” of immigration have connected the two issues in the minds of many Tunisians.\nAnother hindrance to apprehending the IMF negotiations, of course, is that Tunisian politicians in both the government and the opposition are competing to oppose the deal rather than trying to prepare the population for what may have to happen in the future. We examined how prone people are to partisanship by exposing a subset of respondents to messages from popular politicians, including President Saied and the labor union UGTT, that expressed opposition to the deal. The message, which said that the deal could “compromise Tunisia’s economic and political stability”, statistically increased concerns over violations of Tunisia’s sovereignty by the IMF, though the effect was noticeably concentrated among President Saied supporters. For these partisans, we only needed to remind them of President Saied’s opposition to reduce their support for the deal.\nFurthermore, even though President Saied’s high-minded rhetoric has yet to yield progress on the economic front, Tunisians do not appear to hold him responsible. When we asked which political and social actors Tunisians believe are most responsible for the “Tunisian government’s high government debts”, the largest number (over 30 percent) chose the Islamist party Ennahda, even though this group has been out of office since Saied’s coup in the summer of 2021. Tunisian respondents were also more likely to blame the powerful labor union UGTT that represents public sector workers than Saied. Although the president is certainly facing pressure for lack of success in easing the economic crisis, many Tunisians still do not see him as the primary reason for their economic problems–at least when compared to other players.\nAt the same time, our polling points to some ways that the IMF and other supporters of the deal could improve its messaging to Tunisians. First, younger Tunisians are much less likely to report hearing about the deal or understanding it. Only 20 percent of Tunisians in their twenties report hearing about the IMF deal on a daily basis compared with more than 50 percent of Tunisians in their sixties. Similarly, roughly twice as many men than women report hearing about the deal on a daily basis. Strategically focusing messaging on younger Tunisians and women could help reduce some of the misunderstanding and lack of clarity about what the deal entails. Furthermore, the IMF would do well by clarifying that immigration levels are not a precondition for Tunisia to sign a deal.\nUltimately, the fate of the deal lies in the hands of Tunisia’s political leaders, but popular pressure could help move the needle on the government’s tepid progress in negotiations. After all, the more complex reactions from Tunisians offer more opportunities for building popular support than the strong disapproval offered by the country’s fractured leadership."
  },
  {
    "objectID": "post/flat_earth/index.html",
    "href": "post/flat_earth/index.html",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "",
    "text": "Perhaps no other subject in applied statistics and machine learning has caused people as much trouble as the humble logit model. Most people who learn either subject start off with some form of linear regression, and having grasped it, are then told that if they have a variable with only two values, none of the rules apply. Horror of horrors: logit is a non-linear model, a doom from which few escape unscathed. Interpreting logit coefficients is a fraught exercise as not only applied researchers but also statisticians have endless arguments about how this should be done.\nIn this post I de-mystify logit—and show why it’s such a useful model—by using maps. Yes, non-linear spaces can be tricky to understand, but as humans we can think reasonably well in up to three dimensions. If you know what the difference between a map and a globe is, then you can run and interpret logit just fine.\nI’m also going to argue based on this analogy that people who assiduously avoid logit are not so dissimilar from flat earthers. It’s true that moving from a map to a globe induces additional complications, but the alternative is run into significant problems in navigation. Similarly, applying the workhouse OLS model to a binary outcome isn’t wrong, but it can lead you astray when you want to figure out the distance between two points. Once you read this post, I hope you’ll see why logit is not all that complicated and should be the preferred model with a binary outcome."
  },
  {
    "objectID": "post/flat_earth/index.html#b-is-for-bernoulli",
    "href": "post/flat_earth/index.html#b-is-for-bernoulli",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "B Is For Bernoulli",
    "text": "B Is For Bernoulli\nI’ll start first with a review of what a logit model actually is as there is some confusion on this score. Logit is the wrong name for what most people are estimating; the model is in fact the Bernoulli distribution. This distribution has the following form for a variable \\(Y\\) that takes on two values, i.e. 0 or 1:\n\\[\nP(Y = k) = \\begin{cases} p, & \\text{if } k=1, \\\\1-p, & \\text{if } k=0. \\end{cases}\n\\]\nAs soon as the math appears, of course, it looks complicated. This formula is actually the simplest statistical distribution once you break it down. What it says is that, on average, the probability that a variable \\(Y\\) is equal to 1 is determined by the parameter \\(p\\), which represents a probability or a proportion. Suppose that \\(Y\\) has 10 values, 30% of which are zeroes (0s) and 70% of which are 1s. In that case, \\(p\\) would be equal to 0.7 or 70% because 70% of the time \\(Y\\) is equal to 1. The probability of 0s is then \\(1-p\\) or 100% - 70% = 30%. Easy peezy.\nWhat is important to remember, though, is that \\(p\\) is a proportion, so it can’t be greater than 1 or less than 0. (That’s why the Bernoulli and Beta distributions are connected, which I discuss in my other post on the topic). You can’t have \\(Y\\) be equal to 1 more than 100% of the time or less than 0% of the time; that’s nonsensical.\nYou’ve probably noticed that so far we haven’t mentioned the word “logit” at all. Where does this tricky little monster come in?\nWell if all we want to do is calculate the average proportion of 1s in \\(Y\\) then all we need is \\(p\\). But what if we want to see if there is a relationship between some covariate \\(X\\), say how old a person is, and a binary variable like \\(Y\\)? For example, suppose we want to know if older people are more likely to vote for Biden: voting can take on two values, either voting (1) or not voting (0). As an example, I’ll generate some plausible-looking age data for a bunch of citizens that we’ll call our covariate \\(X\\):\n\nX &lt;- rpois(1000, 45)\nggplot(tibble(x=X),aes(x=x)) + geom_histogram() + theme_barbie()\n\n\n\n\n\n\n\n\nWe see that the average person in our data is about 45 years old and the age range is between roughly 20 and 60. What we want to understand is whether on average older (or younger) people tend to vote more for Biden. But we have a problem - \\(p\\) in the Bernoulli distribution cannot be any bigger than 1 or less than 0, and our age variable \\(X\\) goes from 20 to 70. Yikes! We can’t just pop \\(X\\) into \\(p\\) and without going way outside that range.\nIf you take a look at \\(X\\), you might think, “OK, well, just divide \\(X\\) by 100 and it’ll be between 0 and 1.” Great thought! Let’s do that and plot \\(X\\) again:\n\nggplot(tibble(x=X/100),aes(x=x)) + geom_histogram() + theme_barbie()\n\n\n\n\n\n\n\n\nWe just squashed \\(X\\) to be small enough that we could just pop it into the Bernoulli distribution:\n\\[\nP(Y = k) = \\begin{cases} X, & \\text{if } k=1, \\\\1-X, & \\text{if } k=0. \\end{cases}\n\\]\nHere we simply replaced \\(p\\) with our covariate \\(X\\). Very nice! We can even generate random data using a simple algorithm from this function:\n\nDraw a random number between 0 and 1.\nIf the random number is less than \\(X\\), set \\(Y\\) equal to 1, and 0 otherwise.\n\nHere’s what our random data \\(Y\\) look like compared to \\(X\\):\n\nY &lt;- as.numeric(runif(length(X))&lt;(X/100))\n\nggplot(data=tibble(Y,X),aes(y=Y,x=X/100)) +\n  geom_point(alpha=0.5) +\n  stat_smooth() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nWhat our plot shows is that when X is high, we tend to get more 1s in \\(Y\\), and when \\(X\\) is low, we tend to get more 0s. The blue line shows what the average value of \\(Y\\) is for a given age. When age is quite high, about 0.65 or 65 years, then \\(Y\\) is about 0.65 or 65% on average (as we expect given the 1:1 relationship).\nIf you look at this formula, though, you might think something is a bit off. Do we think that someone’s age as a fraction will be exactly equal to their probability of voting for Biden? Will voters who are 70 years old have an exactly 70% chance of voting for him? We only have one option for describing that relationship, and a 1:1 correspondence seems very limiting. So what do we do if we think this relationship is more nuanced?\nTo allow the relationship to be less exact, we’ll take a page out of linear regression and add a coefficient to multiply \\(X\\), which we’ll call \\(\\beta\\). We’ll add \\(\\beta\\) to our little formula:\n\\[\nP(Y = k) = \\begin{cases} \\beta X, & \\text{if } k=1, \\\\1-\\beta X, & \\text{if } k=0. \\end{cases}\n\\]\nNow let’s say \\(\\beta\\) is equal to 0.5. Then, the probability that someone will vote for Biden if they are 70 years old is only 35%. If \\(\\beta\\) goes up, then that relationship would be stronger. Adding a coefficient allows us to express the relationship between \\(X\\) and \\(Y\\) with a lot more nuance.\nYou might see some further issues, though. What if \\(\\beta\\) is equal to 0? In that case, the probability that someone votes for Biden is also equal to exactly 0%. In probability world, that’s not a good thing—we can’t be 100% certain that someone will or will not vote for Biden. We’ll get around this problem by adding another parameter called the intercept, \\(\\alpha\\):\n\\[\nP(Y = k) = \\begin{cases} \\alpha + \\beta X, & \\text{if } k=1, \\\\1-(\\alpha + \\beta X), & \\text{if } k=0. \\end{cases}\n\\]\nThis is a handy dandy trick to allow \\(\\beta\\) to equal 0 without having the probability of \\(Y\\) to be equal to 0 as well. Say \\(\\alpha\\) is 0.35–then if \\(\\beta\\) is 0, 35% of people would still vote Biden. \\(\\alpha\\) could be thought of as a baseline rate—without considering the effect of age, how many people vote for Biden? Once we add \\(\\alpha\\), this equation is the same as the slope of a line (good ol’ \\(mx + b\\)), which makes it a linear model (linear means a straight line).\nWe now have a linear model we want to stick in \\(p\\) to model the relationship between age (\\(X\\)) and voting for Biden (\\(Y\\)). We’ve made progress, but we’re still gonna have issues squashing \\(X\\) into a proportion. The trick is that we need \\(\\alpha + \\beta X\\) to always output a number between 0 and 1. We can’t have a proportion greater than 1 or less than 0. For these reasons, to use this model we have to pick specific values for \\(\\alpha\\) and \\(\\beta\\) that allow us to keep within those bounds. For example, we could set \\(\\alpha\\) to 0.1 (a baseline rate of 10% of 1s) and \\(\\beta\\) to 0.5. Let’s simulate some data with those numbers and plot it:\n\nY &lt;- as.numeric(runif(length(X))&lt;(0.1 + .5*(X/100)))\n\nggplot(data=tibble(Y,X),aes(y=Y,x=X/100)) +\n  geom_point(alpha=0.5) +\n  stat_smooth() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nWe now have a much weaker relationship between age and voting: voting for Biden increases a bit as people age, but not much. But we do now have a much more nuanced relationship than we started out with by using the divide by 100 method. Very cool!\nHowever, you might be wondering—what if we want even more diverse relationships? What if \\(\\beta\\) were greater than 1? Or 3? Or 10? Let’s plot what happens when we set \\(\\alpha\\) = 0.1 and \\(\\beta\\)=3:\n\nY &lt;- as.numeric(runif(length(X))&lt;(0.1 + 3*(X/100)))\n\nggplot(data=tibble(Y,X),aes(y=Y,x=X/100)) +\n  geom_point(alpha=0.5) +\n  stat_smooth() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nYikes! Now we’re only getting values for \\(Y\\) that are equal to 1. What happened to all the 0s? When we multiply \\(X\\) by 3, we now get values way bigger than 100%. For an age of 50, \\(X/100\\) = 0.5, and \\(3\\cdot X=1.5\\) or 150%. Shucks! This means we can’t just pick whatever values we want for \\(\\beta\\) or \\(\\alpha\\). That’s frustrating especially if we think that relationship could be really strong and we want \\(\\beta\\) to be large."
  },
  {
    "objectID": "post/flat_earth/index.html#not-all-heroes-are-linear",
    "href": "post/flat_earth/index.html#not-all-heroes-are-linear",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "Not All Heroes Are Linear",
    "text": "Not All Heroes Are Linear\nWe now have two options. We can keep on guessing what values might work for \\(\\alpha\\) and \\(\\beta\\), or… we could get a map. A map that would allow us to translate or transform our linear model \\(\\alpha + \\beta X\\) into the cramped space of \\(p\\). Like the teleporter in Star Trek, we need something that can move our value of \\(\\alpha + \\beta X\\) to the right spot on \\(p\\) without going over 1 and under 0.\nThis is where our good friend logit comes in. Logit is a function that can take in any number–literally any number–and magically transform it to a number that is strictly between 0 and 1. It’s a magical map that let’s us move from the world of ages and to the world of proportions between 0 and 1. NB: Technically this is not the logit function but rather the inverse logit function. But everyone calls the model “logit” so I’ll just use that term.\nThe function itself is a bit ugly, but I’ll include it here along with our linear model as the input and \\(p\\) as the output:\n\\[\np = \\frac{1}{1 + e^{-(\\alpha + \\beta X)}}\n\\]\nI won’t spend a lot of time on explaining the function other than to note that it has the number \\(e\\), which is where a lot of the magic comes from. We can test whether it can really do these transformations by plotting a range of numbers for our linear model and seeing what the logit function spits out:\n\nbeta &lt;- .2\nalpha &lt;- -10\n\nlogit &lt;- function(x)  { 1 / (1 + exp(-(x))) }\n\ntibble(linear_model=alpha + beta * X) %&gt;% \n  mutate(p=logit(linear_model)) %&gt;% \n  ggplot(aes(y=p, x=linear_model)) +\n  geom_point() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nOn the bottom you can see all the values for our linear model when \\(\\alpha=-10\\) and \\(\\beta=0.2\\). Across the range of ages, the linear model runs from -2.5 to 2.5 or way outside the bounds of \\(p\\). But our logit function spits out a number for \\(p\\) that is between 0 and 1. It also has this cool bend-y shape, or what’s known as a sigmoid. Basically, the line is straight or linear right around the midpoint of \\(p\\) or 0.5. When it gets close to the end points of \\(p\\), or proportions that are very high or very low, it starts to bend like a driver avoiding a barrier in a car.\nIf you notice, though, while the line curves, it never changes the direction. When the linear model goes up, so does \\(p\\). That’s why we can use this function–we will learn about the direction and magnitude of the relationship between age and voting. We just learn that information while respecting the bounds of proportions and the full range of our covariates. Pretty cool!"
  },
  {
    "objectID": "post/flat_earth/index.html#a-whole-new-world",
    "href": "post/flat_earth/index.html#a-whole-new-world",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "A Whole New World",
    "text": "A Whole New World\nOften people get confused or suspicious at this point. What’s up with this weird S-curve thing? Linear models are nice and straight. Why do we need the bend-y thing?\nTo understand this, we can return back to our analogies of maps. The maps that we use are all two-dimensional: we can put them on a table and they lie flat. However, we know that they aren’t completely accurate because the Earth is not flat, it’s round. A globe is the truly accurate representation of the Earth. A flattened map is going to inevitably cause distortions.\nAs a result, when we trace a flight path on a flat map, the plane’s journey is always curved as in the picture below:\n\nThis flight path–which happens to be for a very long flight from Paris to Tahiti–seems bizarrely roundabout on the map. You would think that the straightest path between Paris and Tahiti would be a straight line. This would be true if the Earth were flat, but it’s not. The Earth is round, and the map is a linear projection of a 3-D object onto a 2-D surface. When we do that and calculate the most direct route, we end up with this weird bend-y shape: just like our logit plot above.\nBy the same analogy, our linear model is our simplified version of reality that allows us to understand the relationship between \\(X\\) and \\(Y\\). Logit is the transformation that takes us from this simplified world to the real world of \\(Y\\) via \\(p\\). It does that by warping the space that the straight line of our linear model travels. To see this, let’s take another look at our logit plot:\n\nbeta &lt;- .2\nalpha &lt;- -10\n\nlogit &lt;- function(x)  { 1 / (1 + exp(-(x))) }\n\ntibble(linear_model=alpha + beta * X) %&gt;% \n  mutate(p=logit(linear_model)) %&gt;% \n  ggplot(aes(y=p, x=linear_model)) +\n  geom_point() +\n  theme_barbie()\n\n\n\n\n\n\n\n\nWhen the linear model is 0–or \\(p\\) is around 0.5–the logit function allows \\(p\\) to change relatively quickly as the linear model changes. However, when the linear model gets very low or very high–i.e., \\(p\\) is close to 0 or 1–the logit function only lets \\(p\\) change slowly as the linear model changes. To get from a \\(p\\) of 0.1 to almost 0 requires the linear model to move about -2 points on the \\(x\\) axis, but to get from a \\(p\\) of 0.5 to a \\(p\\) of 0.25 only requires a movement of about -0.5 in the linear model space.\nThe magical part of logit is that it creates more space as the linear model gets very big or very small. The linear model keeps moving at the same rate (\\(\\beta\\)) but \\(p\\), the proportion, does not. \\(p\\) slows down as though more and more space kept appearing. Like Jim Carrey in the Truman Show who can never escape from his artificial world, the linear model can never escape the bounds of \\(p\\). The closer it gets to the bounds, the farther it has to travel–the more warped the linear model space becomes relative to \\(p\\). In other words, at the edges the linear model map is less accurate and so we see a stronger bend in the logit function to compensate."
  },
  {
    "objectID": "post/flat_earth/index.html#flat-earthers-and-the-logit-function",
    "href": "post/flat_earth/index.html#flat-earthers-and-the-logit-function",
    "title": "Lost in Transformation: The Horror and Wonder of Logit",
    "section": "Flat Earthers and the Logit Function",
    "text": "Flat Earthers and the Logit Function\nWhat is logit then? A handy dandy scaling function that maps a linear model to a proportion between 0 and 1. That’s all it does. The actual distribution is the Bernoulli (though sadly, no one calls it that). A logit function isn’t special—there are other similar functions, just as there are a lot of map coordinate transformations (talk to a GIS person sometime; it’ll bore you to tears).\nThere are people who really don’t like logit. Usually it’s something to do with how the function complicates things and makes these bend-y shapes. It does require some thinking as we have to think a bit about what our linear model coefficient \\(\\beta\\) means. The linear model is a straight line, but the space it is passing through changes as \\(p\\) changes. What that means substantively is that \\(\\beta\\), or the effect of age on the proportion voting for Biden, will vary depending on how big or small the proportion voting for Biden is. If that proportion is small, a big value of \\(\\beta\\) will result in much smaller movement in voting than when the proportion voting is closer to 50%.\nIt is this bend-y (or non-linear) nature of logit that leads some people to just ignore the Bernoulli and use a linear model. That is an easy way to deal with the problem, but unfortunately, we cannot simply ignore this transformation. Like flat earthers, if we ignore the fact that the Earth is round, we won’t be able to calculate distances accurately. If we have a linear model that can get larger or smaller than 0% or 100%, then we will end up with nonsensical predictions from our linear model like a plane that ends up at the wrong airport because the pilot believed the Earth was flat.\nSo how do you interpret a logit model coefficients? There are two ways once you understand what the logit function does:\n\nLeave it as it is. The coefficient \\(\\beta\\) is the relationship of the covariate (\\(X\\)) to the outcome (\\(Y\\)) in the linear or “flat” space. The larger \\(\\beta\\) is, the stronger the relationship is between the covariate and the outcome. No, we don’t know exactly how much the proportion will change as \\(X\\) changes, but we can still interpret \\(\\beta\\) as expressing what that change looks like in the simple space of the linear model.\nConvert it to changes in \\(Y\\) via the proportion \\(p\\). Just like a plane figuring out distances using transformation of coordinates, we can figure out the average change in a proportion \\(p\\) by averaging (or marginalizing) over different logit functions for a given \\(\\beta\\). I won’t go into the technical details, but we have very handy R packages to calculate what are called marginal effects: how much does the “real world” \\(p\\) change as our linear model coefficient \\(\\beta\\) changes?\n\nThere are plenty of vignettes about how to calculate marginal effects on the R package marginaleffects website. Here I’ll show how to calculate a marginal effect of \\(X\\) on \\(Y\\) with our linear model coefficients of \\(\\alpha=-10\\) and \\(\\beta=0.2\\). To do so I’ll first simulate data with these values and then fit a logit (i.e. Bernoulli) model with the R function glm:\n\n# simulate our outcome Y given our linear model parameters\n\nY &lt;- as.numeric(runif(length(X))&lt;logit(alpha + beta*X))\n\n# make a dataset and fit the model for Y and X\n# note that glm says the family is \"Binomial\"\n# Bernoulli is a special case of the Binomial\n# and our link (scaling) function is logit!\n\nsim_data &lt;- tibble(Y=Y, X=X)\nlogit_fit &lt;- glm(Y ~ X,data=sim_data,family=binomial(link=\"logit\"))\n\nsummary(logit_fit)\n\n\nCall:\nglm(formula = Y ~ X, family = binomial(link = \"logit\"), data = sim_data)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2091  -0.7986  -0.4686   0.8678   2.5201  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -9.78933    0.70117  -13.96   &lt;2e-16 ***\nX            0.19578    0.01484   13.20   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1250.71  on 999  degrees of freedom\nResidual deviance:  998.47  on 998  degrees of freedom\nAIC: 1002.5\n\nNumber of Fisher Scoring iterations: 5\n\n\nAbove we see the output of the glm command that shows us the values of \\(\\alpha\\) (the intercept, as noted in the command output) and \\(\\beta\\), which is listed as the Estimate for X. These are the values of these parameters in the linear, or simple, space. There is nothing wrong with these coefficients: they tell you how much \\(X\\) is changing with respect to \\(Y\\) in the linear space. How much actual change in \\(Y\\) happens, though, depends on exactly where we are (or how high or low the proportion of 1s is in \\(Y\\)).\nIf we want to convert these relationships to the distances/effects in the outcome \\(Y\\) (i.e. changes in the proportion \\(p\\)), we can use the avg_slopes function from marginaleffects:\n\nlibrary(marginaleffects)\n\navg_slopes(logit_fit,variables=\"X\")\n\n\n Term Estimate Std. Error    z Pr(&gt;|z|)  2.5 % 97.5 %\n    X   0.0324    0.00158 20.5   &lt;0.001 0.0293 0.0355\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high \n\n\nNote that this Estimate is way smaller than the Estimate for \\(\\beta\\) from the glm command. That estimate was from the linear space and so is much bigger. The marginaleffects command is telling us how much \\(p\\), or the proportion of 1s in \\(Y\\), changed as our covariate \\(X\\) increased. It did this by averaging over different logit functions given changes in \\(X\\) (you can see the package info for more technical details). On average, as age increased by 1 year, the proportion voting for Biden increased by 0.0325 or 3.25% with a confidence interval of (2.93%, 3.56%).\nVery cool! Now that you understand logit models and how to interpret linear model coefficients in both the linear space and via marginal effects, you know pretty much everything you need to know about this wonderful little function logit. Next time you have a binary outcome like voting for Biden, or anything else, give logit a go! It’s your map to a whole new world.\n\nAll By Myself Jasmine GIFfrom All By Myself GIFs"
  },
  {
    "objectID": "post/astrazeneca/index.html",
    "href": "post/astrazeneca/index.html",
    "title": "Why People Are Doubting the AstraZeneca Vaccine Report",
    "section": "",
    "text": "In this blog post, I use Gelman and Loken’s garden of forking paths analysis to construct a simulation showing why skepticism of AstraZeneca’s vaccine results is warranted at this early stage. This simulation uses the numbers from their press release to illustrate how noise in treatment regimes can undermine serendipitous results. As Gelman and Loken describe, researchers can stumble on conclusions which they are then very able to provide post hoc justifications for. Unfortunately, when incentives to get results are strong, human beings are also very likely to focus on the positive at the expense of obtaining the full picture.\nSimilar to my last post on Pfizer’s vaccine, I start with Eric Novik’s excellent blog post on how to calculate the relevant statistics for the vaccine, i.e. vaccine efficacy (VE). This is defined as:\n\\[\nVE = 1 - \\frac{p_t}{p_c}\n\\]\nWhere \\(p_t\\) is the proportion of cases in the treatment (vaccinated) group with COVID-19 and \\(p_c\\) is the proportion of cases in the control (un-vaccinated) group). In essence, if we assume the vaccinated group will have no more cases than the control group, this statistic will converge to 1 as \\(p_t\\) goes to zero, so VE of 100% would be a case in which there are no cases in the treatment group.\nThey don’t give us all the information to figure out how many people were infected with COVID-19 in treatment versus control, but we can infer \\(p_t\\) and \\(p_c\\) by solving two equations given the fact that we know that the proportion infected in the whole trial was equal to \\(\\frac{131}{11636}\\) and the overall VE was 0.7:\n\\[\\begin{align}\n1 - \\frac{p_t}{p_c} &= 0.7\\\\\np_t + p_c &= \\frac{131}{11636}\n\\end{align}\\]\nThankfully, this system has a single solution where \\(p_t\\) is equal to (~0.0026) and \\(p_c\\) is equal to (~0.0087). In other words, the proportion infected in the treatment group was about four times less than the control group. We’ll assume that these distributions are the correct ones, and then sample subgroup-varying \\(p_{ti}\\) and \\(p_{ci}\\) from Beta distributions that are fairly tight around these values. This will allow us to model treatment heterogeneity within the sample. The distribution of possible VEs given subgroup heterogeneity can be seen in the plot below:\n\n# use the mean/variance parameterization of the beta distribution\n\npt &lt;- 393/151268\npc &lt;- 655/75634\n\n# Generate values for VE given beta distributions for pt/pc\n\nVE &lt;-  1 - rbeta(10000,pt*5000,(1-pt)*5000)/rbeta(10000,pc*5000,(1-pc)*5000)\n\nhist(VE)\n\n\n\n\n\n\n\n\nThis plot shows that the true VE in the population is on average 0.7 but could vary substantially. It could be below 0.5 for a small subset of the population and above 0.9 for a small subset of the population. To simulate our data, we will draw VEs from this beta distribution separately for each of four possible subgroups \\(i \\in \\{1, 2, 3, 4\\}\\) in a hypothetical study:\n\\[\nVE_i \\sim 1 - \\frac{Beta(5000p_{ti},5000(1-p_{ti}))}{Beta(5000p_{ci},5000(1-p_{ci}))}\n\\]\nFour subgroups are chosen to be roughly equal to the size of the half-dose group in the AstraZeneca study given a sample size of 11,636. I add a further step in that I assume that the probability of VE being reported for a subgroup \\(i\\), which I term \\(Pr(r=1)\\), is increasing in the rate of VE. That is, as VE rises, the subgroup analysis is more likely to be reported. For these reasons, we can distinguish between the full range of VE estimates for a subgroup \\(i\\), \\(VE_i\\), and the particular subgroup \\(i\\) that is reported, \\(VE_{ir=1}\\):\n\\[\nPr(VE_{ir}) = Pr(r|VE_i)VE_{ir=1} + (1 - Pr(r|VE_i))VE_{ir=0}\n\\]\nThis equation shows that the extent of the bias in using only reported results is equal to \\((1 - Pr(r|VE_i))VE_{ir=0}\\), or the probability that a result won’t be reported times the level of VE in the non-reported subgroups.\nThe R code to generate this model in terms of treatment/control COVID cases and whether or not subgroup analyses are reported is as follows:\n\n# number of samples\n\nN &lt;- 1000\n\nsim_data &lt;- lapply(1:N,function(n) {\n  \n  # generate subgroup treatment/control COVID proportions\n  \n  sub_pt &lt;- rbeta(4,pt*5000,(1-pt)*5000)\n  sub_pc &lt;- rbeta(4,pc*5000,(1-pc)*5000)\n  \n  \n  # generate subgroup VEs\n  \n  VE &lt;-  1 - sub_pt/sub_pc\n  \n  # generate COVID case data with binomial distribution\n  \n  covid_t &lt;- sapply(sub_pt, function(pt) rbinom(n=1,size = floor(11636/4),prob=pt))\n  covid_c &lt;- sapply(sub_pc, function(pc) rbinom(n=1,size = floor(11636/4),prob=pc))\n  \n  # output data\n  \n  tibble(draw=n,\n         groups=1:4,\n         sub_pt=sub_pt,\n         sub_pc=sub_pc,\n         VE=VE,\n         VE_r=as.numeric(runif(n=4)&lt;plogis(-250 + 300*VE)),\n         covid_t = covid_t,\n         covid_c=covid_c)\n  \n}) %&gt;% bind_rows\n\nGiven this simulation, only a minority (4%) of subgroup analyses are reported. The average VE for the reported subgroups is 77% as opposed to a VE of 69% across all simulations. As such, the simulation assumes that it is unlikely that subgroups with low vaccine efficacy will end up being reported, while subgroups with stronger efficacy are more likely to be reported.\nFor each random draw from the simulation, I can then fit a model that analyses only those analyses that are reported:\n\n# modified code from Eric Novik\n\nvac_model &lt;- \"data {\n  int g; // number of sub-groups\n  array[g] int&lt;lower=0&gt; r_c; // num events, control\n  array[g] int&lt;lower=0&gt; r_t; // num events, treatment\n  array[g] int&lt;lower=1&gt; n_c; // num cases, control\n  array[g] int&lt;lower=1&gt; n_t; // num cases, treatment\n}\nparameters {\n  array[g] real&lt;lower=0, upper=1&gt; p_c; // binomial p for control\n  array[g] real&lt;lower=0, upper=1&gt; p_t; // binomial p for treatment \n}\ntransformed parameters {\n  real VE = mean(1 - to_vector(p_t) ./ to_vector(p_c));  // average vaccine effectiveness across groups\n}\nmodel {\n  p_t ~ beta(2, 2); // weakly informative,\n  p_c ~ beta(2, 2); // centered around no effect\n  r_c ~ binomial(n_c, p_c); // likelihood for control\n  r_t ~ binomial(n_t, p_t); // likelihood for treatment\n}\ngenerated quantities {\n  vector[g] effect   = to_vector(p_t) - to_vector(p_c);      // treatment effect\n  vector[g] log_odds = log(to_vector(p_t) ./ (1 - to_vector(p_t))) -\n                           log(to_vector(p_c) ./ (1 - to_vector(p_c)));\n}\"\n\nto_stan &lt;- cmdstan_model(write_stan_file(vac_model))\n\n\n\nest_bias &lt;- lapply(unique(sim_data$draw), function(i) {\n  \n  sink(\"output.txt\")\n  \n  this_data &lt;- filter(sim_data,draw==i,VE_r==1)\n  \n  stan_data &lt;- list(g=length(unique(this_data$groups)),\n                    r_c=as.array(this_data$covid_c),\n                    r_t=as.array(this_data$covid_t),\n                  n_c=as.array(round(rep(floor(11636/4),\n                                length(unique(this_data$groups)))/2)),\n                  n_t=as.array(round(rep(floor(11636/4),\n                                length(unique(this_data$groups)))/2)))\n  \n  if(stan_data$g&lt;1) {\n    tibble(draw=i,\n           VE=NA)\n     \n  } else {\n    est_mod &lt;- to_stan$sample(data=stan_data,\n                              seed=624018,\n                              chains=1,iter_warmup=500,iter_sampling=1000,refresh=500)\n  \n    draws &lt;- est_mod$draws() %&gt;% as_draws_df\n  \n    tibble(draw=i,\n         VE=draws$VE)\n  }\n  \n  sink()\n  \n}) %&gt;% bind_rows\n\nI can then plot the density of the estimated reported VE from this simulation along with a line indicating the true average VE in the population:\n\nest_bias %&gt;% \n  ggplot(aes(x=VE)) +\n  geom_density(fill=\"blue\",alpha=0.5) +\n  geom_vline(xintercept=mean(sim_data$VE),linetype=2) +\n  theme_tufte() +\n  xlim(c(0,1)) +\n  xlab(\"Reported VE\") +\n  ylab(\"\") +\n  annotate(\"text\",x=mean(sim_data$VE),y=3,label=paste0(\"True VE = \",round(mean(sim_data$VE)*100,1),\"%\"))\n\n\n\n\n\n\n\n\nAs can be seen, it is much more likely that VEs higher than the true average VE will be reported. The extent of the bias is driven by the simulation and how much weight the simulation puts on discovering high VE values. Given the profit that AstraZeneca stands to gain, and the fame and prestige for the involved academics, it would seem logical that reported analyses from subgroups will tend to be subgroups that out-perform the average.\nNote that this simulation shows how AstraZeneca could be reporting valid statistical results, yet these results can still be a biased estimate of what we want to know, which is how the vaccine works for the population as a whole. The possibility of random noise in subgroups and the fact that subgroups are likely to respond differently means that we should be skeptical of analyses that only report certain subgroups instead of all subgroups. Only when we understand the variability in subgroups can we say whether the reported finding in AstraZeneca’s press release represents a real break-through or simply luck. Of course, they can test it directly by doing another trial, which it seems to be is their intention. Serendipity, though, isn’t enough of a reason to trust these results unless we can examine all of their data."
  },
  {
    "objectID": "post/fixed_effects/index.html",
    "href": "post/fixed_effects/index.html",
    "title": "What Panel Data Is Really All About",
    "section": "",
    "text": "Interested in more social science on contemporary issues? Check out my just-released book with Cambridge University Press, and use discount code KUBINEC23 to get 20% off.\nWe’ve all been in that seminar where the author puts up a slide containing regression coefficients, and buried in the bottom line of the table we can see little Ys and Ns indicating the kind of panel data model employed. Quite often, these brief descriptions of models are taken as a mere statistical ritual, believed to be efficacious at appeasing the mercurial statistical deities, but rarely if ever investigated more thoroughly. Furthermore, no one wants to be in the difficult position of discussing panel data models, which inevitably boils down to a conversation laced with econometric terms generating more heat than light.\nSo what if I told you … panel data, or data with two dimensions, such as repeated observations on multiple cases over time, is really not that complicated. In fact, there are only two basic ways to analyze panel data, which I will explain briefly in this piece, just as every panel dataset has two basic dimensions (cases and time). However, when we confuse these dimensions, bad things can happen. In fact, one of the most popular panel data models, the two-way fixed effects model–widely used in the social sciences–is in fact statistical nonsense because it does not clearly distinguish between these two dimensions. This statement should sound implausible to you–really?, but it’s quite easy to demonstrate, as I’ll show you in this post.\nThis blog post is based on a recent publication with Jonathan Kropko which you can access here. In this post I provide a more reader-friendly overview of the article, dropping the academic-ese and focusing on substance as I think there are important issues for many people doing research.\nIn short: there are an untold number of analyses of panel data affected by an issue that is almost impossible to identify because R and Stata obscure the problem. Thanks to multi-collinearity checks that automatically drop predictors in regression models, a two-way fixed effects model can produce sensible-looking results that are not just irrelevant to the question at hand, but practically nonsense. Instead, we would all be better served by using simpler 1-way fixed effects models (intercepts on time points or cases/subjects, but not both)."
  },
  {
    "objectID": "post/fixed_effects/index.html#how-we-think-about-panel-data",
    "href": "post/fixed_effects/index.html#how-we-think-about-panel-data",
    "title": "What Panel Data Is Really All About",
    "section": "How We Think About Panel Data",
    "text": "How We Think About Panel Data\nPanel data is one of the richest resources we have for observational inference in the social and even natural sciences. By comparing cases to each other as they change over time, such as countries and gross domestic product (GDP), we can learn much more than we can by only examining isolated cases. However, researchers for decades have been told by econometricians and applied statisticians that they must use a certain kind of panel data model or risk committing grave inferential errors. As a result, probably the most common panel data model is to include case fixed effects, or a unique intercept (i.e. a dummy variable) for every case in the panel data model. The second most likely is to include a fixed effect or intercept for every time point and case in the data. The belief is that including all these intercepts will control for omitted variables because there are factors unique to cases or time points that will be “controlled for” by these intercepts.\nThe result of this emphasis on the lurking dangers of panel data inference results in what I have decided to call the Cheeseburger Syndrome. This phenomenon was first studied by Saturday Night Live in a classic 1980s skit:\n\n\nApplied researchers are in the position of customers in the video, continually asking for models that will analyze the variation in their panel data which actually exists. Applied statisticians are often the cooks, informing customers that regardless of what particular dataset they have, they will only ever be able to get a “cheeseburger.” As a result, there is remarkable uniformity of application of panel data models in the social sciences, even if these models don’t always fit the data very well.\nWhat we put forward in our piece linked above is that any statistical model should have, as its first requirement, that it match the researcher’s question. Problems of omitted variables are important, but necessarily secondary. It does not matter how good the cheeseburger is if the researcher really wants eggs easy over.\nIn addition, fixed effects models do not control for omitted variables. What fixed effect models do is isolate one dimension of variance in the model. As a result, any variables that don’t vary on that dimension are by definition removed from the model. This side-effect is trumpeted as the great inferential benefit of fixed effect models, but it has nothing to do with inference. Fixed effects (or their cousin, random effects/hierarchical models) are simply about selecting which part of the panel dataset is most germane to the analysis.\nThe rule of thumb that we put forward in our paper is that fixed effects/dummy variables/intercepts on cases correspond to the following research question:\n\nHow much does a case change in relation to itself over time?\n\nWhile fixed effects/dummy variables/intercepts on time points correspond to the following research question:\n\nHow much does a case change relative to other cases?\n\nSome questions are fruitfully asked when comparing cases to themselves over time. For example, if a case is a human being, we might want to know whether obtaining more education leads to higher earnings. Conversely, if a case is a country, we might want to know if wealthier countries tend to be more democratic than poorer countries. Some questions primarily employ within-case variation, while others look at cross-sectional variation. Both are present in any panel dataset, and both are potentially interesting.\nIf fixed effects enable comparisons, then what happens if we have dummy variables/intercepts for every case and time point (the so-called two-way fixed effects model)? What comparison are we then making? As it turns out, the answer to this question is not very clear at all. I refer you to the paper above for a full exposition, but in essence, because cases and time points are nested, we end up making comparisons across both dimensions simultaneously, and this is just as obtuse as it sounds. There is no clear research question that matches this model.\nFurthermore, if the original aim was to remove omitted variables, these omitted variables inevitably end up in the estimate again because a two-way estimate necessarily relates to both dimensions of variance simultaneously. As a result, it is not very clear what the point is. The one known use of the model is for difference-in-difference estimation, but only with two time points (see our paper for more detail)."
  },
  {
    "objectID": "post/fixed_effects/index.html#exposition-with-data",
    "href": "post/fixed_effects/index.html#exposition-with-data",
    "title": "What Panel Data Is Really All About",
    "section": "Exposition with Data",
    "text": "Exposition with Data\nThis all may sound to you like a nice story. But how can you really know we are telling the truth? There are plenty of equations in our paper, but there is nothing like being able to see the data. One of the contributions of our paper is to create a simulation for panel data where we can control the within-case and cross-sectional variation separately for the same panel data set. This allows us to compare all of the possible panel data models with the same dataset while including or excluding omitted variables and other issues.\nTo show you how this works, I will generate a dataset with a single covariate in which the effect of that covariate is +1 for within-case comparisons and -3 for cross-sectional comparisons. There is noise in the data, but the effect is the same for all cases and for all cross-sections. The following code simulates fifty cases and fifty time points using our panelsim package (currently available only via Github):\n\n# to install this package, use \n# remotes::install_github(\"saudiwin/panelsim\")\n\nrequire(panelsim)\n\n# generate dataset with fixed coefficients within cases and cross-section\n# no variation in effects across cases or time\n\ngen_data &lt;- tw_data(N=50,\n                    T=50,\n                    case.eff.mean = 1,\n                    cross.eff.mean = -3,\n                    cross.eff.sd = 0,\n                    case.eff.sd = 0,\n                    noise.sd=.25)\n\nBecause there is only one covariate, we can pretty easily visualize the relationships in the cross sectional and within-case variation. First, the value of the outcome/response \\(y\\) is shown for five cases, where one dot represents the value of \\(x\\) for each time point for that case:\n\ngen_data$data %&gt;%\n  filter(case&lt;5) %&gt;% \n  ggplot(aes(x=x,y=y)) +\n           geom_point() +\n           stat_smooth(method=\"lm\") +\n           facet_wrap(~case,scales=\"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs can be seen, there is a consistent positive relationship of approximately +1 between \\(x\\) and \\(y\\) within cases. We can also examine the relationship for the cross-section by subsetting the data to each time point and plotting the cases for five of the time points:\n\ngen_data$data %&gt;%\n  filter(time&lt;5) %&gt;% \n  ggplot(aes(x=x,y=y)) +\n           geom_point() +\n           stat_smooth(method=\"lm\") +\n           facet_wrap(~time,scales=\"free\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThere is a separate and quite distinct relationship in the cross section between \\(x\\) and \\(y\\). Both components are present in the outcome. To find the actual coefficient, we can simply fit linear regression models on the generated data, first with intercepts/dummies for cases:\n\nsummary(lm(y ~ x + factor(case),data=gen_data$data))\n\n\nCall:\nlm(formula = y ~ x + factor(case), data = gen_data$data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8932 -0.1599 -0.0015  0.1682  0.8304 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.773045   0.035151  21.992  &lt; 2e-16 ***\nx               1.002403   0.018501  54.182  &lt; 2e-16 ***\nfactor(case)2  -0.184927   0.049529  -3.734 0.000193 ***\nfactor(case)3   1.466627   0.050043  29.307  &lt; 2e-16 ***\nfactor(case)4  -1.363988   0.049898 -27.335  &lt; 2e-16 ***\nfactor(case)5  -0.884801   0.049660 -17.817  &lt; 2e-16 ***\nfactor(case)6   1.097485   0.049812  22.032  &lt; 2e-16 ***\nfactor(case)7  -0.992114   0.049705 -19.960  &lt; 2e-16 ***\nfactor(case)8  -1.597938   0.050057 -31.922  &lt; 2e-16 ***\nfactor(case)9  -2.126106   0.050445 -42.147  &lt; 2e-16 ***\nfactor(case)10 -0.028781   0.049525  -0.581 0.561205    \nfactor(case)11 -1.171168   0.049831 -23.503  &lt; 2e-16 ***\nfactor(case)12 -0.132703   0.049526  -2.679 0.007423 ** \nfactor(case)13 -0.612141   0.049588 -12.344  &lt; 2e-16 ***\nfactor(case)14  0.866259   0.049705  17.428  &lt; 2e-16 ***\nfactor(case)15 -0.867871   0.049670 -17.473  &lt; 2e-16 ***\nfactor(case)16  0.514860   0.049600  10.380  &lt; 2e-16 ***\nfactor(case)17 -0.633858   0.049621 -12.774  &lt; 2e-16 ***\nfactor(case)18 -0.696507   0.049617 -14.038  &lt; 2e-16 ***\nfactor(case)19  0.806729   0.049680  16.239  &lt; 2e-16 ***\nfactor(case)20 -0.764169   0.049643 -15.393  &lt; 2e-16 ***\nfactor(case)21 -0.597111   0.049591 -12.041  &lt; 2e-16 ***\nfactor(case)22 -1.405450   0.049928 -28.150  &lt; 2e-16 ***\nfactor(case)23  0.261895   0.049546   5.286 1.36e-07 ***\nfactor(case)24  0.973181   0.049738  19.566  &lt; 2e-16 ***\nfactor(case)25 -0.710288   0.049618 -14.315  &lt; 2e-16 ***\nfactor(case)26 -0.718412   0.049631 -14.475  &lt; 2e-16 ***\nfactor(case)27 -0.946765   0.049698 -19.050  &lt; 2e-16 ***\nfactor(case)28 -3.194416   0.051604 -61.902  &lt; 2e-16 ***\nfactor(case)29  0.120342   0.049531   2.430 0.015184 *  \nfactor(case)30 -0.965050   0.049712 -19.413  &lt; 2e-16 ***\nfactor(case)31 -1.108207   0.049794 -22.256  &lt; 2e-16 ***\nfactor(case)32 -0.888764   0.049668 -17.894  &lt; 2e-16 ***\nfactor(case)33 -0.920184   0.049700 -18.515  &lt; 2e-16 ***\nfactor(case)34  0.588541   0.049613  11.863  &lt; 2e-16 ***\nfactor(case)35 -0.008302   0.049525  -0.168 0.866881    \nfactor(case)36 -1.068951   0.049757 -21.484  &lt; 2e-16 ***\nfactor(case)37 -2.534691   0.050831 -49.865  &lt; 2e-16 ***\nfactor(case)38 -0.699826   0.049616 -14.105  &lt; 2e-16 ***\nfactor(case)39  0.389961   0.049578   7.866 5.46e-15 ***\nfactor(case)40 -1.291089   0.049878 -25.885  &lt; 2e-16 ***\nfactor(case)41 -2.527182   0.050822 -49.726  &lt; 2e-16 ***\nfactor(case)42 -1.718180   0.050056 -34.325  &lt; 2e-16 ***\nfactor(case)43  0.539154   0.049589  10.872  &lt; 2e-16 ***\nfactor(case)44  0.946498   0.049741  19.029  &lt; 2e-16 ***\nfactor(case)45 -1.672768   0.050093 -33.393  &lt; 2e-16 ***\nfactor(case)46 -1.924235   0.050325 -38.236  &lt; 2e-16 ***\nfactor(case)47  0.297708   0.049551   6.008 2.16e-09 ***\nfactor(case)48  0.045939   0.049527   0.928 0.353731    \nfactor(case)49  1.051133   0.049777  21.117  &lt; 2e-16 ***\nfactor(case)50 -1.102510   0.049764 -22.155  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2476 on 2449 degrees of freedom\nMultiple R-squared:  0.9173,    Adjusted R-squared:  0.9156 \nF-statistic: 543.2 on 50 and 2449 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see in the regression coefficient above that the coefficient for \\(x\\) is almost exactly +1.\nNext we can fit a model with cases/intercepts for time points (cross-sectional variation):\n\nsummary(lm(y ~ x + factor(time),data=gen_data$data))\n\n\nCall:\nlm(formula = y ~ x + factor(time), data = gen_data$data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9114 -0.1674 -0.0046  0.1699  0.7753 \n\nCoefficients:\n               Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)    -0.68479    0.03530  -19.400  &lt; 2e-16 ***\nx              -2.99280    0.01924 -155.534  &lt; 2e-16 ***\nfactor(time)2   1.51868    0.05005   30.345  &lt; 2e-16 ***\nfactor(time)3   2.26458    0.05073   44.637  &lt; 2e-16 ***\nfactor(time)4  -1.04731    0.04975  -21.050  &lt; 2e-16 ***\nfactor(time)5   1.73263    0.05025   34.482  &lt; 2e-16 ***\nfactor(time)6   1.26293    0.04988   25.319  &lt; 2e-16 ***\nfactor(time)7   0.06635    0.04952    1.340  0.18044    \nfactor(time)8   1.67967    0.05018   33.472  &lt; 2e-16 ***\nfactor(time)9  -0.75507    0.04964  -15.211  &lt; 2e-16 ***\nfactor(time)10 -0.55308    0.04957  -11.157  &lt; 2e-16 ***\nfactor(time)11  1.34417    0.04996   26.905  &lt; 2e-16 ***\nfactor(time)12  2.20623    0.05064   43.565  &lt; 2e-16 ***\nfactor(time)13  0.02975    0.04952    0.601  0.54803    \nfactor(time)14  0.10429    0.04952    2.106  0.03530 *  \nfactor(time)15  0.27429    0.04955    5.536 3.43e-08 ***\nfactor(time)16  1.05558    0.04981   21.193  &lt; 2e-16 ***\nfactor(time)17 -0.40503    0.04954   -8.175 4.68e-16 ***\nfactor(time)18  0.86367    0.04971   17.375  &lt; 2e-16 ***\nfactor(time)19  1.57862    0.05013   31.492  &lt; 2e-16 ***\nfactor(time)20 -0.01728    0.04952   -0.349  0.72717    \nfactor(time)21  0.62057    0.04961   12.508  &lt; 2e-16 ***\nfactor(time)22 -1.38478    0.04995  -27.721  &lt; 2e-16 ***\nfactor(time)23  0.31574    0.04954    6.374 2.20e-10 ***\nfactor(time)24  2.96579    0.05157   57.515  &lt; 2e-16 ***\nfactor(time)25  0.24178    0.04953    4.881 1.12e-06 ***\nfactor(time)26  2.83592    0.05144   55.133  &lt; 2e-16 ***\nfactor(time)27 -0.86284    0.04968  -17.369  &lt; 2e-16 ***\nfactor(time)28  0.92505    0.04975   18.594  &lt; 2e-16 ***\nfactor(time)29  1.05202    0.04980   21.125  &lt; 2e-16 ***\nfactor(time)30  0.15122    0.04953    3.053  0.00229 ** \nfactor(time)31  1.81587    0.05033   36.082  &lt; 2e-16 ***\nfactor(time)32  1.38787    0.04999   27.763  &lt; 2e-16 ***\nfactor(time)33  1.87706    0.05038   37.257  &lt; 2e-16 ***\nfactor(time)34  1.76956    0.05029   35.184  &lt; 2e-16 ***\nfactor(time)35  1.55739    0.05016   31.048  &lt; 2e-16 ***\nfactor(time)36  0.57794    0.04960   11.653  &lt; 2e-16 ***\nfactor(time)37  0.10647    0.04952    2.150  0.03166 *  \nfactor(time)38 -0.72418    0.04962  -14.594  &lt; 2e-16 ***\nfactor(time)39  1.80861    0.05033   35.938  &lt; 2e-16 ***\nfactor(time)40  0.24163    0.04953    4.879 1.14e-06 ***\nfactor(time)41  1.27448    0.04994   25.521  &lt; 2e-16 ***\nfactor(time)42  1.82843    0.05029   36.361  &lt; 2e-16 ***\nfactor(time)43  1.57770    0.05011   31.487  &lt; 2e-16 ***\nfactor(time)44  1.50850    0.05006   30.135  &lt; 2e-16 ***\nfactor(time)45  0.23318    0.04953    4.708 2.64e-06 ***\nfactor(time)46 -0.92491    0.04969  -18.613  &lt; 2e-16 ***\nfactor(time)47  1.97346    0.05044   39.125  &lt; 2e-16 ***\nfactor(time)48  0.60207    0.04961   12.135  &lt; 2e-16 ***\nfactor(time)49  0.75674    0.04964   15.244  &lt; 2e-16 ***\nfactor(time)50 -1.38077    0.04996  -27.637  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2476 on 2449 degrees of freedom\nMultiple R-squared:  0.9173,    Adjusted R-squared:  0.9156 \nF-statistic: 543.4 on 50 and 2449 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the estimated coefficient is almost exactly what we generated. Success!\nHowever, this brings us back to one of the questions we started with. We know the within-case relationship and the cross-sectional relationship, so what happens if we put dummies/intercepts on both cases and time? Well let’s find out:\n\nsummary(lm(y~x + factor(case) + factor(time),data=gen_data$data))\n\n\nCall:\nlm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.87158 -0.16705 -0.00396  0.16311  0.78026 \n\nCoefficients: (1 not defined because of singularities)\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.806372   0.087811   9.183  &lt; 2e-16 ***\nx               1.000661   0.143392   6.978 3.84e-12 ***\nfactor(case)2  -0.184865   0.049834  -3.710 0.000212 ***\nfactor(case)3   1.465951   0.074525  19.671  &lt; 2e-16 ***\nfactor(case)4  -1.363415   0.068454 -19.917  &lt; 2e-16 ***\nfactor(case)5  -0.884457   0.057091 -15.492  &lt; 2e-16 ***\nfactor(case)6   1.096983   0.064585  16.985  &lt; 2e-16 ***\nfactor(case)7  -0.991716   0.059427 -16.688  &lt; 2e-16 ***\nfactor(case)8  -1.597253   0.075111 -21.265  &lt; 2e-16 ***\nfactor(case)9  -2.125203   0.089352 -23.785  &lt; 2e-16 ***\nfactor(case)10 -0.028795   0.049592  -0.581 0.561545    \nfactor(case)11 -1.170649   0.065430 -17.892  &lt; 2e-16 ***\nfactor(case)12 -0.132674   0.049635  -2.673 0.007568 ** \nfactor(case)13 -0.611905   0.053240 -11.493  &lt; 2e-16 ***\nfactor(case)14  0.865860   0.059440  14.567  &lt; 2e-16 ***\nfactor(case)15 -0.867514   0.057639 -15.051  &lt; 2e-16 ***\nfactor(case)16  0.514605   0.053863   9.554  &lt; 2e-16 ***\nfactor(case)17 -0.633567   0.055045 -11.510  &lt; 2e-16 ***\nfactor(case)18 -0.696223   0.054818 -12.701  &lt; 2e-16 ***\nfactor(case)19  0.806361   0.058129  13.872  &lt; 2e-16 ***\nfactor(case)20 -0.763847   0.056206 -13.590  &lt; 2e-16 ***\nfactor(case)21 -0.596870   0.053399 -11.178  &lt; 2e-16 ***\nfactor(case)22 -1.404855   0.069741 -20.144  &lt; 2e-16 ***\nfactor(case)23  0.261759   0.050838   5.149 2.83e-07 ***\nfactor(case)24  0.972749   0.061053  15.933  &lt; 2e-16 ***\nfactor(case)25 -0.710003   0.054856 -12.943  &lt; 2e-16 ***\nfactor(case)26 -0.718107   0.055565 -12.924  &lt; 2e-16 ***\nfactor(case)27 -0.946375   0.059073 -16.020  &lt; 2e-16 ***\nfactor(case)28 -3.193052   0.122837 -25.994  &lt; 2e-16 ***\nfactor(case)29  0.120274   0.049902   2.410 0.016019 *  \nfactor(case)30 -0.964644   0.059789 -16.134  &lt; 2e-16 ***\nfactor(case)31 -1.107721   0.063746 -17.377  &lt; 2e-16 ***\nfactor(case)32 -0.888409   0.057538 -15.440  &lt; 2e-16 ***\nfactor(case)33 -0.919792   0.059160 -15.548  &lt; 2e-16 ***\nfactor(case)34  0.588263   0.054612  10.772  &lt; 2e-16 ***\nfactor(case)35 -0.008290   0.049589  -0.167 0.867240    \nfactor(case)36 -1.068500   0.061960 -17.245  &lt; 2e-16 ***\nfactor(case)37 -2.533613   0.101653 -24.924  &lt; 2e-16 ***\nfactor(case)38 -0.699544   0.054758 -12.775  &lt; 2e-16 ***\nfactor(case)39  0.389746   0.052653   7.402 1.84e-13 ***\nfactor(case)40 -1.290532   0.067565 -19.101  &lt; 2e-16 ***\nfactor(case)41 -2.526108   0.101383 -24.916  &lt; 2e-16 ***\nfactor(case)42 -1.717495   0.075071 -22.878  &lt; 2e-16 ***\nfactor(case)43  0.538917   0.053290  10.113  &lt; 2e-16 ***\nfactor(case)44  0.946063   0.061202  15.458  &lt; 2e-16 ***\nfactor(case)45 -1.672061   0.076523 -21.850  &lt; 2e-16 ***\nfactor(case)46 -1.923394   0.085197 -22.576  &lt; 2e-16 ***\nfactor(case)47  0.297558   0.051103   5.823 6.56e-09 ***\nfactor(case)48  0.045901   0.049675   0.924 0.355567    \nfactor(case)49  1.050663   0.062931  16.696  &lt; 2e-16 ***\nfactor(case)50 -1.102052   0.062325 -17.682  &lt; 2e-16 ***\nfactor(time)2   0.009865   0.089885   0.110 0.912618    \nfactor(time)3  -0.027398   0.115374  -0.237 0.812314    \nfactor(time)4  -0.039227   0.044426  -0.883 0.377342    \nfactor(time)5  -0.038629   0.098266  -0.393 0.694277    \nfactor(time)6   0.015206   0.081771   0.186 0.852496    \nfactor(time)7  -0.052421   0.051843  -1.011 0.312051    \nfactor(time)8  -0.009808   0.095634  -0.103 0.918319    \nfactor(time)9  -0.029774   0.042955  -0.693 0.488287    \nfactor(time)10 -0.073201   0.043597  -1.679 0.093275 .  \nfactor(time)11 -0.034386   0.085805  -0.401 0.688639    \nfactor(time)12  0.003032   0.112421   0.027 0.978486    \nfactor(time)13 -0.092023   0.051904  -1.773 0.076362 .  \nfactor(time)14  0.017896   0.051201   0.350 0.726720    \nfactor(time)15 -0.099884   0.057487  -1.738 0.082424 .  \nfactor(time)16 -0.059046   0.077744  -0.759 0.447632    \nfactor(time)17 -0.066464   0.044756  -1.485 0.137669    \nfactor(time)18 -0.039199   0.071529  -0.548 0.583733    \nfactor(time)19 -0.040240   0.093375  -0.431 0.666546    \nfactor(time)20 -0.019978   0.049628  -0.403 0.687311    \nfactor(time)21 -0.021785   0.064293  -0.339 0.734760    \nfactor(time)22 -0.016429   0.049358  -0.333 0.739273    \nfactor(time)23  0.016278   0.055739   0.292 0.770279    \nfactor(time)24 -0.021054   0.138843  -0.152 0.879486    \nfactor(time)25 -0.016068   0.054798  -0.293 0.769382    \nfactor(time)26 -0.054303   0.135548  -0.401 0.688739    \nfactor(time)27 -0.034151   0.043223  -0.790 0.429545    \nfactor(time)28 -0.075377   0.074360  -1.014 0.310839    \nfactor(time)29 -0.046340   0.077258  -0.600 0.548686    \nfactor(time)30 -0.062577   0.053830  -1.162 0.245150    \nfactor(time)31 -0.049767   0.101325  -0.491 0.623362    \nfactor(time)32 -0.037785   0.087273  -0.433 0.665091    \nfactor(time)33 -0.052008   0.103393  -0.503 0.614999    \nfactor(time)34 -0.059567   0.100140  -0.595 0.552009    \nfactor(time)35 -0.104873   0.094761  -1.107 0.268534    \nfactor(time)36 -0.010170   0.062857  -0.162 0.871484    \nfactor(time)37 -0.082164   0.053289  -1.542 0.123243    \nfactor(time)38 -0.057402   0.042945  -1.337 0.181464    \nfactor(time)39 -0.056943   0.101323  -0.562 0.574169    \nfactor(time)40  0.007376   0.054276   0.136 0.891908    \nfactor(time)41 -0.070988   0.084778  -0.837 0.402484    \nfactor(time)42  0.010645   0.099772   0.107 0.915043    \nfactor(time)43 -0.013565   0.092496  -0.147 0.883418    \nfactor(time)44 -0.015229   0.090355  -0.169 0.866166    \nfactor(time)45  0.010920   0.054013   0.202 0.839797    \nfactor(time)46 -0.060932   0.043387  -1.404 0.160328    \nfactor(time)47 -0.020632   0.105521  -0.196 0.844996    \nfactor(time)48 -0.038896   0.064256  -0.605 0.545015    \nfactor(time)49  0.028808   0.066612   0.432 0.665437    \nfactor(time)50        NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2479 on 2401 degrees of freedom\nMultiple R-squared:  0.9187,    Adjusted R-squared:  0.9154 \nF-statistic:   277 on 98 and 2401 DF,  p-value: &lt; 2.2e-16\n\n\nYou might expect that perhaps the two-way coefficient would be somewhere between the cross-sectional and within-case coefficients. Nope. It’s equal in this simulation to roughly 1, but that depends on how much variance there is in the cross-section and the within-case components of the dataset as they are now being combined in a non-linear fashion (if you can’t wrap your mind around this, don’t worry, it’s nearly impossible to do and unfruitful if you do accomplish it). However, the coefficient is still equal to a value that appears reasonable, although it is much less precise, and we might be willing to traipse along with it. However, you should pay attention to what happened in the model summary above–one of the time dummies (factor(time)50) came back with a missing (NA) coefficient! What is that about?\nI’m so glad you asked. As we mentioned earlier, R has the ability to check for multi-collinearity in the predictor variables to avoid getting errors when attempting to estimate linear regression. Multi-collinearity can arise for reasons that have nothing to do with fixed effects, such as accidentally including variables that are sums of each other, etc. It often happens in fixed effects regression models when people try to include variables that only vary in the cross-section when using case fixed effects (a symptom of the Cheeseburger Syndrome mentioned previously). In this instance, though, there is no reason to think that there is multi-collinearity as we generated the data with two continuous variables (\\(x\\) and \\(y\\)). The data are perfect with no missingness. So how could this happen?\nThis discovery is one of the central contributions of our paper. Whenever panel data has a single effect for the cross-section and for within-case variation–as is the dominant way of thinking about panel data–the two-way fixed effects coefficient is statistically unidentified. That is, trying to estimate it is equivalent to saying 2 + 2 = 5 or the earth is as round as a square. As we show in the paper, it algebraically reduces to dividing by zero. R magically saved the day by dropping a variable, but we can show what would happen if R had not intervened.\nIn the code below I manually estimate a regression using the matrix inversion formula, \\((X^TX)^{-1}X^Ty\\), where \\(X\\) in this case is all of our predictor variables, including the case/time dummies:\n\nX &lt;- model.matrix(y~x + factor(case) + factor(time),data=gen_data$data)\ny &lt;- gen_data$data$y\ntry(solve(t(X)%*%X)%*%t(X)%*%y)\n\nError in solve.default(t(X) %*% X) : \n  system is computationally singular: reciprocal condition number = 5.58323e-19\n\n\nYou can see the error message: the system (matrix computation of OLS regression) is computationally singular. You’ll need to see the paper for how this all breaks down, but essentially the question you are putting to R is nonsensical. You simply cannot estimate a single joint effect while including all the variables. Instead, you need to drop one, which essentially means the effect is relative to the whichever intercept happened to be dropped (in this case time point 50). The coefficient could change if we simply re-arrange the labeling of the time fixed effects, as in the following:\n\ngen_data$data$time &lt;- as.numeric(factor(gen_data$data$time,levels=sample(unique(gen_data$data$time))))\nsummary(lm(y~x + factor(case) + factor(time),data=gen_data$data))\n\n\nCall:\nlm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.87158 -0.16705 -0.00396  0.16311  0.78026 \n\nCoefficients: (1 not defined because of singularities)\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     0.827062   0.058816  14.062  &lt; 2e-16 ***\nx               0.892381   0.169360   5.269 1.49e-07 ***\nfactor(case)2  -0.181065   0.049934  -3.626 0.000294 ***\nfactor(case)3   1.423934   0.082322  17.297  &lt; 2e-16 ***\nfactor(case)4  -1.327772   0.074606 -17.797  &lt; 2e-16 ***\nfactor(case)5  -0.863081   0.059798 -14.433  &lt; 2e-16 ***\nfactor(case)6   1.065727   0.069627  15.306  &lt; 2e-16 ***\nfactor(case)7  -0.966975   0.062893 -15.375  &lt; 2e-16 ***\nfactor(case)8  -1.554645   0.083062 -18.717  &lt; 2e-16 ***\nfactor(case)9  -2.069070   0.100829 -20.521  &lt; 2e-16 ***\nfactor(case)10 -0.029652   0.049597  -0.598 0.549997    \nfactor(case)11 -1.138408   0.070718 -16.098  &lt; 2e-16 ***\nfactor(case)12 -0.130897   0.049657  -2.636 0.008442 ** \nfactor(case)13 -0.597254   0.054619 -10.935  &lt; 2e-16 ***\nfactor(case)14  0.841102   0.062910  13.370  &lt; 2e-16 ***\nfactor(case)15 -0.845316   0.060527 -13.966  &lt; 2e-16 ***\nfactor(case)16  0.498709   0.055464   8.992  &lt; 2e-16 ***\nfactor(case)17 -0.615509   0.057060 -10.787  &lt; 2e-16 ***\nfactor(case)18 -0.678563   0.056754 -11.956  &lt; 2e-16 ***\nfactor(case)19  0.783446   0.061177  12.806  &lt; 2e-16 ***\nfactor(case)20 -0.743853   0.058618 -12.690  &lt; 2e-16 ***\nfactor(case)21 -0.581894   0.054834 -10.612  &lt; 2e-16 ***\nfactor(case)22 -1.367817   0.076249 -17.939  &lt; 2e-16 ***\nfactor(case)23  0.253267   0.051327   4.934 8.59e-07 ***\nfactor(case)24  0.945844   0.065030  14.545  &lt; 2e-16 ***\nfactor(case)25 -0.692276   0.056805 -12.187  &lt; 2e-16 ***\nfactor(case)26 -0.699163   0.057758 -12.105  &lt; 2e-16 ***\nfactor(case)27 -0.922121   0.062427 -14.771  &lt; 2e-16 ***\nfactor(case)28 -3.108184   0.141697 -21.936  &lt; 2e-16 ***\nfactor(case)29  0.115992   0.050029   2.318 0.020507 *  \nfactor(case)30 -0.939410   0.063371 -14.824  &lt; 2e-16 ***\nfactor(case)31 -1.077463   0.068540 -15.720  &lt; 2e-16 ***\nfactor(case)32 -0.866360   0.060394 -14.345  &lt; 2e-16 ***\nfactor(case)33 -0.895419   0.062541 -14.317  &lt; 2e-16 ***\nfactor(case)34  0.570972   0.056476  10.110  &lt; 2e-16 ***\nfactor(case)35 -0.007546   0.049593  -0.152 0.879081    \nfactor(case)36 -1.040438   0.066215 -15.713  &lt; 2e-16 ***\nfactor(case)37 -2.466600   0.115948 -21.273  &lt; 2e-16 ***\nfactor(case)38 -0.681990   0.056674 -12.034  &lt; 2e-16 ***\nfactor(case)39  0.376359   0.053819   6.993 3.47e-12 ***\nfactor(case)40 -1.255870   0.073466 -17.095  &lt; 2e-16 ***\nfactor(case)41 -2.459329   0.115618 -21.271  &lt; 2e-16 ***\nfactor(case)42 -1.674928   0.083011 -20.177  &lt; 2e-16 ***\nfactor(case)43  0.524163   0.054686   9.585  &lt; 2e-16 ***\nfactor(case)44  0.918966   0.065225  14.089  &lt; 2e-16 ***\nfactor(case)45 -1.628044   0.084840 -19.190  &lt; 2e-16 ***\nfactor(case)46 -1.871074   0.095680 -19.556  &lt; 2e-16 ***\nfactor(case)47  0.288205   0.051692   5.575 2.75e-08 ***\nfactor(case)48  0.043567   0.049713   0.876 0.380918    \nfactor(case)49  1.021395   0.067481  15.136  &lt; 2e-16 ***\nfactor(case)50 -1.073533   0.066692 -16.097  &lt; 2e-16 ***\nfactor(time)2  -0.069009   0.100571  -0.686 0.492671    \nfactor(time)3  -0.108184   0.073336  -1.475 0.140295    \nfactor(time)4   0.002837   0.043759   0.065 0.948307    \nfactor(time)5  -0.011388   0.082981  -0.137 0.890857    \nfactor(time)6  -0.079838   0.110453  -0.723 0.469861    \nfactor(time)7  -0.066293   0.048598  -1.364 0.172660    \nfactor(time)8  -0.135578   0.123915  -1.094 0.274015    \nfactor(time)9  -0.149671   0.096133  -1.557 0.119623    \nfactor(time)10 -0.135415   0.137057  -0.988 0.323244    \nfactor(time)11 -0.046205   0.101477  -0.455 0.648917    \nfactor(time)12 -0.094439   0.062068  -1.522 0.128252    \nfactor(time)13 -0.056940   0.061062  -0.933 0.351172    \nfactor(time)14 -0.074651   0.076729  -0.973 0.330690    \nfactor(time)15 -0.064301   0.086107  -0.747 0.455286    \nfactor(time)16 -0.148654   0.105819  -1.405 0.160210    \nfactor(time)17 -0.109373   0.139416  -0.785 0.432820    \nfactor(time)18 -0.023932   0.052512  -0.456 0.648610    \nfactor(time)19 -0.054156   0.088109  -0.615 0.538842    \nfactor(time)20 -0.035535   0.098979  -0.359 0.719611    \nfactor(time)21 -0.116553   0.143594  -0.812 0.417053    \nfactor(time)22 -0.010896   0.065124  -0.167 0.867139    \nfactor(time)23 -0.030352   0.055015  -0.552 0.581207    \nfactor(time)24 -0.097372   0.166084  -0.586 0.557742    \nfactor(time)25 -0.033847   0.056850  -0.595 0.551644    \nfactor(time)26 -0.144292   0.145023  -0.995 0.319857    \nfactor(time)27 -0.076492   0.070012  -1.093 0.274696    \nfactor(time)28 -0.081450   0.086159  -0.945 0.344576    \nfactor(time)29 -0.059062   0.059659  -0.990 0.322275    \nfactor(time)30 -0.069904   0.049340  -1.417 0.156679    \nfactor(time)31 -0.116713   0.102264  -1.141 0.253863    \nfactor(time)32 -0.050535   0.050595  -0.999 0.317977    \nfactor(time)33 -0.025185   0.043191  -0.583 0.559876    \nfactor(time)34 -0.026497   0.046296  -0.572 0.567147    \nfactor(time)35 -0.059114   0.048596  -1.216 0.223940    \nfactor(time)36 -0.109133   0.105935  -1.030 0.303025    \nfactor(time)37 -0.119734   0.053185  -2.251 0.024457 *  \nfactor(time)38 -0.042987   0.101938  -0.422 0.673288    \nfactor(time)39 -0.035869   0.047662  -0.753 0.451788    \nfactor(time)40 -0.113464   0.165575  -0.685 0.493239    \nfactor(time)41 -0.059933   0.110559  -0.542 0.587805    \nfactor(time)42 -0.009158   0.057266  -0.160 0.872963    \nfactor(time)43 -0.126494   0.150871  -0.838 0.401878    \nfactor(time)44 -0.059636   0.047396  -1.258 0.208424    \nfactor(time)45 -0.039694   0.107192  -0.370 0.711183    \nfactor(time)46 -0.056278   0.054291  -1.037 0.300026    \nfactor(time)47 -0.146145   0.129554  -1.128 0.259404    \nfactor(time)48 -0.088757   0.069468  -1.278 0.201491    \nfactor(time)49 -0.136982   0.103233  -1.327 0.184661    \nfactor(time)50        NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2479 on 2401 degrees of freedom\nMultiple R-squared:  0.9187,    Adjusted R-squared:  0.9154 \nF-statistic:   277 on 98 and 2401 DF,  p-value: &lt; 2.2e-16\n\n\nAs you can see, the coefficient changed because we arbitrarily swapped the order of the time fixed effects, and as R will drop the last one in the case of multi-collinearity, the estimate changed.\nAt this point, you should be concerned. One of our frustrations with this piece is that although the working paper has been in circulation for years, no one seems to have cared about this apparently quite important problem.\nYou might wonder though that you’ve seen two-way fixed effects models where this didn’t happen. As we show in the paper, the two-way FE model is identified if we generate the data differently. What we have to do is use a different effect of \\(x\\) on \\(y\\) for each time point/case, or what can be called a varying slopes model (slope for regression coefficient). We are not varying the fixed effects/intercepts themselves, rather we are varying the relationship between \\(x\\) and \\(y\\) across time points and cases. We must do this to prevent the two-way FE model from being unidentified.\nTo demonstrate this, we will generate new data except the coefficients will vary across case and time points randomly:\n\n# make the slopes vary with the .sd parameters\ngen_data &lt;- tw_data(N=20,\n                    T=50,\n                    case.eff.mean = 1,\n                    cross.eff.mean = -1,\n                    cross.eff.sd =.25,\n                    case.eff.sd =1,\n                    noise.sd=1)\n\nsummary(lm(y~x + factor(case) + factor(time),data=gen_data$data))\n\n\nCall:\nlm(formula = y ~ x + factor(case) + factor(time), data = gen_data$data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5458 -0.7073  0.0026  0.6446  3.1709 \n\nCoefficients:\n                Estimate Std. Error  t value Pr(&gt;|t|)    \n(Intercept)    -0.673576   0.264996   -2.542 0.011188 *  \nx              -0.812224   0.005523 -147.067  &lt; 2e-16 ***\nfactor(case)2   0.431969   0.201817    2.140 0.032582 *  \nfactor(case)3   0.169660   0.201920    0.840 0.400994    \nfactor(case)4   0.216513   0.201806    1.073 0.283604    \nfactor(case)5   0.377437   0.201835    1.870 0.061794 .  \nfactor(case)6   0.233856   0.201762    1.159 0.246726    \nfactor(case)7   0.291743   0.202146    1.443 0.149292    \nfactor(case)8   0.040054   0.201779    0.199 0.842696    \nfactor(case)9   0.182815   0.201763    0.906 0.365123    \nfactor(case)10  0.197116   0.201782    0.977 0.328886    \nfactor(case)11  0.264956   0.201782    1.313 0.189480    \nfactor(case)12  0.075685   0.201799    0.375 0.707708    \nfactor(case)13  0.249261   0.201795    1.235 0.217062    \nfactor(case)14  0.118726   0.201765    0.588 0.556382    \nfactor(case)15  0.494571   0.201828    2.450 0.014451 *  \nfactor(case)16 -0.085249   0.203634   -0.419 0.675579    \nfactor(case)17  0.383878   0.201766    1.903 0.057403 .  \nfactor(case)18  0.290350   0.201837    1.439 0.150619    \nfactor(case)19  0.483656   0.201790    2.397 0.016734 *  \nfactor(case)20  0.287368   0.201792    1.424 0.154759    \nfactor(time)2   0.596679   0.319058    1.870 0.061780 .  \nfactor(time)3   1.407443   0.319158    4.410 1.16e-05 ***\nfactor(time)4  -0.328379   0.320361   -1.025 0.305616    \nfactor(time)5  -0.119690   0.319047   -0.375 0.707634    \nfactor(time)6   1.539948   0.319015    4.827 1.62e-06 ***\nfactor(time)7   0.338443   0.319042    1.061 0.289052    \nfactor(time)8   0.528072   0.319048    1.655 0.098232 .  \nfactor(time)9  -0.032228   0.319030   -0.101 0.919556    \nfactor(time)10  0.765270   0.319056    2.399 0.016657 *  \nfactor(time)11  0.789097   0.319058    2.473 0.013568 *  \nfactor(time)12  2.149822   0.319199    6.735 2.87e-11 ***\nfactor(time)13  0.118301   0.319032    0.371 0.710862    \nfactor(time)14 -0.734958   0.319015   -2.304 0.021452 *  \nfactor(time)15  0.888259   0.319109    2.784 0.005486 ** \nfactor(time)16  0.553245   0.319027    1.734 0.083221 .  \nfactor(time)17  0.652004   0.319030    2.044 0.041264 *  \nfactor(time)18  1.351653   0.319104    4.236 2.50e-05 ***\nfactor(time)19  0.260828   0.319061    0.817 0.413860    \nfactor(time)20 -0.662932   0.319060   -2.078 0.038005 *  \nfactor(time)21  1.509116   0.319095    4.729 2.60e-06 ***\nfactor(time)22 -0.014879   0.319029   -0.047 0.962810    \nfactor(time)23 -0.538377   0.319023   -1.688 0.091827 .  \nfactor(time)24  0.493220   0.319046    1.546 0.122464    \nfactor(time)25 -1.820174   0.319100   -5.704 1.57e-08 ***\nfactor(time)26 -0.571426   0.319015   -1.791 0.073583 .  \nfactor(time)27  0.093017   0.319026    0.292 0.770683    \nfactor(time)28 -1.328269   0.319035   -4.163 3.43e-05 ***\nfactor(time)29  0.342019   0.319038    1.072 0.283983    \nfactor(time)30 -0.774338   0.319014   -2.427 0.015401 *  \nfactor(time)31  0.461852   0.319026    1.448 0.148040    \nfactor(time)32  0.308478   0.319046    0.967 0.333858    \nfactor(time)33  1.343319   0.319094    4.210 2.80e-05 ***\nfactor(time)34  1.989206   0.319258    6.231 7.03e-10 ***\nfactor(time)35 -0.422407   0.319019   -1.324 0.185801    \nfactor(time)36  0.614837   0.319085    1.927 0.054299 .  \nfactor(time)37 -0.416240   0.319061   -1.305 0.192359    \nfactor(time)38  0.779448   0.319089    2.443 0.014762 *  \nfactor(time)39 -0.847622   0.319015   -2.657 0.008019 ** \nfactor(time)40  3.245566   0.319204   10.168  &lt; 2e-16 ***\nfactor(time)41 -0.589393   0.319031   -1.847 0.064999 .  \nfactor(time)42  0.509922   0.319178    1.598 0.110469    \nfactor(time)43 -0.815928   0.319029   -2.558 0.010699 *  \nfactor(time)44  1.200743   0.320664    3.745 0.000192 ***\nfactor(time)45 -0.677540   0.319015   -2.124 0.033946 *  \nfactor(time)46  0.329598   0.319049    1.033 0.301842    \nfactor(time)47  0.254068   0.319037    0.796 0.426027    \nfactor(time)48  0.728453   0.319070    2.283 0.022652 *  \nfactor(time)49  2.376336   0.319302    7.442 2.25e-13 ***\nfactor(time)50  0.525069   0.319064    1.646 0.100172    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.009 on 930 degrees of freedom\nMultiple R-squared:  0.9625,    Adjusted R-squared:  0.9598 \nF-statistic: 346.3 on 69 and 930 DF,  p-value: &lt; 2.2e-16\n\n\nBada bing bada boom. Now there are no missing coefficients. However, the coefficient on \\(x\\) isn’t any easier to interpret, and in fact it is in even less transparent as it involves taking averages of averages of coefficients in the cross-section and within-case variation (say that five times fast). It is also peculiar that the model can only be fit with this kind of variation in the slopes, as opposed to the intercepts, as might appear more logical. The models with fixed effects only for cases or time points do not have this problem at all.\nWhat makes this lack of identification particularly malicious is that it is virtually impossible to identify unless someone takes the trouble as we did to generate data from scratch. Visually, the dataset with single coefficients for cross-sectional/within-case variation appears pristine. It is only deep under the hood that the problem appears. The fact that R (and Stata has similar behavior) can magically make the problem go away by changing the data only makes it less likely that the problem will ever be identified. We simply do not know how many papers in the literature have been affected by this problem (or by similar situations where the regression slopes are almost fixed across cases or time points).\nThere is one important caveat to this post. The two-way fixed effects model can be a difference-in -differences model, but only if there are exactly two (2) time points. It is not possible to run a two-way FE model with many time points and call it difference-in-difference as there are the same difficulties in interpretation. We discuss this more in the paper."
  },
  {
    "objectID": "post/fixed_effects/index.html#summary",
    "href": "post/fixed_effects/index.html#summary",
    "title": "What Panel Data Is Really All About",
    "section": "Summary",
    "text": "Summary\nIn summary, fixed effects models are very useful for panel data as they help isolate dimensions of variation that matter for research questions. They are not magical tools for causal inference, but rather frameworks for understanding data. It is important to think about which dimension (cross-section or within-case) is more relevant, and then go with that dimension. All other concerns, including modeling spatial or time autocorrelation, omitted variables and endogeneity are all secondary to this first and most important point, which is what comparisons can be drawn from the data?\nOne important area that this can be applied to is allowing people to fit more models with fixed effects on time points rather than cases. There are many questions that can be best understood by comparing cases to each other rather than cases to themselves over time. Studying long-run institutions like electoral systems, for example, can only be understood in terms of cross-sectional variation. There is nothing unique or special about the within-case model.\nSome claim that within-case variation is better because cases have less heterogeneity than the cross-section. However, there is no way this can be true a priori. If minimizing heterogeneity is the aim, then it is important to consider the time frame and how units change over time. For example, if we have a very long time series, say 200 years, and we are comparing the U.S. and Canada, then we might believe that the comparison of the U.S. of 1800 to the Canada of 1800 (the cross-section) has less noise than a comparison of the U.S. of 1800 to the U.S. of 2020 (within-case variation). We need to think more about our data and what it represents rather than taking the short-cut of employing the same model everyone else uses.\nWhile we did not discuss random effects/hierarchical models in this post, the same principles apply even if “partial pooling” is used rather than “no pooling”. Intercepts are designed to draw comparisons, and however the intercepts are modeled, it is important to think about what they are saying about the data, and whether that statement makes any sense.\nSo if you don’t want to eat that cheeseburger… we release you. Go enjoy your tofu-based meat alternative with relish."
  },
  {
    "objectID": "post/kubinec_model_draft/index.html",
    "href": "post/kubinec_model_draft/index.html",
    "title": "A Proposed Model for Partial Identification of SARS-CoV2 Infection Rates Given Observed Tests and Cases",
    "section": "",
    "text": "For an up to date version of this model, please see our paper at https://osf.io/preprints/socarxiv/jp4wk/."
  },
  {
    "objectID": "post/religion_and_sex/index.html",
    "href": "post/religion_and_sex/index.html",
    "title": "Which Religious Groups Have the Most Sex?",
    "section": "",
    "text": "Interested in more social science on contemporary issues? Check out my just-released book with Cambridge University Press and use discount code KUBINEC23 to get 20% off.\nThere has been plenty of discussion about declining fertility rates and patterns of marriage among people in the United States following the news that the US birth rate declined to its lowest since the Great Depression. There are a lot of debates about why this is the case, whether it be the expense of raising children, the lack of flexible options for women with careers, or a loss of societal values favoring larger families. Part of the backdrop of this demographic change, of course, are changes in how Americans identify themselves, particularly in terms of religion. For the last several decades, Americans have moved away from traditional religious categories like Christianity towards so-called unaffiliated status.\nThis matters for birth rates because it seems like people who do not have a religious identity are also noticeably less likely to have children. While doing some reading on this subject on my own, which I find fascinating–is there a connection between these changes in religious affiliation and child bearing?–I also wanted to see whether belonging to a religious group changes sexual frequency. In other words, does becoming more or less religious lead to more sexual activity overall? There are a plethora of studies about virtually every side of American sex lives, but surprisingly I couldn’t answer this question from a review of studies (admittedly limited by my knowledge of the field and Google Scholar). As a result, in this post I’ll set out to answer this basic descriptive question:\nSo that you don’t have to read the post, here are the topline findings:"
  },
  {
    "objectID": "post/religion_and_sex/index.html#data",
    "href": "post/religion_and_sex/index.html#data",
    "title": "Which Religious Groups Have the Most Sex?",
    "section": "Data",
    "text": "Data\nTo answer this question, I’ll use data from the truly phenomenal General Social Survey. As a social scientist who does most of my work outside of the developed West, I am perpetually amazed at the quality of data about the lives of people in wealthier countries, ranging from their sexual proclivities to wealth, income, and political partisanship. In the Middle East where I do the majority of my research, these types of studies are just becoming more prevalent, and collecting data on sexual activity is largely still taboo.\nDigression aside, the rest of this post I will show the data processing steps to answer this question. The data are easily available from GSS’ site linked above, and they even have a handy dandy R script to load the data (see this post’s Rmarkdown file for details). I was able to obtain a range of demographic and social questions for the survey’s entire duration of 1978 - 2018, although the sexual frequency questions only started in the late 1980s."
  },
  {
    "objectID": "post/religion_and_sex/index.html#description",
    "href": "post/religion_and_sex/index.html#description",
    "title": "Which Religious Groups Have the Most Sex?",
    "section": "Description",
    "text": "Description\nThe plot below shows the levels of reported sexual frequency over time for each of the possible responses to the GSS question about how often the respondent had sex in the past 12 months:\n\nGSS %&gt;% \n  filter(!is.na(SEXFREQ)) %&gt;% \n  group_by(SEXFREQ,YEAR) %&gt;% \n  count() %&gt;% \n  group_by(YEAR) %&gt;% \n  mutate(prop_n=n/sum(n)) %&gt;% \n  ggplot(aes(y=prop_n,x=YEAR)) +\n  geom_area(fill=\"blue\",alpha=0.5) +\n  facet_wrap(~SEXFREQ,scale=\"free_y\") + \n  theme_tufte() +\n  scale_y_continuous(labels=scales::percent_format(accuracy = 1)) +\n  labs(y=\"Percent Reporting\",x=\"Year of Survey\") + \n  ggtitle(\"About how often did you have sex during the last 12 months?\",subtitle=\"Data from General Social Survey\")\n\n\n\n\n\n\n\n\nWe can see from the data above that the levels of reported sexual frequency are pretty stable over time. This is good news in terms of survey validity as we wouldn’t expect these numbers to dramatically bounce around from year to year. It is clear that there have been slow but steady decreases in the proportion reporting weekly intercourse and rising levels of those who have sex a month or 1-2 times a month. By contrast, those who have sex four or more times a week has remained relatively constant, if a relatively small minority of respondents (~5%).\nIt is important to note as well the weird dip in the data in 2012 for those reporting no sexual activity. This is apparently an issue with the 2012 survey reporting of these questions, which is something we’ll need to be aware of in this data analysis. Always plot your data!"
  },
  {
    "objectID": "post/religion_and_sex/index.html#religion-and-sex",
    "href": "post/religion_and_sex/index.html#religion-and-sex",
    "title": "Which Religious Groups Have the Most Sex?",
    "section": "Religion and Sex",
    "text": "Religion and Sex\nNow that we’ve examined the data, we’ll go ahead and begin breaking down these categories by religious affiliation. As a first cut, we’ll use a question in the GSS that asked people to report the broad religious tradition they belong to, and re-do the plot above with lines for each of these religious traditions. To clean up the plot a bit we’ll exclude some very small religious categories.\n\nGSS %&gt;% \n  filter(!is.na(SEXFREQ),\n         !is.na(RELIG),\n         !(RELIG %in% c(\"Native American\",\"Inter\",\"Other Eastern\",\"Other\",\"Buddhism\",\"Hinduism\",\"Orthodox\",\"Christian\",\"Muslim\")),\n         YEAR!=2012) %&gt;% \n  group_by(SEXFREQ,RELIG,YEAR) %&gt;% \n  count() %&gt;% \n  group_by(YEAR,RELIG) %&gt;% \n  mutate(prop_n=n/sum(n)) %&gt;% \n  ggplot(aes(y=prop_n,x=YEAR)) +\n  geom_line(aes(colour=RELIG,linetype=RELIG)) +\n  facet_wrap(~SEXFREQ,scale=\"free_y\") + \n  theme_tufte() +\n  scale_y_continuous(labels=scales::percent_format(accuracy = 1)) +\n  scale_colour_viridis_d() +\n  labs(y=\"Percent Reporting\",x=\"Year of Survey\") + \n  ggtitle(\"About how often did you have sex during the last 12 months?\",subtitle=\"Data from General Social Survey\")\n\n\n\n\n\n\n\n\nThis plot shows a lot more bounce or variation over time, especially for the categories with fewer respondents, such as Jewish people. This can make it difficult to pull out clear trends, although it would seem that people who report no religious affiliation have more frequent sexual activity at least in some categories, i.e. high levels of intercourse (more than 4 times per week) and also fewer abstainers. Jewish people, by contrast, may be more likely to report infrequent sexual activity (once a month or once or twice a month).\nWe can get a more precise answer for all religious groups in the GSS if we take the average of the past five years (2013 to 2018). I report these in the plot below:\n\nGSS %&gt;% \n  filter(!is.na(SEXFREQ),\n         !is.na(RELIG),\n         YEAR&gt;2012) %&gt;% \n  mutate(RELIG=fct_collapse(RELIG,\n                            Other=c(\"Other Eastern\",\"Other\",\n                                    \"Native American\",\"Inter\",\n                                    \"Christian\",\"Orthodox\"))) %&gt;% \n  group_by(SEXFREQ,RELIG) %&gt;% \n  count() %&gt;% \n  group_by(RELIG) %&gt;% \n  mutate(prop_n=n/sum(n),\n         lower=binom.confint(n,sum(n),method=\"bayes\")$lower,\n         upper=binom.confint(n,sum(n),method=\"bayes\")$upper,\n         RELIG=factor(RELIG,levels=c(\"Protestant\",\"Catholic\",\n                                     \"None\",\"Jewish\",\"Muslim\",\"Hinduism\",\n                                     \"Buddhism\",\"Other\"))) %&gt;%\n  ggplot(aes(y=prop_n,x=RELIG)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper),\n                  colour=\"blue\",alpha=0.5,size=.1) +\n  facet_wrap(~SEXFREQ,scales=\"free_x\") + \n  theme_tufte() +\n  scale_y_continuous(labels=scales::percent_format(accuracy = 1)) +\n  scale_colour_viridis_d() +\n  labs(y=\"Percent Reporting\",x=\"Religious Tradition\") + \n  coord_flip() +\n  ggtitle(\"About how often did you have sex during the last 12 months?\",subtitle=\"Data from General Social Survey\")\n\n\n\n\n\n\n\n\nThe plot above shows proportions along with a 5% to 95% confidence interval of likely values. The wide intervals for the smaller religious categories shows the uncertainty in these estimates even when averaging over four years’ worth of data. Only when comparing between nones, Catholics and Protestants can we make any clear comparisons, and it would seem again that people without religious affiliation are less likely to abstain from sex and also somewhat more likely to report higher rates of sexual activity (2-3 and 4 or more times per week). For the other categories, there is relatively little difference between religious categories, and the other groups are too small to make strong inferences."
  },
  {
    "objectID": "post/religion_and_sex/index.html#religious-devotion",
    "href": "post/religion_and_sex/index.html#religious-devotion",
    "title": "Which Religious Groups Have the Most Sex?",
    "section": "Religious Devotion",
    "text": "Religious Devotion\nWhile the analysis above allows us to answer some very basic questions about rates of sexual activity, it leaves a lot unanswered. One of the most important problems is that we are using reported religious categories rather than a measure of religious observance. While we might think that people without religious affiliation are equally un-affiliated (though perhaps not always), we certainly can’t say that for those with religious affiliation. Some are much more involved in their religious tradition, and we might think of them as “better” representativeness of what it means to be religious.\nThe GSS has a few questions that can help us tackle this question. From here on I’ll focus primarily on differences between Protestants, Catholics and unaffiliated as they are the largest groups in the survey, and we’ll keep our focus on the last four years of data. We’ll use two questions to differentiate respondents. First, we’ll use a question describing how often respondents pray as a measure of devotion. To measure theological differences, we’ll use a question about beliefs about the Bible.\nWe can then look at rates of sexual activity for these three main groups when comparing religious people by their levels of prayer and types of theological convictions. To do so, we’ll break down the Protestant/Catholic group by prayer activity and theological conviction, and compare these subsets to people with no religion as a group. First, for prayer:\n\nGSS %&gt;% \n  filter(!is.na(SEXFREQ),\n         !is.na(RELIG),\n         YEAR&gt;2012,\n         RELIG %in% c(\"None\",\"Protestant\",\"Catholic\")) %&gt;% \n  mutate(RELIG=fct_collapse(RELIG,Christian=c(\"Protestant\",\"Catholic\")),\n         PRAY_RELIG=ifelse(RELIG!=\"Christian\",\"No religion\",as.character(PRAY)),\n         PRAY_RELIG=ordered(PRAY_RELIG, levels=c(\"No religion\",\"Several times a day\",\n                                    \"Once a day\",\n                                    \"Several times a week\",\n                                    \"Once a week\",\n                                    \"Less than once a week\",\n                                    \"Never\"))) %&gt;% \n  filter(!is.na(PRAY_RELIG)) %&gt;% \n  group_by(SEXFREQ,PRAY_RELIG) %&gt;% \n  count() %&gt;% \n  group_by(PRAY_RELIG) %&gt;% \n  mutate(prop_n=n/sum(n),\n         lower=binom.confint(n,sum(n),method=\"bayes\")$lower,\n         upper=binom.confint(n,sum(n),method=\"bayes\")$upper) %&gt;%\n  ggplot(aes(y=prop_n,x=PRAY_RELIG)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper),\n                  colour=\"blue\",alpha=0.5,size=.1) +\n  facet_wrap(~SEXFREQ,scales=\"free_x\") + \n  theme_tufte() +\n  scale_y_continuous(labels=scales::percent_format(accuracy = 1)) +\n  scale_colour_viridis_d() +\n  labs(y=\"Percent Reporting\",x=\"How Often Do You Pray?\") + \n  coord_flip() +\n  ggtitle(\"About how often did you have sex during the last 12 months?\",subtitle=\"Data from General Social Survey\")\n\n\n\n\n\n\n\n\nThe plot above provides some more nuance to the story: those who pray a lot tend to abstain more and have less frequent sex at high rates, though again the record is much more mixed for intermediate levels of sexual activity. For people who report religious affiliation but don’t pray much, they actually report higher sexual frequency than people who are religiously unaffiliated, though the estimates are somewhat imprecise. We can also look at a similar breakdown for beliefs about the bible:\n\nGSS %&gt;% \n  filter(!is.na(SEXFREQ),\n         !is.na(RELIG),\n         YEAR&gt;2012,\n         RELIG %in% c(\"None\",\"Protestant\",\"Catholic\")) %&gt;% \n  mutate(RELIG=fct_collapse(RELIG,Christian=c(\"Protestant\",\"Catholic\")),\n         BIBLE_RELIG=ifelse(RELIG!=\"Christian\",\"No religion\",as.character(BIBLE)),\n         BIBLE_RELIG=ordered(BIBLE_RELIG, levels=c(\"No religion\",\"Actual word\",\"Inspired word\",\"Ancient book\",\n                                    \"Other\"))) %&gt;% \n  filter(!is.na(BIBLE_RELIG),BIBLE_RELIG!=\"Other\") %&gt;% \n  group_by(SEXFREQ,BIBLE_RELIG) %&gt;% \n  count() %&gt;% \n  group_by(BIBLE_RELIG) %&gt;% \n  mutate(prop_n=n/sum(n),\n         lower=binom.confint(n,sum(n),method=\"bayes\")$lower,\n         upper=binom.confint(n,sum(n),method=\"bayes\")$upper) %&gt;%\n  ggplot(aes(y=prop_n,x=BIBLE_RELIG)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper),\n                  colour=\"blue\",alpha=0.5,size=.1) +\n  facet_wrap(~SEXFREQ,scales=\"free_x\") + \n  theme_tufte() +\n  scale_y_continuous(labels=scales::percent_format(accuracy = 1)) +\n  scale_colour_viridis_d() +\n  labs(y=\"Percent Reporting\",x=\"Beliefs About the Bible\") + \n  coord_flip() +\n  ggtitle(\"About how often did you have sex during the last 12 months?\",subtitle=\"Data from General Social Survey\")\n\n\n\n\n\n\n\n\nAgain, the results are similar and if anything even stronger. Those who hold more conservative beliefs about the Bible are more likely to abstain from sex and report very high levels of weekly sexual activity. Again, the differences are much less pronounced for intermediate levels."
  },
  {
    "objectID": "post/religion_and_sex/index.html#religion-marriage-and-sex",
    "href": "post/religion_and_sex/index.html#religion-marriage-and-sex",
    "title": "Which Religious Groups Have the Most Sex?",
    "section": "Religion, Marriage and Sex",
    "text": "Religion, Marriage and Sex\nThere is one more difference I will examine before wrapping up this exercise. One issue with the above analysis is that it does not take into account marriage. We might expect that marriage would explain why some religious people do not engage in sex, i.e., beliefs that sex outside of marriage is impermissible. For that reason, in this section I do a further analysis and compare Protestants, Catholics and the unaffiliated by whether or not they are married. To do so I create a new variable in which I combine marriage status and religious status.\n\nGSS %&gt;% \n  filter(!is.na(SEXFREQ),\n         !is.na(RELIG),\n         YEAR&gt;2012,\n         RELIG %in% c(\"None\",\"Protestant\",\"Catholic\")) %&gt;% \n  mutate(RELIG=fct_collapse(RELIG,Christian=c(\"Protestant\",\"Catholic\")),\n         MAR_RELIG=case_when(RELIG==\"None\" & MARITAL==\"Married\"~\"No religion - Married\",\n                             RELIG==\"None\"~\"No religion - Unmarried\",\n                             RELIG==\"Christian\" & MARITAL==\"Married\"~\"Christian - Married\",\n                             RELIG==\"Christian\"~\"Christian - Unmarried\"),\n         MAR_RELIG=ordered(MAR_RELIG, levels=c(\"No religion - Married\",\n                                               \"Christian - Married\",\n                                               \"No religion - Unmarried\",\n                                               \"Christian - Unmarried\"))) %&gt;% \n  filter(!is.na(MAR_RELIG),MAR_RELIG!=\"Other\") %&gt;% \n  group_by(SEXFREQ,MAR_RELIG) %&gt;% \n  count() %&gt;% \n  group_by(MAR_RELIG) %&gt;% \n  mutate(prop_n=n/sum(n),\n         lower=binom.confint(n,sum(n),method=\"bayes\")$lower,\n         upper=binom.confint(n,sum(n),method=\"bayes\")$upper) %&gt;%\n  ggplot(aes(y=prop_n,x=MAR_RELIG)) +\n  geom_pointrange(aes(ymin=lower,ymax=upper),\n                  colour=\"blue\",alpha=0.5,size=.1) +\n  facet_wrap(~SEXFREQ,scales=\"free_x\") + \n  theme_tufte() +\n  scale_y_continuous(labels=scales::percent_format(accuracy = 1)) +\n  scale_colour_viridis_d() +\n  labs(y=\"Percent Reporting\",x=\"Religion and Marital Status\") + \n  coord_flip() +\n  ggtitle(\"About how often did you have sex during the last 12 months?\",subtitle=\"Data from General Social Survey (2013 - 2018).\")\n\n\n\n\n\n\n\n\nWe see that this plot helps clarifies things. Virtually all the differences between religious and non-religious people have to do with marriage. Married religious and non-religious people have sex at roughly similar frequencies; married religious people in fact report very frequent sex at slightly higher rates than non-religious paper. For unmarried people, however, the differences can be stark. Over 40% of unmarried Christian respondents report no sexual activity in the past year, versus fewer than 30% for non-religious people.\nAgain, and quite interestingly, the differences for unmarried people are only pronounced for either abstention or more frequent sexual activity (one week or more). For more monthly sexual activity there do not appear to be differences between religious and non-religious people."
  },
  {
    "objectID": "post/religion_and_sex/index.html#conclusion",
    "href": "post/religion_and_sex/index.html#conclusion",
    "title": "Which Religious Groups Have the Most Sex?",
    "section": "Conclusion",
    "text": "Conclusion\nThere are indeed profound differences in the rates of sexual activity by religious affiliation, even just reported membership in a religious tradition. The differences become much more pronounced when we take into account religious beliefs and religious practices, but it is clear that marriage is arguably the most important factor. Married people report largely similar levels of sexual activity regardless of religious inclination, and if anything religious married people may have very frequent sex more often than non-religious married people.\nIt should be clear by now that making these comparisons is difficult. I did not look at income, people’s family history or number of children in their marriage, all of which could affect sexual activity. Instead, I wanted to draw some basic descriptive findings, which I think can help frame this question. It also provides some important evidence that sexual activity and religion do appear to be closely intertwined, even if it’s not always clear exactly what is driving the differences."
  },
  {
    "objectID": "post/maternity21/index.html",
    "href": "post/maternity21/index.html",
    "title": "Why You Should Be Careful About the MaterniT 21 Test",
    "section": "",
    "text": "Interested in more writing on the shady dealings of big business? Check out my just-released book with Cambridge University Press and use discount code KUBINEC23 to get 20% off.\nMy wife and I have been faced with a decision in our pregnancies that has always caused me some consternation: should we take the MaterniT 21 test to see if our baby might have Down’s syndrome (trisomy 21) or other genetic abnormalities? This test, marketed by the company LabCorp, offers a way to analyze the blood of a pregnant woman for DNA markers that could indicate genetic problems. While non-invasive, it isn’t cheap (at least in the United States), and if it returns a positive result, it could be used for more invasive procedures like amniocentesis that could be harmful to the fetus.\nSo while it might seem like a good idea (let’s learn as much as we can), the test can only guide decisions if it’s reasonably accurate. However, due to corporate greed and manipulated statistics, people put way too much trust in the test when they probably shouldn’t. As a result, women and their partners may be led to make unwise decisions based on misplaced confidence in the test.\nIn this post I explain why you shouldn’t buy into the company’s marketing about the test’s accuracy–in fact, LabCorp is patently misleading consumers about how useful its test is for diagnosing Down’s syndrome in utero. It will take a bit of time to read through this post, so I’ll give the highlights first:"
  },
  {
    "objectID": "post/maternity21/index.html#investigating-labcorp",
    "href": "post/maternity21/index.html#investigating-labcorp",
    "title": "Why You Should Be Careful About the MaterniT 21 Test",
    "section": "Investigating LabCorp",
    "text": "Investigating LabCorp\nLabCorp (under its subsidiary Integrated Genetics) offers a brochure for the test on its website. This brochure mentions how accurate the test is in only one place:\n\n\n\n\n\n\n\n\n\nWhile they don’t define what the technical terms are in the parentheses, 97.9% does sound pretty good. In another part of the brochure, the company says that “the test delivers clear positive or negative results.” I take this as enough evidence to say that LabCorp wants women to believe that the test will determine whether the fetus has Down’s syndrome with reasonable confidence, such as a greater than 90% probability.\nNow for the fun part–I’m going to break down what is meant by the number cited in the brochure above and trace it back to the original research to see what it actually means. You may have noticed the footnote in the image above–that is a reference to an article published in 2014 that evaluated tests like MaterniT 21, known as cell-free DNA tests. A scan of the article’s results shows that LabCorp did indeed get the number right–97.9% for trisomy 21. They did not, of course, mention that the article found much lower rates of detection for other trisomies and genetic conditions, but for now we’ll ignore that oversight. The test is marketed as a way to detect Down’s syndrome, so let’s focus on just that as the test’s selling point.\nAs I was doing research, I found a more recent paper that did a meta-analysis, which is a compilation of the results of all previous research, in this case on tests like MaterniT 21. This paper is quite useful for evaluating the test as it aggregates information from many different researchers, lowering the concern that any one study was a fluke. It turns out that this meta-analysis has a number very close to what the brochure article cited – 99.2%, i.e., even higher.\nAt this point you may think, “your concerns about this test seem unfounded.” Ah, but we have not yet discussed what these numbers actually mean, which if you notice, the brochure did not provide any more information about. All it said was 97.9% positive-predictive value for a “high-risk cohort”.\nMy intuition is that relatively few readers of the brochure would have any idea what that refers to. Most people just want to know how accurate the test is, and 97.9% sounds really high, so it must be good. In reality, it is much harder to know how well a test works, because the usefulness of a test depends on how hard it is to find what the test detects.\nExplaining this point is difficult, so I’ll use as an example that is easier to relate to. I’m sure you’ve come across the hand-held metal detectors of the kind that people like to use for treasure hunting on beaches (see picture below). Suppose you are a person who has taken up an interest in treasure hunting and you want to know how useful these metal detector will be in helping you find Blackbeard’s long-lost hoard.\nWhat you really want to know is, if the thing beeps, does it mean you’ve found treasure? The answer to this question just so happens to be the same calculation as the “positive-predictive value” that the test company put in their brochure.\n\n\n\n\n\n\n\n\n\nThe detector can easily go wrong if it beeps when it comes across a worthless piece of scrap metal. On the other hand, the detector could also fail if it didn’t beep because a treasure hoard was buried too deep. The first case is called a false positive: the machine beeped but you didn’t find any treasure. The second case is called a false negative: you were standing on treasure and the machine didn’t help you out one bit.\nSo let’s get back to the brochure. The positive predictive value is equal to the chance that your metal detector’s beep means you hit pay dirt. But if you ask a metal detector salesperson what the chance is that you find treasure if the machine beeps, you might get some evasive answers/sarcastic smirks. He or she certainly won’t be able to tell you how likely it is that you find treasure, unless… the salesperson actually knows how much treasure there is in the sand (in which case you might wonder why they are still a salesperson). The point is, you can only calculate a positive predictive value for your detector if you know how likely it is you’ll be standing on treasure at any point along the beach. If treasure is impossibly rare–which it probably is, I hate to break it to you–you’ll be digging up a lot of empty sand or rusty beer cans even if the machine’s sensors are working fine.\nThe point of this whole analogy is that knowing what you should do if the machine beeps depends on how much treasure is in the sand. If you knew that you were standing right next to a sunken pirate ship, you would probably want to dig up every beep you hear. If you’re on the Jersey shore and the sand is littered with old Dr. Pepper cans and fish hooks, you might not want to dig up anything unless you need the exercise.\nNow I can finally explain the test company’s deception. They provided a positive predictive value, which is what you want to know: what’s the chance the fetus has Down’s syndrome if the test comes back positive? But LabCorp pulled this number from a high-risk cohort. Doing so is no different than selling you a metal detector and telling you it’ll detect treasure 97% percent of the time so long as you’re standing on Blackbeard’s grave.\nSo where did this high-risk cohort come from? What is very important to know (and completely ignored by the test brochure) is the fact that the risk of Down’s syndrome increases dramatically with age. The chart below is from the CDC, and you can see that the rate of a live birth with Down’s syndrome increases very rapidly once a woman reaches 40 years of age.\n\n\n\n\n\n\n\n\n\nNow let’s go back to the papers they cited as proof of the positive predictive value. I’ll only focus on the meta-analysis as it represents a lot of combined research. Unfortunately, and to me somewhat surprisingly, the paper does not report the median age of women in the study. However, because we know the actual number of women who had children with Down’s syndrome from the paper, we can figure out how old the women were by calculating the percentage who ended up having babies with Down’s. I calculate this in the code below based on a table of data in the article. The code is shown for transparency’s sake, and I include the full table at the end of this blog post as a reference.\n\njournal_data &lt;- readxl::read_xlsx(\"downs_table.xlsx\")\nprint(paste0(\"Percentage of live births with Down's Syndrome in the meta-analysis study is: \",round(sum(journal_data$positives)/(sum(journal_data$positives) + sum(journal_data$negatives)),3)*100,\"%\"))\n\n[1] \"Percentage of live births with Down's Syndrome in the meta-analysis study is: 4.6%\"\n\n\nA rate of 4.6% of babies with Down’s syndrome means that the women in the study were probably 46 years old on average, according to data from the National Down Syndrome Society. By comparison, the CDC says that the rate of Down’s syndrome among newborns for the population as a whole is only 0.001%. The fetuses in the studies in this meta-analysis had a rate of Down’s syndrome 50 times higher than average. In other words, the testing company used the equivalent of a beach full of buried treasure to evaluate the usefulness of its metal detector.\nAt this point, you might start to feel suspicious about the articles I cited. Were the scientists running these studies in cahoots with the testing company? Probably not. Doing these kinds of studies is expensive–they have to enroll women, then test them, then evaluate their babies after delivery–and if they had an average set of women, they wouldn’t have very many fetuses that would ultimately turn out to have Down’s, requiring them to enroll many more subjects in the study. In a similar sense, if you were designing a metal detector, you might go out and test it by burying some gold coins in the beach and see if it works rather than stroll aimlessly around until you found treasure. So we shouldn’t necessarily blame the scientists, though we certainly should question the LabCorp’s intentionally deceptive use of the study’s statistics in selling the test.\nWe can get more useful positive predictive values by re-calculating the results from the article above and adjusting the Down’s syndrome prevalence to match younger women’s ages. I calculate this in the code block below and plot the positive predictive value by age:\n\njournal_data &lt;- mutate(journal_data,detection=as.numeric(stringr::str_extract(detection,\"[0-9]+\")),\n                       fpr=as.numeric(stringr::str_extract(fpr,\"[0-9]+\")))\n\nsensitivity &lt;- sum(journal_data$positives)/(sum(journal_data$positives) + (sum(journal_data$positives) - sum(journal_data$detection)))\n\nspecificity &lt;- sum(journal_data$negatives)/(sum(journal_data$negatives) + sum(journal_data$fpr))\n\n# need Down's syndrome prevalence data by age\n# Data taken from: https://journals.sagepub.com/doi/abs/10.1136/jms.9.1.2\n# Using predicted values from model vs. observed values\n# Morris, Dutton and Alberman (2002)\n\nage_trends &lt;- readxl::read_xlsx(\"downs_estimates.xlsx\") %&gt;% filter(age&gt;13, age&lt;53) %&gt;% \n  mutate(pred_ods=as.numeric(stringr::str_remove(pred_ods,\"1:\")),\n         # convert odds to percentages\n         pred_ods=1/pred_ods,\n         # false positive rate\n         fp=(1 - pred_ods)*(1 - specificity),\n         ppv=pred_ods / (pred_ods + fp))\n\nage_trends %&gt;% \n  ggplot(aes(y=ppv,x=age)) +\n  geom_line() + \n  ggtitle(\"Chance of Fetus with Downs if MaterniT 21 Comes Back Positive\") +\n  scale_y_continuous(labels=scales::percent) + \n  geom_hline(yintercept=0.5,linetype=2) +\n  geom_hline(yintercept=0.979,linetype=3,colour=\"red\") +\n  theme(panel.background = element_blank()) +\n  ylab(\"Chance of Fetus with Down's Syndrome\") +\n  xlab(\"Age of Mother\") +\n  annotate(\"text\",x=c(20,42),y=c(.52,.99),label=c(\"50%\",\"LabCorp Brochure: 97.9%\"))\n\n\n\n\n\n\n\n\nAs can be seen, the chance that a fetus has Down’s Syndrome if the test comes back positive–the positive predictive value–does not start to rise sharply until a woman is in her late 30s and 40s. Even in her early 30s, a positive test result only means there is a 50 percent chance that the fetus has Down’s Syndrome. I calculated these numbers from the meta-analysis that showed the test was pretty accurate, so the numbers could even be lower with the results of a different, less optimistic study.\nYou can see now that the test company’s positive predictive value of 97.9% only applies to women at the far end of the curve–around age 42 or 43 and higher. For the vast majority of women in child-bearing years, this number is simply inaccurate. Technically, the test company can claim they included the proviso “in a high-risk cohort”, but it seems very unlikely anyone would go as far as I did to know what that means. And it turns out, it means a lot. For women under 30, it’s still more likely than not that their fetus is free of Down’s even if the test comes back positive. If women and their partners were told, “even if the test comes back positive, there’s still only a 40 percent chance the fetus has Down’s Syndrome”, I do wonder how many people would still choose to take it.\nThe real punch, though, is that this non-invasive test can be used as a reason to do a more dangerous, invasive test. One potential test a doctor might recommend next, amniocentesis, has a risk of miscarriage as high as 1 percent. Let’s suppose that a million women under 30 take this test. According to my analysis above, there would probably be 832 false positive results (test results that came back positive but the fetus does not have Down’s Syndrome). If everyone went on to do amniocentesis to confirm the diagnosis, that would result in 8 miscarriages of otherwise healthy pregnancies. Considering that there are about 4 million live births per year in the United States, and the average age of a woman giving birth is 27, these disturbing numbers are quite plausible.\nWhile it would seem that most people (and their doctors) would choose more invasive testing to confirm Down’s before reaching conclusions, it still strikes me as entirely plausible that someone would consider aborting the fetus based on the first, non-invasive positive result. That particular possibility frightens me, that someone might take the first relatively inaccurate test as a reason to terminate a pregnancy.\nFor all these reasons, do share this research with others. It’s important to know what tests really do, and also to pressure Congress and regulators to force test providers to release clear and non-misleading statistics. It should be as easy for them to compile a chart like the one I did above. In fact, these kinds of plots should be included in all scholarly research on tests to avoid people mis-characterizing results."
  },
  {
    "objectID": "post/maternity21/index.html#downs-syndrome-data-table",
    "href": "post/maternity21/index.html#downs-syndrome-data-table",
    "title": "Why You Should Be Careful About the MaterniT 21 Test",
    "section": "Down’s Syndrome Data Table",
    "text": "Down’s Syndrome Data Table\nThis table derived from Table 2 in Gil, Quezada, Revello, Akolekar, and Nicolaides (2015).\n\n\n\n\n\nStudy\nMethod\nGA\nTrue Positives\nDetected\nTrue Negatives\nFalse Positives\n\n\n\n\nChiu (2011)41\nMPSS\n13 (—)\n86\n86\n146\n3\n\n\nEhrich (2011)42\nMPSS\n16 (8–36)\n39\n39\n410\n1\n\n\nPalomaki (2011)43\nMPSS\n15 (8–21)\n212\n209\n1471\n3\n\n\nSehnert (2011)44\nMPSS\n15 (10–28)\n13\n13\n34\n0\n\n\nAshoor (2012)45\nCSS\n12 (11–13)\n50\n50\n347\n0\n\n\nBianchi (2012)46\nMPSS\n15 (10–23)\n89\n89\n404\n0\n\n\nJiang (2012)48\nMPSS\n— (10–34)\n16\n16\n887\n0\n\n\nLau (2012)49\nMPSS\n12 (11–28)\n11\n11\n97\n0\n\n\nNicolaides (2012)50\nCSS\n12 (11–13)\n8\n8\n1941\n0\n\n\nNorton (2012)51\nCSS\n16 (10–38)\n81\n81\n2888\n1\n\n\nSparks (2012)53\nCSS\n18 (11–36)\n36\n36\n131\n0\n\n\nGuex (2013)55\nMPSS\n12 (11–13)\n30\n30\n146\n0\n\n\nLiang (2013)57\nMPSS\n21 (11–39)\n39\n39\n367\n0\n\n\nNicolaides (2013)59\nSNP\n13 (11–13)\n25\n25\n204\n0\n\n\nSong (2013)61\nMPSS\n16 (11–21)\n8\n8\n1733\n0\n\n\nVerweij (2013)62\nCSS\n14 (10–28)\n18\n17\n486\n0\n\n\nBianchi (2014)63\nMPSS\n17 (8–39)\n5\n5\n1947\n6\n\n\nComas (2014)64\nCSS/SNP\n14 (9–23)\n4\n4\n311\n0\n\n\nPergament (2014)71\nSNP\n14 (7–40)\n58\n58\n905\n0\n\n\nPorreco (2014)72\nMPSS\n17 (9–37)\n137\n137\n3185\n3\n\n\nShaw (2014)73\nMPSS\n&gt; 12\n11\n11\n184\n0\n\n\nStumm (2014)74\nMPSS\n15 (11–32)\n41\n40\n430\n0\n\n\nQuezada (2015)75\nCSS\n10 (10–11)\n32\n32\n2753\n1\n\n\nSong (2015)76\nMPSS\n9 (8–12)\n2\n2\n201\n0"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "On this page you can see a list of my articles, books, and other published works (see table of contents on the left) along with associated code and datasets. You can view my CV here. My research has focused on the following questions that span the political economy of development, data science, causal inference and Middle East politics:\nOften these questions overlap, such as when I use new statistical tools I develop to study corruption in the MENA region and elsewhere. My R packages, ordbetareg and idealstan, both came out of my research in studying online experiments and legislature behavior, and have also been used in other studies to date. I am both a skeptic of data science & AI and a developer of data-scientific methods because I see great promise and peril in quantification.",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/index.html#journal-articles",
    "href": "research/index.html#journal-articles",
    "title": "Research",
    "section": "Journal articles",
    "text": "Journal articles\n\n\n\n    \n        \n            \n                Robert Kubinec and Helen Milner, “Taxes in the Time of Revolution: An Experimental Test of the Rentier State during Algeria's Hirak,” World Politics 76, no. 2 (April 2024): 294-333, doi: 10.1353/wp.2024.a924508\n            \n\n            \n            \n                \n                    \n                            Political Economy\n                        \n                    \n                    \n                            Middle East Politics\n                        \n                    \n                    \n                            Online Experiments\n                        \n                    \n                    \n                            Bayesian Statistics\n                        \n                    \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code & Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Research",
    "section": "Working papers",
    "text": "Working papers",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/index.html#book-chapters",
    "href": "research/index.html#book-chapters",
    "title": "Research",
    "section": "Book chapters",
    "text": "Book chapters\n\n\n\n    \n        \n            \n                Suparna Chaudhry and Andrew Heiss, “Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on NGOs,” chap. 2 in Beyond the Boomerang: New Patterns in Transcalar Advocacy, eds. Christopher L. Pallas and Elizabeth Bloodgood (Tuscaloosa, AL: University of Alabama Press, 2022).\n            \n\n            \n            \n                \n                    \n                            Civil society\n                        \n                    \n                    \n                            NGO regulations\n                        \n                    \n                    \n                            Human rights\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Governments threatened—\n            \n                 / NGO regulations\n            \n                 / shift funding; missions.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint (PDF)\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint (HTML)\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Analysis notebook\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, “Causal Inference,” chap. 10 in R for Political Data Science: A Practical Guide, ed. Francisco Urdinez and Andrés Cruz (Boca Raton, Florida: Chapman and Hall / CRC, 2021), 235–274, doi: 10.1201/9781003010623-10.\n            \n\n            \n            \n                \n                    \n                            Causal inference\n                        \n                    \n                    \n                            Methods\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                DAGs, inference, R!\n            \n                 / Observational data?\n            \n                 / Tell causal stories!\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, “NGOs and Authoritarianism,” chap. 38 in Routledge Handbook of NGOs and International Relations, ed. Thomas Davies (London: Routledge, 2019).\n            \n\n            \n            \n                \n                    \n                            NGOs\n                        \n                    \n                    \n                            Civil society\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n                    \n                            Authoritarianism\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                It’s complicated.\n            \n                 / Dictators love NGOs,\n            \n                 / but also they don’t.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Tana Johnson and Andrew Heiss, “Liberal Institutionalism,” chap. 8 in International Organization and Global Governance, 2nd ed., ed. Thomas G. Weiss and Rorden Wilkinson (London: Routledge, 2018), 123–34, doi: 10.4324/9781315301914.\n            \n\n            \n            \n                \n                    \n                            International relations\n                        \n                    \n                    \n                            Institutions\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                Stuck between “ism”s,\n            \n                 / liberal global theory\n            \n                 / has rich past; future.\n            \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/index.html#reviews",
    "href": "research/index.html#reviews",
    "title": "Research",
    "section": "Reviews",
    "text": "Reviews\n\n\n\n    \n        \n            \n                Andrew Heiss, review of Meghan Elizabeth Kallman, The Death of Idealism: Development and Anti-Politics in the Peace Corps, Contemporary Sociology, 50, no. 6 (November 2021): 486–88, doi: 10.1177/00943061211050046g.\n            \n\n            \n            \n                \n                    \n                            International development\n                        \n                    \n                    \n                            Foreign aid\n                        \n                    \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint (PDF)\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint (HTML)\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Andrew Heiss, review of Taco Brandsen, Willem Trommel, and Bram Verschuere (eds.), Manufacturing Civil Society: Principles, Practices, and Effects, Voluntas: International Journal of Voluntary and Nonprofit Organizations 26, no. 2 (April 2014): 728–30, doi: 10.1007/s11266-014-9541-3.\n            \n\n            \n            \n                \n                    \n                            Civil society\n                        \n                    \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/index.html#dormant-working-papers",
    "href": "research/index.html#dormant-working-papers",
    "title": "Research",
    "section": "Dormant working papers",
    "text": "Dormant working papers\n\n\n\n    \n        \n            \n                \"Sources of Advocacy: When Does the Media Give Voice to Egyptian Advocacy NGOs?\" (with Ken Rogerson, Duke University)\n            \n\n            \n            \n                \n                    \n                            Egypt\n                        \n                    \n                    \n                            Text analysis\n                        \n                    \n                    \n                            Topic modeling\n                        \n                    \n                    \n                            NGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                \"Discovering Discourse: The Relationship between Media and NGOs in Egypt between 2011–13\" (with Ken Rogerson, Duke University)\n            \n\n            \n            \n                \n                    \n                            Egypt\n                        \n                    \n                    \n                            Text analysis\n                        \n                    \n                    \n                            Topic modeling\n                        \n                    \n                    \n                            NGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/index.html#selected-seminar-papers",
    "href": "research/index.html#selected-seminar-papers",
    "title": "Research",
    "section": "Selected seminar papers",
    "text": "Selected seminar papers\n\n\n\n    \n        \n            \n                “Do Democracies Discourage NGO Cooperation?\"Replication and extension of Amanda Murdie. 2014. “Scrambling for Contact: The Determinants of Inter-NGO Cooperation in Non-Western Countries.\" Review of International Organizations 9, no. 3 (September): 309–31.\n            \n\n            \n            \n                \n                    \n                            INGOs\n                        \n                    \n                    \n                            NGOs\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                “Explaining Support for Undemocratic Leaders in Democracies in the Middle East”Replication and extension of Amaney Jamal and Mark Tessler. 2008. “Attitudes in the Arab World.\" Journal of Democracy 19, no. 1 (January): 97–110.\n            \n\n            \n            \n                \n                    \n                            Authoritarianism\n                        \n                    \n            \n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Raw output\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Poster\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/index.html#translations",
    "href": "research/index.html#translations",
    "title": "Research",
    "section": "Translations",
    "text": "Translations\n\n\n\n    \n        \n            \n                Abdel Samad, Hamid. 2011. \"Farewell, Heaven (Wadaʿan Ayatuha al-Samaaʾ / وداعا أيتها السماء).\" In The Literary Life of Cairo: One Hundred Years in the Heart of the City, edited by Samia Mehrez, translated by Andrew Heiss, 347–50. Cairo: American University in Cairo Press.\n            \n\n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Book\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n        \n            \n                Prince, Mona. 2011. \"Three Suitcases for Departure (Thalatha Haqaʾib lil-Safar / ثلاثة حقائب للسفر).\" In The Literary Life of Cairo: One Hundred Years in the Heart of the City, edited by Samia Mehrez, translated by Andrew Heiss, 212–13. Cairo: American University in Cairo Press.\n            \n\n            \n\n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Book\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             PDF\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "I have been a consultant to non-profits and private companies about statistics, especially Bayesian statistics, applied data science, and online survey research & experiments. If you are interested in having me consult for your organization, please contact me via email at rkubinec@mailbox.sc.edu.",
    "crumbs": [
      "Consulting"
    ]
  },
  {
    "objectID": "ordbetareg.html",
    "href": "ordbetareg.html",
    "title": "Ordered Beta Regression",
    "section": "",
    "text": "Proportion data–percentiles, slider scales, visual-analog scales–have long puzzled statisticians because they combine two things: a continuous measure (the proportion) and a discrete measure (the top value of 1 or 100% and the bottom value of 0 or 0%). Conventional statistical models like the oft-used OLS regression implicitly assume these boundaries do not exist; this means an OLS regression can predict to absurd values like 115% of patients being sick or -5% of legislators being elected.\nTo address this issue, I developed the ordered beta regression model that combines two things: beta regression, which is defined for any bounded continuous scale, and ordered logit, which works for discrete categories. By doing this, you can fit an ordered beta regression for any percentile/proportion for both the middle continuous part and the bounds of the scale. This model can also predict within this scale as well.\nI describe the model and how to do this type of modeling in this blog post. For more detail on the inner workings of the model, I refer you to the paper:\nKubinec, Robert. 2023. “Ordered Beta Regression: A Parsimonious, Well-Fitting Model for Continuous Data with Lower and Upper Bounds.” Political Analysis 31(4): 519–36. doi: http://doi.org/10.1017/pan.2022.20.\nFor an ungated version of the article, see the pre-publication draft on OSF:\nhttps://osf.io/preprints/socarxiv/2sx6y",
    "crumbs": [
      "Ordered Beta Regression"
    ]
  },
  {
    "objectID": "idealstan.html",
    "href": "idealstan.html",
    "title": "idealstan",
    "section": "",
    "text": "The idealstan R package is designed to implement generalized ideal point models, offering flexible tools for analyzing social choice processes, especially in political science. It supports inference for time-varying ideal points, handles non-ignorable missing data through a two-stage selection model, and accommodates mixed data types (binary, ordinal, continuous). The package is particularly suited for large datasets, employing efficient Bayesian inference techniques like variational inference and MCMC parallelization. The models can be used to assess latent traits in various social contexts, beyond traditional legislative analysis.\nAt present this R package is under active development and a new stable version will be released soon. The current version can be accessed via the develop branch of the package Github repo: https://github.com/saudiwin/idealstan/tree/develop.",
    "crumbs": [
      "idealstan"
    ]
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-links",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-links",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#abstract",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#abstract",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Abstract",
    "text": "Abstract\nAn increasing number of countries have recently cracked down on non-governmental organizations (NGOs). Much of this crackdown is sanctioned by law and represents a bureaucratic form of repression that could indicate more severe human rights abuses in the future. This is especially the case for democracies, which unlike autocracies, may not aggressively attack civic space. We explore if crackdowns on NGOs predict broader human rights repression. Anti-NGO laws are among the most subtle means of repression and attract lesser domestic and international condemnation compared to the use of violence. Using original data on NGO repression, we test whether NGO crackdown is a predictor of political terror, and violations of physical integrity rights and civil liberties. We find that while de jure anti-NGO laws provide little information in predicting future repression, their patterns of implementation—or de facto civil society repression—predicts worsening respect for physical integrity rights and civil liberties."
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-figures",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#important-figures",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 6: Marginal effects of changing levels of civil society repression on the probability of specific levels of political terror and predicted latent human rights values\n\n\n\n\n\nFigure 7: The disconnect between Egypt’s de jure 2002 law and the widespread de facto repression of civil society a decade later"
  },
  {
    "objectID": "research/articles/chaudhry-heiss-ngos-repression/index.html#citation",
    "href": "research/articles/chaudhry-heiss-ngos-repression/index.html#citation",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Citation",
    "text": "Citation\n\n{{&lt; ai zotero &gt;}} Add to Zotero {{&lt; fa square-arrow-up-right &gt;}}\n\n@article{ChaudhryHeiss:2022,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Doi = {10.1080/14754835.2022.2030205},\n    Journal = {Journal of Human Rights},\n    Number = {2},\n    Pages = {123--140},\n    Title = {NGO Repression as a Predictor of Worsening Human Rights Abuses},\n    Volume = {21},\n    Year = {2022}}"
  },
  {
    "objectID": "research/index.html#books",
    "href": "research/index.html#books",
    "title": "Research",
    "section": "Books",
    "text": "Books\n\n\n\n    \n        \n            \n                Kubinec, Robert. Making Democracy Safe for Business: Corporate Politics During the Arab Uprisings. Cambridge University Press: 2023. doi: 10.1017/9781009273541\n            \n\n            \n            \n                \n                    \n                            Political Economy\n                        \n                    \n                    \n                            Corruption\n                        \n                    \n                    \n                            Middle East Politics\n                        \n                    \n                    \n                            Online Experiments\n                        \n                    \n                    \n                            Causal Graphs\n                        \n                    \n            \n            \n            \n            \n                \n                    \n                        \n                             Full details »\n                        \n                    \n                    \n                        \n                    \n                        \n                        \n                        \n                             Preprint\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Final version\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Code & Data\n                        \n                    \n                        \n                    \n                        \n                        \n                        \n                             Add to Zotero\n                        \n                    \n                        \n                    \n                \n            \n        \n        \n\n\n\nNo matching items",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/books/kubinec-2023/index.html#important-links",
    "href": "research/books/kubinec-2023/index.html#important-links",
    "title": "Making Democracy Safe for Business: Corporate Politics During the Arab Uprisings",
    "section": "Important links",
    "text": "Important links\n\nPodcast about the book\nOnline Article Draft\nGitHub repository with code and data"
  },
  {
    "objectID": "research/books/kubinec-2023/index.html#abstract",
    "href": "research/books/kubinec-2023/index.html#abstract",
    "title": "Making Democracy Safe for Business: Corporate Politics During the Arab Uprisings",
    "section": "Abstract",
    "text": "Abstract\nBusinesses in the Middle East and North Africa have failed to bring sustainable development despite decades of investment from the private and public sectors. Yet we still know little about why the Arab Uprisings failed to usher in more transparent government that could break this enduring cycle of corruption and mismanagement. Examining posttransition politics in Egypt and Tunisia, Kubinec employs interviews and quantitative surveys to map out the corrupting influence of businesses on politics. He argues that businesses must respond to changes in how perks and privileges are distributed after political transitions, either by forming political coalitions or creating new informal connections to emerging politicians. Employing detailed case studies and original experiments, Making Democracy Safe for Business advances our empirical understanding of the study of the durability of corruption in general and the dismal results of the Arab Uprisings in particular."
  },
  {
    "objectID": "research/books/kubinec-2023/index.html#data-and-code",
    "href": "research/books/kubinec-2023/index.html#data-and-code",
    "title": "Making Democracy Safe for Business: Corporate Politics During the Arab Uprisings",
    "section": "Data and code",
    "text": "Data and code\nThe analyses in the book can be reproduced with R code and data available at GitHub."
  },
  {
    "objectID": "research/books/kubinec-2023/index.html#citation",
    "href": "research/books/kubinec-2023/index.html#citation",
    "title": "Making Democracy Safe for Business: Corporate Politics During the Arab Uprisings",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@book{kubinec2023democracy,\n    Author = {Robert Kubinec},\n    Doi = {10.1017/9781009273541},\n    Publisher = {Cambridge University Press},\n    Title = {Making Democracy Safe for Business: Corporate Politics During the Arab Uprisings},\n    Year = {2023}}"
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-repression/index.html",
    "href": "research/essays/chaudhry-heiss-ngos-repression/index.html",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-repression/index.html#important-links",
    "href": "research/essays/chaudhry-heiss-ngos-repression/index.html#important-links",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-repression/index.html#abstract",
    "href": "research/essays/chaudhry-heiss-ngos-repression/index.html#abstract",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Abstract",
    "text": "Abstract\nAn increasing number of countries have recently cracked down on non-governmental organizations (NGOs). Much of this crackdown is sanctioned by law and represents a bureaucratic form of repression that could indicate more severe human rights abuses in the future. This is especially the case for democracies, which unlike autocracies, may not aggressively attack civic space. We explore if crackdowns on NGOs predict broader human rights repression. Anti-NGO laws are among the most subtle means of repression and attract lesser domestic and international condemnation compared to the use of violence. Using original data on NGO repression, we test whether NGO crackdown is a predictor of political terror, and violations of physical integrity rights and civil liberties. We find that while de jure anti-NGO laws provide little information in predicting future repression, their patterns of implementation—or de facto civil society repression—predicts worsening respect for physical integrity rights and civil liberties."
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-repression/index.html#important-figures",
    "href": "research/essays/chaudhry-heiss-ngos-repression/index.html#important-figures",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Important figures",
    "text": "Important figures\n\n\n\nFigure 6: Marginal effects of changing levels of civil society repression on the probability of specific levels of political terror and predicted latent human rights values\n\n\n\n\n\nFigure 7: The disconnect between Egypt’s de jure 2002 law and the widespread de facto repression of civil society a decade later"
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-repression/index.html#citation",
    "href": "research/essays/chaudhry-heiss-ngos-repression/index.html#citation",
    "title": "NGO Repression as a Predictor of Worsening Human Rights Abuses",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryHeiss:2022,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Doi = {10.1080/14754835.2022.2030205},\n    Journal = {Journal of Human Rights},\n    Number = {2},\n    Pages = {123--140},\n    Title = {NGO Repression as a Predictor of Worsening Human Rights Abuses},\n    Volume = {21},\n    Year = {2022}}"
  },
  {
    "objectID": "research/essays/heiss-2012/index.html",
    "href": "research/essays/heiss-2012/index.html",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "",
    "text": "In the weeks since the January 25 revolution there have been dozens of explanations for Mubarak’s downfall. Political scientists, diplomats, economists, sociologists, and even international aid workers have proposed theories to explain the economic, political, historical, and social causes of the Egyptian revolution. Despite the plethora of interdisciplinary theories in the press and in academia, few—if any—have analyzed Mubarak’s resignation in the light of managerial dynamics and behavior.\nIn addition to a ripe political environment, horrible economic conditions, and a mobilized and angry population, Mubarak’s fall from power can be attributed to his failure as a public manager. This paper analyzes Mubarak’s managerial strategy throughout the course of his presidency by (1) reviewing the structure of the National Democratic Party (NDP) and Mubarak’s relationship with it; (2) analyzing the foundational principles and assumptions of his strategy‚ both as the leader of Egypt and as the chairman of the NDP; and (3) tracing the application of that strategy and determining its effectiveness in confronting “the management challenge” over a period of 30 years."
  },
  {
    "objectID": "research/essays/heiss-2012/index.html#abstract",
    "href": "research/essays/heiss-2012/index.html#abstract",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "",
    "text": "In the weeks since the January 25 revolution there have been dozens of explanations for Mubarak’s downfall. Political scientists, diplomats, economists, sociologists, and even international aid workers have proposed theories to explain the economic, political, historical, and social causes of the Egyptian revolution. Despite the plethora of interdisciplinary theories in the press and in academia, few—if any—have analyzed Mubarak’s resignation in the light of managerial dynamics and behavior.\nIn addition to a ripe political environment, horrible economic conditions, and a mobilized and angry population, Mubarak’s fall from power can be attributed to his failure as a public manager. This paper analyzes Mubarak’s managerial strategy throughout the course of his presidency by (1) reviewing the structure of the National Democratic Party (NDP) and Mubarak’s relationship with it; (2) analyzing the foundational principles and assumptions of his strategy‚ both as the leader of Egypt and as the chairman of the NDP; and (3) tracing the application of that strategy and determining its effectiveness in confronting “the management challenge” over a period of 30 years."
  },
  {
    "objectID": "research/essays/heiss-2012/index.html#figure",
    "href": "research/essays/heiss-2012/index.html#figure",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "Figure",
    "text": "Figure\nFigure 1: NDP Leadership Organizational Chart, 2010 (based on original chart by The Arabist)\n\n\n\nFigure 1: NDP Leadership Organizational Chart, 2010"
  },
  {
    "objectID": "research/essays/heiss-2012/index.html#citation",
    "href": "research/essays/heiss-2012/index.html#citation",
    "title": "The Failed Management of a Dying Regime: Hosni Mubarak, Egypt’s National Democratic Party, and the January 25 Revolution",
    "section": "Citation",
    "text": "Citation\n@article{Heiss:2012,\n    Author = {Andrew Heiss},\n    Issue = {Spring},\n    Journal = {Journal of Third World Studies},\n    Number = {1},\n    Pages = {155-171},\n    Title = {The Failed Management of a Dying Regime: {Hosni Mubarak}, {Egypt's National Democratic Party}, and the {January} 25 Revolution},\n    Volume = {28},\n    Year = {2012}}"
  },
  {
    "objectID": "research/essays/peck-heiss-2021/index.html",
    "href": "research/essays/peck-heiss-2021/index.html",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/essays/peck-heiss-2021/index.html#important-links",
    "href": "research/essays/peck-heiss-2021/index.html#important-links",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "",
    "text": "Paper (preprint)\nStatistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/essays/peck-heiss-2021/index.html#abstract",
    "href": "research/essays/peck-heiss-2021/index.html#abstract",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Abstract",
    "text": "Abstract\nEcological theorists have generated several yet unresolved disputes that try to untangle the difficulty in understanding the nature of complex ecological communities. In this paper, we combine two recent theoretical approaches that used together suggest a promising way to consider how evolutionary and ecological processes may be used to frame a general theory of community ecology and its functional stability. First, we consider the theoretical proposal by Mark Vellend (2016) to focus on a small set of higher-level evolutionary and ecological processes that act on species within an ecological community. These processes provide a basis for ecological theory similar to the way in which theoretical population genetics has focused on a small set of mathematical descriptions to undergird its theory. Second, we explore ideas that might be applied to ecosystem functioning developed by Alvaro Moreno and Matteo Mossio’s (2015) work on how biologically autonomous systems emerge from closure of relevant constraints. To explore the possibility that combining these two ideas may provide a more general theoretical understanding of ecological communities, we have developed a stochastic, agent-based model, with agents representing species, that explores the potential of using evolutionary and ecological processes as a constraint on the flow of species through an ecosystem. We explore how these ideas help illuminate aspects of stability found in many ecological communities. These agent-based modeling results provide in-principle arguments that suggest that constraint closure, using evolutionary and ecological processes, explains general features of ecological communities. In particular, we find that our model suggests a perspective useful in explaining repeated patterns of stability in ecological evenness, species turnover, species richness, and in measures of fitness."
  },
  {
    "objectID": "research/essays/peck-heiss-2021/index.html#important-figure",
    "href": "research/essays/peck-heiss-2021/index.html#important-figure",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Important figure",
    "text": "Important figure\nFigure 4: Boxplots show ecological measures across parameter space for different numbers of constraints generated by 2000 iterations of the simulation model across all 64 possible permutations of the proposed constraints being present or absent, and with uniform random values of parameters pulled from their possible values in the model iterations. The whiskers in the box plot show the range of variation. The actual data used to construct the boxplots are shown as fine gray dots to the right of each box plot. (a) Effect of the number of constraints on landscape fitness. (b) Effect of the number of constraints on ecological evenness. (c) Effect of the number of constraints on the average cell species richness (number of species defined as functional groups) present in the cell-niche. (d) Effect of the number of constraints on the difference between turnover in mutualistic linked and unlinked species. The turnover rate is the number of new species created at each time step in each cell.\n\n\n\nFigure 4: Boxplots show ecological measures across parameter space for different numbers of constraints generated by 2000 iterations of the simulation model across all 64 possible permutations of the proposed constraints being present or absent"
  },
  {
    "objectID": "research/essays/peck-heiss-2021/index.html#citation",
    "href": "research/essays/peck-heiss-2021/index.html#citation",
    "title": "Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{PeckHeiss:2021,\n    Author = {Steven L. Peck and Andrew Heiss},\n    Doi = {10.1111/oik.07621},\n    Journal = {Oikos},\n    Month = {9},\n    Number = {9},\n    Pages = {1425--1439},\n    Title = {Can Constraint Closure Provide a Generalized Understanding of Community Dynamics in Ecosystems?},\n    Volume = {130},\n    Year = {2021}}"
  },
  {
    "objectID": "research/essays/heiss-kelley-2017/index.html",
    "href": "research/essays/heiss-kelley-2017/index.html",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "",
    "text": "How do the preferences and behavior of both donor organizations and host countries affect the strategies, activities, and effectiveness of international NGOs (INGOs)? Recent books by Sarah Bush, Jessica Teets, and Amanda Murdie bring unique ideas and empirical evidence to illustrate different parts of this question. To discuss the arguments in each book, as well as explore incidental ties between the three, we suggest a simple framework for organizing and understanding the dual institutional constraints on INGOs. In this essay, we use this framework to identify how each book addresses these influences on INGOs and how, in some cases, INGOs can reverse the direction of influence."
  },
  {
    "objectID": "research/essays/heiss-kelley-2017/index.html#abstract",
    "href": "research/essays/heiss-kelley-2017/index.html#abstract",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "",
    "text": "How do the preferences and behavior of both donor organizations and host countries affect the strategies, activities, and effectiveness of international NGOs (INGOs)? Recent books by Sarah Bush, Jessica Teets, and Amanda Murdie bring unique ideas and empirical evidence to illustrate different parts of this question. To discuss the arguments in each book, as well as explore incidental ties between the three, we suggest a simple framework for organizing and understanding the dual institutional constraints on INGOs. In this essay, we use this framework to identify how each book addresses these influences on INGOs and how, in some cases, INGOs can reverse the direction of influence."
  },
  {
    "objectID": "research/essays/heiss-kelley-2017/index.html#figure",
    "href": "research/essays/heiss-kelley-2017/index.html#figure",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Figure",
    "text": "Figure\nFigure 2: The dual environmental constraints confronting INGOs\n\n\n\nFigure 2: The dual environmental constraints confronting INGOs"
  },
  {
    "objectID": "research/essays/heiss-kelley-2017/index.html#books-reviewed",
    "href": "research/essays/heiss-kelley-2017/index.html#books-reviewed",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Books reviewed",
    "text": "Books reviewed\n\nSarah Sunn Bush, The Taming of Democracy Assistance: Why Democracy Promotion Does Not Confront Dictators (Cambridge, UK: Cambridge University Press, 2015), doi: 10.1017/cbo9781107706934.\nJessica C. Teets, Civil Society under Authoritarianism: The China Model (New York: Cambridge University Press, 2014), doi: 10.1017/cbo9781139839396.\nAmanda Murdie, Help or Harm: The Human Security Effects of International NGOs (Stanford: Stanford University Press, 2014), doi: 10.2307/j.ctvqsdpq8."
  },
  {
    "objectID": "research/essays/heiss-kelley-2017/index.html#citation",
    "href": "research/essays/heiss-kelley-2017/index.html#citation",
    "title": "Between a Rock and a Hard Place: International NGOs and the Dual Pressures of Donors and Host Governments",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissKelley:2017,\n    Author = {Andrew Heiss and Judith G. Kelley},\n    Doi = {10.1086/691218},\n    Journal = {Journal of Politics},\n    Month = {4},\n    Number = {2},\n    Pages = {732--41},\n    Title = {Between a Rock and a Hard Place: International {NGOs} and the Dual Pressures of Donors and Host Governments},\n    Volume = {79},\n    Year = {2017}}"
  },
  {
    "objectID": "research/essays/witesman-heiss-2016/index.html",
    "href": "research/essays/witesman-heiss-2016/index.html",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "",
    "text": "Collaboration and its promotion by funders continue to accelerate. Although research has identified significant transaction costs associated with collaboration, little empirical work has examined the broader, societal-level economic outcomes of a resource-sharing environment. Does an environment that encourages collaboration shift our focus toward certain types of social objectives and away from others? This paper uses agent-based Monte Carlo simulation to demonstrate that collaboration is particularly useful when resources are rare but a social objective is commonly held. However, collaboration can lead to bad outcomes when the objective is not commonly shared; in such cases, markets outperform collaborative arrangements. These findings suggest that encouraging a resource-sharing environment can lead to inefficiencies even worse than market failure. We also demonstrate that failure to account for transaction costs when prescribing collaboration can result in quantifiably lower outcome levels than expected."
  },
  {
    "objectID": "research/essays/witesman-heiss-2016/index.html#abstract",
    "href": "research/essays/witesman-heiss-2016/index.html#abstract",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "",
    "text": "Collaboration and its promotion by funders continue to accelerate. Although research has identified significant transaction costs associated with collaboration, little empirical work has examined the broader, societal-level economic outcomes of a resource-sharing environment. Does an environment that encourages collaboration shift our focus toward certain types of social objectives and away from others? This paper uses agent-based Monte Carlo simulation to demonstrate that collaboration is particularly useful when resources are rare but a social objective is commonly held. However, collaboration can lead to bad outcomes when the objective is not commonly shared; in such cases, markets outperform collaborative arrangements. These findings suggest that encouraging a resource-sharing environment can lead to inefficiencies even worse than market failure. We also demonstrate that failure to account for transaction costs when prescribing collaboration can result in quantifiably lower outcome levels than expected."
  },
  {
    "objectID": "research/essays/witesman-heiss-2016/index.html#figure",
    "href": "research/essays/witesman-heiss-2016/index.html#figure",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "Figure",
    "text": "Figure\nFigure 1: Simulation results\n\n\n\nFigure 1: Simulation results"
  },
  {
    "objectID": "research/essays/witesman-heiss-2016/index.html#citation",
    "href": "research/essays/witesman-heiss-2016/index.html#citation",
    "title": "Nonprofit collaboration and the resurrection of market failure: How a resource sharing environment can suppress social objectives",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{WitesmanHeiss:2016,\n    Author = {Eva Witesman and Andrew Heiss},\n    Doi = {10.1007/s11266-016-9684-5},\n    Journal = {Voluntas: International Journal of Voluntary and Nonprofit Organizations},\n    Month = {8},\n    Number = {4},\n    Pages = {1500--1528},\n    Title = {Nonprofit Collaboration and the Resurrection of Market Failure: How a Resource-Sharing Environment Can Suppress Social Objectives},\n    Volume = {28},\n    Year = {2016}}"
  },
  {
    "objectID": "research/essays/chaudhry-dotson-heiss-2021/index.html#important-links",
    "href": "research/essays/chaudhry-dotson-heiss-2021/index.html#important-links",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Important links",
    "text": "Important links\n\nPaper (preprint)\nAppendix (preprint)\nStatistical analysis notebook\nGitHub repository\nExperiment preregistration (research question #2)"
  },
  {
    "objectID": "research/essays/chaudhry-dotson-heiss-2021/index.html#abstract",
    "href": "research/essays/chaudhry-dotson-heiss-2021/index.html#abstract",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Abstract",
    "text": "Abstract\nThe phenomenon of closing civic space has adversely impacted INGO funding. We argue that individual private donors can be important in sustaining the operations of INGOs working in repressive contexts. Individual donors do not use the same performance-based metrics as official aid donors. Rather, trust can be an important component of individual donor support for nonprofits working towards difficult goals. How does trust in charitable organizations influence individuals’ preferences to donate, especially when these groups face crackdown? Using a simulated market for philanthropic donations based on data from a nationally representative sample of individuals in the United States who regularly donate to charity, we find that trust in INGOs matters substantially in shaping donor preferences. Donor profiles with high levels of social trust are likely to donate to INGOs with friendly relationships with host governments. This support holds steady if INGOs face criticism or crackdown. In contrast, donor profiles with lower levels of social trust prefer to donate to organizations that do not face criticism or crackdown abroad. The global crackdown on NGOs may thus possibly sour NGOs’ least trusting individual donors. Our findings have practical implications for INGOs raising funds from individuals amid closing civic space."
  },
  {
    "objectID": "research/essays/chaudhry-dotson-heiss-2021/index.html#important-figure",
    "href": "research/essays/chaudhry-dotson-heiss-2021/index.html#important-figure",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Important figure",
    "text": "Important figure\nFigure 4: Average predicted donation market shares across all personas, segmented by persona public affairs knowledge, political ideology, and social trust across different NGO–host government relationships\n\n\n\nFigure 4: Average predicted donation market shares across all personas, segmented by persona public affairs knowledge, political ideology, and social trust across different NGO–host government relationships"
  },
  {
    "objectID": "research/essays/chaudhry-dotson-heiss-2021/index.html#data-and-code",
    "href": "research/essays/chaudhry-dotson-heiss-2021/index.html#data-and-code",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Data and code",
    "text": "Data and code\nThe project is reproducible with R code available at GitHub. Follow the instructions there to install all supporting files and R packages.\nThis project includes the following data files:\n\ndata/raw_data/final_data.rds: Original results from the Qualtrics survey. This is hosted at OSF because of its size. Running targets::tar_make(survey_results_file) will download the .rds file from OSF and place it in data/raw_data. The code for cleaning and processing this data is part of a separate project, “Why Donors Donate”.\ndata/derived_data/survey_results.csv: CSV version of the survey data.\ndata/derived_data/survey_results.yaml: YAML metadata describing the syntax of the survey data.\ndata/raw_data/posterior_draws/public_political_social_charity_demo.rds: Gamma (Γ) coefficients from our multilevel Bayesian model. This is hosted at OSF because of its size. Running targets::tar_make(gamma_draws_file) will download the .rds file from OSF and place it in data/raw_data/posterior_draws. The code for running this model is part of a separate project, “Why Donors Donate”.\ndata/raw_data/Market Simulator Version 01.xlsx: An interactive Excel version of the market simulator to help demonstrate the intuition behind all the moving parts of the simulation."
  },
  {
    "objectID": "research/essays/chaudhry-dotson-heiss-2021/index.html#citation",
    "href": "research/essays/chaudhry-dotson-heiss-2021/index.html#citation",
    "title": "Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryDotsonHeiss:2021,\n    Author = {Suparna Chaudhry and Marc Dotson and Andrew Heiss},\n    Doi = {10.1111/1758-5899.12984},\n    Journal = {Global Policy},\n    Month = {7},\n    Number = {S5},\n    Pages = {45--58},\n    Title = {Who Cares About Crackdowns? Exploring the Role of Trust in Individual Philanthropy},\n    Volume = {12},\n    Year = {2021}}"
  },
  {
    "objectID": "research/essays/heiss-2019-taking-control/index.html",
    "href": "research/essays/heiss-2019-taking-control/index.html",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository\nRead-only PDF"
  },
  {
    "objectID": "research/essays/heiss-2019-taking-control/index.html#important-links",
    "href": "research/essays/heiss-2019-taking-control/index.html#important-links",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "",
    "text": "Paper (preprint)\nGitHub repository\nRead-only PDF"
  },
  {
    "objectID": "research/essays/heiss-2019-taking-control/index.html#abstract",
    "href": "research/essays/heiss-2019-taking-control/index.html#abstract",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Abstract",
    "text": "Abstract\nA wave of legislative and regulatory crackdown on international nongovernmental organizations (INGOs) has constricted the legal environment for foreign advocacy groups interested in influencing domestic and global policy. Although the legal space for advocacy is shrinking, many INGOs have continued their work and found creative ways to adapt to these restrictions, sometimes even reshaping the regulatory environments of their target countries in their favor. In this article, I explore what enables INGOs to cope with and reshape their regulatory environments. I bridge international relations and interest group literatures to examine the interaction between INGO resource configurations and institutional arrangements. I argue that specific resource and managerial characteristics provide organizations with ‘programmatic flexibility’ that enables groups to adjust their strategies without changing their core mission. I illustrate and test this argument with case studies of Article 19 and AMERA International and demonstrate how organizations with high programmatic flexibility can navigate regulations and shape policy in their target country, while those without this flexibility are shut out of policy discussions and often the target country itself. I conclude by exploring how the interaction between internal characteristics and institutional environments shape and constrain the effects of interest groups in global governance."
  },
  {
    "objectID": "research/essays/heiss-2019-taking-control/index.html#important-figures",
    "href": "research/essays/heiss-2019-taking-control/index.html#important-figures",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Important figures",
    "text": "Important figures\nFigure 1: Relationship between institutional constraints, resource configurations, programmatic flexibility, and advocacy effects\n\n\n\nFigure 1: Relationship between institutional constraints, resource configurations, programmatic flexibility, and advocacy effects"
  },
  {
    "objectID": "research/essays/heiss-2019-taking-control/index.html#citation",
    "href": "research/essays/heiss-2019-taking-control/index.html#citation",
    "title": "Taking Control of Regulations: How International Advocacy NGOs Shape the Regulatory Environments of their Target Countries",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{Heiss:2019,\n    Author = {Andrew Heiss},\n    Doi = {10.1057/s41309-019-00061-0},\n    Journal = {Interest Groups and Advocacy},\n    Month = {9},\n    Number = {3},\n    Pages = {356--375},\n    Title = {Taking Control of Regulations: How International Advocacy {NGOs} Shape the Regulatory Environments of their Target Countries},\n    Volume = {8},\n    Year = {2019}}"
  },
  {
    "objectID": "research/essays/heiss-johnson-2016/index.html",
    "href": "research/essays/heiss-johnson-2016/index.html",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "",
    "text": "Recent scholarship works to open the “black box” of international non-governmental organizations (INGOs), explaining their activities without conventional assumptions of altruism and high-mindedness. We review Borders Among Activists by Sarah Stroup, The Opening up of International Organizations by Jonas Tallberg, et al., and Internal Affairs by Wendy Wong and pinpoint how each book makes important contributions to understanding the determinants of INGO activities. After developing an organizing framework that permits careful analysis of the internal, interactive, and institutional factors that influence policy outcomes, we demonstrate the merits of the framework by applying it to each book. We evaluate each work’s contributions to individual layers in the framework, locating each book within its intended scholarly context. We identify ties between books, examining how each work implicitly treats the other layers of our framework. The conclusion identifies a new research agenda for INGO studies and outlines promising avenues for future INGO scholarship."
  },
  {
    "objectID": "research/essays/heiss-johnson-2016/index.html#abstract",
    "href": "research/essays/heiss-johnson-2016/index.html#abstract",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "",
    "text": "Recent scholarship works to open the “black box” of international non-governmental organizations (INGOs), explaining their activities without conventional assumptions of altruism and high-mindedness. We review Borders Among Activists by Sarah Stroup, The Opening up of International Organizations by Jonas Tallberg, et al., and Internal Affairs by Wendy Wong and pinpoint how each book makes important contributions to understanding the determinants of INGO activities. After developing an organizing framework that permits careful analysis of the internal, interactive, and institutional factors that influence policy outcomes, we demonstrate the merits of the framework by applying it to each book. We evaluate each work’s contributions to individual layers in the framework, locating each book within its intended scholarly context. We identify ties between books, examining how each work implicitly treats the other layers of our framework. The conclusion identifies a new research agenda for INGO studies and outlines promising avenues for future INGO scholarship."
  },
  {
    "objectID": "research/essays/heiss-johnson-2016/index.html#important-figures",
    "href": "research/essays/heiss-johnson-2016/index.html#important-figures",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Important figures",
    "text": "Important figures\nFigure 1 from the paper, showing a unified framework for understanding the types of influences on INGO behavior and policy outcomes. The triangular shape conveys the specificity of the layers: from narrow “micro” phenomena at the internal layer to much wider “macro” phenomena at the institutional layer.\n\n\n\nFigure 1: A Unified Framework for Analyzing INGO Behavior and Outputs"
  },
  {
    "objectID": "research/essays/heiss-johnson-2016/index.html#books-reviewed",
    "href": "research/essays/heiss-johnson-2016/index.html#books-reviewed",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Books reviewed",
    "text": "Books reviewed\n\nSarah S. Stroup, Borders among Activists: International NGOs in the United States, Britain, and France (Ithaca, New York: Cornell University Press, 2012), doi: 10.7591/9780801464256.\nJonas Tallberg, Thomas Sommerer, Theresa Squatrito, and Christer Jönsson, The Opening Up of International Organizations: Transnational Access in Global Governance (Cambridge: Cambridge University Press, 2013), doi: 10.1017/cbo9781107325135.\nWendy H. Wong, Internal Affairs: How the Structure of NGOs Transforms Human Rights (Ithaca: Cornell University Press, 2012), doi: 10.7591/9780801466069."
  },
  {
    "objectID": "research/essays/heiss-johnson-2016/index.html#citation",
    "href": "research/essays/heiss-johnson-2016/index.html#citation",
    "title": "Internal, Interactive, and Institutional Factors: Towards a Unified Theory of INGO Behavior",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissJohnson:2016,\n    Author = {Andrew Heiss and Tana Johnson},\n    Doi = {10.1093/isr/viv014},\n    Journal = {International Studies Review},\n    Month = {9},\n    Number = {3},\n    Pages = {528--41},\n    Title = {Internal, Interactive, and Institutional Factors: Towards a Unified Theory of {INGO} Behavior},\n    Volume = {18},\n    Year = {2016}}"
  },
  {
    "objectID": "research/essays/heiss-kelley-2017a/index.html",
    "href": "research/essays/heiss-kelley-2017a/index.html",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "",
    "text": "Amid the academic and policy critiques of the United States 15-year push to eliminate human trafficking, the perspective of the nongovernmental organizations (NGOs) working with anti-trafficking advocacy and services has been largely ignored. This article presents the results of a global survey of nearly 500 anti-trafficking NGOs in working in 133 countries, and is the first NGO-focused survey of its kind. Based on the results of the survey, we provide an overview of the anti-trafficking NGO sector as a whole, detail the relationship between anti-trafficking NGOs and the US, and account for some of the variation in NGO opinions of US efforts. Notably, we find that NGOs are remarkably satisfied with US-led efforts—despite their acknowledged flaws—and that NGOs believe that American anti-TIP policies are important and, on balance, helpful. These results also provide a warning for the future of the United States’ anti-trafficking advocacy, suggesting that the US avoid politicizing its annual Trafficking in Persons Report."
  },
  {
    "objectID": "research/essays/heiss-kelley-2017a/index.html#abstract",
    "href": "research/essays/heiss-kelley-2017a/index.html#abstract",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "",
    "text": "Amid the academic and policy critiques of the United States 15-year push to eliminate human trafficking, the perspective of the nongovernmental organizations (NGOs) working with anti-trafficking advocacy and services has been largely ignored. This article presents the results of a global survey of nearly 500 anti-trafficking NGOs in working in 133 countries, and is the first NGO-focused survey of its kind. Based on the results of the survey, we provide an overview of the anti-trafficking NGO sector as a whole, detail the relationship between anti-trafficking NGOs and the US, and account for some of the variation in NGO opinions of US efforts. Notably, we find that NGOs are remarkably satisfied with US-led efforts—despite their acknowledged flaws—and that NGOs believe that American anti-TIP policies are important and, on balance, helpful. These results also provide a warning for the future of the United States’ anti-trafficking advocacy, suggesting that the US avoid politicizing its annual Trafficking in Persons Report."
  },
  {
    "objectID": "research/essays/heiss-kelley-2017a/index.html#important-figures",
    "href": "research/essays/heiss-kelley-2017a/index.html#important-figures",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "Important figures",
    "text": "Important figures\nFigure 1: Countries where NGOs work, excluding NGOs working only in the US\n\n\n\nFigure 1: Countries where NGOs work, excluding NGOs working only in the US\n\n\nFigure 4: Embassies or foreign governments NGOs reported as active partners in the fight against human trafficking\n\n\n\nFigure 4: Embassies or foreign governments NGOs reported as active partners in the fight against human trafficking\n\n\nFigure 7: Average importance and positivity of US anti-TIP efforts across regions\n\n\n\nFigure 7: Average importance and positivity of US anti-TIP efforts across regions"
  },
  {
    "objectID": "research/essays/heiss-kelley-2017a/index.html#citation",
    "href": "research/essays/heiss-kelley-2017a/index.html#citation",
    "title": "From the Trenches: A Global Survey of Anti-TIP NGOs and their Views of US Efforts",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{HeissKelley:2017,\n  Author = {Andrew Heiss and Judith G. Kelley},\n  Doi = {10.1080/23322705.2016.1199241},\n  Journal = {Journal of Human Trafficking},\n  Number = {3},\n  Pages = {231--254},\n  Title = {From the Trenches: A Global Survey of Anti-{TIP} {NGOs} and their Views of {US} Efforts},\n  Volume = {3},\n  Year = {2017}}"
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#important-links",
    "href": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#important-links",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Important links",
    "text": "Important links\n\nPaper (preprint)\nSupplement (preprint)\nStatistical analysis notebook\nGitHub repository\nExperiment preregistration"
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#media-coverage",
    "href": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#media-coverage",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Media coverage",
    "text": "Media coverage\n\n“Donors grow more generous when they support nonprofits facing hostile environments abroad,” The Conversation, December 7, 2020"
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#abstract",
    "href": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#abstract",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Abstract",
    "text": "Abstract\nState restrictions on non-governmental organizations (NGOs) have become increasingly pervasive across the globe. While this crackdown has been shown to have a negative impact on public funding flows, we know little about how it impacts private philanthropy. How does information about crackdown abroad, as well as organizational attributes of nonprofits affect individual donors’ willingness to donate internationally? Using a survey experiment, we find that learning about repressive NGO environments increases generosity in that already-likely donors are willing to donate substantially more to legally besieged nonprofits. This generosity persists when mediated by two organizational-level heuristics: NGO issue areas and main funding sources. We discuss the implications of our results on how nonprofits can use different framing appeals to increase fundraising at a time when traditional public donor funding to such organizations is decreasing."
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#important-figures",
    "href": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#important-figures",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Important figures",
    "text": "Important figures\nFigures 2 & 3: Difference in likelihood of donation across crackdown and no crackdown groups, conditioned by other experimental frames + difference in amount donated across crackdown and no crackdown groups, conditioned by other experimental frames\n\n\n\nFigures 2 & 3: Difference in likelihood of donation across crackdown and no crackdown groups, conditioned by other experimental frames + difference in amount donated across crackdown and no crackdown groups, conditioned by other experimental frames"
  },
  {
    "objectID": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#citation",
    "href": "research/essays/chaudhry-heiss-ngos-philanthropy/index.html#citation",
    "title": "Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{ChaudhryHeiss:2021,\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Doi = {10.1177/0899764020971045},\n    Journal = {Nonprofit and Voluntary Sector Quarterly},\n    Month = {6},\n    Number = {3},\n    Pages = {481--505},\n    Title = {Dynamics of International Giving: How Heuristics Shape Individual Donor Preferences},\n    Volume = {50},\n    Year = {2021}}"
  },
  {
    "objectID": "research/index.html#essays-and-op-eds",
    "href": "research/index.html#essays-and-op-eds",
    "title": "Research",
    "section": "Essays and Op-eds",
    "text": "Essays and Op-eds",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/index.html#open-peer-reviews",
    "href": "research/index.html#open-peer-reviews",
    "title": "Research",
    "section": "Open Peer Reviews",
    "text": "Open Peer Reviews",
    "crumbs": [
      "Research"
    ]
  },
  {
    "objectID": "research/articles/kubinec-milner-2024/index.html#important-links",
    "href": "research/articles/kubinec-milner-2024/index.html#important-links",
    "title": "Taxes in the Time of Revolution: An Experimental Test of the Rentier State during Algeria’s Hirak",
    "section": "Important links",
    "text": "Important links\n\nPaper (preprint)\nGitHub repository"
  },
  {
    "objectID": "research/articles/kubinec-milner-2024/index.html#abstract",
    "href": "research/articles/kubinec-milner-2024/index.html#abstract",
    "title": "Taxes in the Time of Revolution: An Experimental Test of the Rentier State during Algeria’s Hirak",
    "section": "Abstract",
    "text": "Abstract\nThis article examines the rentier thesis that a government’s control over oil resources should help it to resist pressures for democratization. The authors’ online survey experiment, implemented during a nationwide mobilization for regime change in Algeria known as the Hirak, used interactive experimental treatments to provide information about the Algerian government’s subsidies for fuel resources and low value-added taxes. Based on a sample of 9,721 Algerians, the authors find that when Algerians learn about their country’s relatively high level of fuel subsidies and low level of taxes, their assessments of the government’s performance improve. However, the treatment has null effects for outcomes involving demands for representation, such as intentions to join protest movements. An analysis of treatment heterogeneity in terms of wealth shows that the wealthy became much more supportive of the regime in response to the treatment while in some cases the poor became much less so. This treatment heterogeneity appears to be related to divergent views among the wealthy and the poor concerning the goals of the protest movement, with the wealthy favoring institutional reforms while the poor prioritized redistributive justice."
  },
  {
    "objectID": "research/articles/kubinec-milner-2024/index.html#data-and-code",
    "href": "research/articles/kubinec-milner-2024/index.html#data-and-code",
    "title": "Taxes in the Time of Revolution: An Experimental Test of the Rentier State during Algeria’s Hirak",
    "section": "Data and code",
    "text": "Data and code\nThe project is reproducible with R code and data available at GitHub."
  },
  {
    "objectID": "research/articles/kubinec-milner-2024/index.html#citation",
    "href": "research/articles/kubinec-milner-2024/index.html#citation",
    "title": "Taxes in the Time of Revolution: An Experimental Test of the Rentier State during Algeria’s Hirak",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@article{kubinec_taxes_2024,\n    title = {Taxes in the {Time} of {Revolution}: {An} {Experimental} {Test} of the {Rentier} {State} during {Algeria}'s {Hirak}},\n    volume = {76},\n    issn = {1086-3338},\n    shorttitle = {Taxes in the {Time} of {Revolution}},\n    url = {https://muse.jhu.edu/pub/1/article/924508},\n    abstract = {This article examines the rentier thesis that a government's control over oil resources should help it to resist pressures for democratization. The authors' online survey experiment, implemented during a nationwide mobilization for regime change in Algeria known as the Hirak, used interactive experimental treatments to provide information about the Algerian government's subsidies for fuel resources and low value-added taxes. Based on a sample of 9,721 Algerians, the authors find that when Algerians learn about their country's relatively high level of fuel subsidies and low level of taxes, their assessments of the government's performance improve. However, the treatment has null effects for outcomes involving demands for representation, such as intentions to join protest movements. An analysis of treatment heterogeneity in terms of wealth shows that the wealthy became much more supportive of the regime in response to the treatment while in some cases the poor became much less so. This treatment heterogeneity appears to be related to divergent views among the wealthy and the poor concerning the goals of the protest movement, with the wealthy favoring institutional reforms while the poor prioritized redistributive justice.},\n    number = {2},\n    urldate = {2024-08-14},\n    journal = {World Politics},\n    author = {Kubinec, Robert and Milner, Helen V.},\n    year = {2024},\n    note = {Publisher: Johns Hopkins University Press},\n    pages = {294--333},\n}"
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "Robert Kubinec",
    "section": "",
    "text": "MIT License\nCopyright (c) 2016-present George Cushen\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "files/data/dataverse_files/County Presidential Returns 2000-2016.html",
    "href": "files/data/dataverse_files/County Presidential Returns 2000-2016.html",
    "title": "Codebook for 2000-2016 County Presidential Data",
    "section": "",
    "text": "The data file countypres_2000-2016 contains county-level returns for presidential elections from 2000 to 2016. The data source is official state election data records.\nNote: County results in Alaska for 2004 are based on official Alaska data, but it is clear the district returns significantly overstate the number of votes cast.\n\n\nThe variables are listed as they appear in the data file.\n\n\n\nDescription: election year\n\n\n\n\n\n\nDescription: state name\n\n\n\n\n\n\nDescription: U.S. postal code state abbreviation\n\n\n\n\n\n\nDescription: county name\n\n\n\n\n\n\nDescription: county FIPS code\n\n\n\n\n\n\nDescription: President\n\n\n\n\n\n\nDescription: name of the candidate\n\n\n\n\n\n\nDescription: party of the candidate\n\n\n\n\n\n\nDescription: votes received by this candidate for this particular party\n\n\n\n\n\n\nDescription: total number of votes cast in this county-year\n\n\n\n\n\n\nDescription: date when dataset was finalized"
  },
  {
    "objectID": "files/data/dataverse_files/County Presidential Returns 2000-2016.html#variables",
    "href": "files/data/dataverse_files/County Presidential Returns 2000-2016.html#variables",
    "title": "Codebook for 2000-2016 County Presidential Data",
    "section": "",
    "text": "The variables are listed as they appear in the data file.\n\n\n\nDescription: election year\n\n\n\n\n\n\nDescription: state name\n\n\n\n\n\n\nDescription: U.S. postal code state abbreviation\n\n\n\n\n\n\nDescription: county name\n\n\n\n\n\n\nDescription: county FIPS code\n\n\n\n\n\n\nDescription: President\n\n\n\n\n\n\nDescription: name of the candidate\n\n\n\n\n\n\nDescription: party of the candidate\n\n\n\n\n\n\nDescription: votes received by this candidate for this particular party\n\n\n\n\n\n\nDescription: total number of votes cast in this county-year\n\n\n\n\n\n\nDescription: date when dataset was finalized"
  },
  {
    "objectID": "ordbetareg.html#about-the-model",
    "href": "ordbetareg.html#about-the-model",
    "title": "Ordered Beta Regression",
    "section": "",
    "text": "Proportion data–percentiles, slider scales, visual-analog scales–have long puzzled statisticians because they combine two things: a continuous measure (the proportion) and a discrete measure (the top value of 1 or 100% and the bottom value of 0 or 0%). Conventional statistical models like the oft-used OLS regression implicitly assume these boundaries do not exist; this means an OLS regression can predict to absurd values like 115% of patients being sick or -5% of legislators being elected.\nTo address this issue, I developed the ordered beta regression model that combines two things: beta regression, which is defined for any bounded continuous scale, and ordered logit, which works for discrete categories. By doing this, you can fit an ordered beta regression for any percentile/proportion for both the middle continuous part and the bounds of the scale. This model can also predict within this scale as well.\nI describe the model and how to do this type of modeling in this blog post. For more detail on the inner workings of the model, I refer you to the paper:\nKubinec, Robert. 2023. “Ordered Beta Regression: A Parsimonious, Well-Fitting Model for Continuous Data with Lower and Upper Bounds.” Political Analysis 31(4): 519–36. doi: http://doi.org/10.1017/pan.2022.20.\nFor an ungated version of the article, see the pre-publication draft on OSF:\nhttps://osf.io/preprints/socarxiv/2sx6y",
    "crumbs": [
      "Ordered Beta Regression"
    ]
  },
  {
    "objectID": "ordbetareg.html#usage-of-ordered-beta-regression",
    "href": "ordbetareg.html#usage-of-ordered-beta-regression",
    "title": "Ordered Beta Regression",
    "section": "Usage of Ordered Beta Regression",
    "text": "Usage of Ordered Beta Regression\nAs of September 21st, 2024, the ordered beta regression paper above has been cited 24 times. The package has been used across the biomedical and social sciences, including studies of endangered species populations, cognitive psychology, hearing loss, bumblebee habitat, and emergency response, just to name a few. For a complete list, see the Google Scholar citation list.",
    "crumbs": [
      "Ordered Beta Regression"
    ]
  },
  {
    "objectID": "ordbetareg.html#how-to-use-the-model",
    "href": "ordbetareg.html#how-to-use-the-model",
    "title": "Ordered Beta Regression",
    "section": "How to Use the Model",
    "text": "How to Use the Model\nAt present, ordered beta regression is available in Stan (Section 3.5), R (Section 3.1), Python (Section 3.2), Stata (Section 3.3), and Julia (Section 3.4). R has the strongest support for the model with multiple implementations, but it is perfectly usable in other statistical frameworks. Below I briefly describe the available packages.\n\nR\nOrdered beta regression is currently available in R in two different packages: ordbetareg and glmmTMB. The primary difference between these two packages is estimation method and the number of ordered beta-specific functions. ordbetareg is a package that estimates a Bayesian implementation as described in the paper above by using brms, a powerful R package for Bayesian regression. ordbetareg is maintained by me and you can see the full source code here on Github (and report an issue with the package if you have one). The package includes auxiliary functions like power analysis and plots that are specific to proportion responses—check out the package vignette for more details.\nglmmTMB, by contrast, is a general purpose regression package that specializes in mixed multilevel models. It implements a broad array of regression models using maximum likelihood. This means that a model estimated in glmmTMB will almost certainly be faster than ordbetareg (although if you set up ordbetareg to use cmdstanr it can get pretty fast). On the other hand, maximum likelihood estimation has some limitations. Arguably the most important one is that it can be tricky to fit a model that doesn’t have observations at the bounds, i.e. either 0 (0%) or 1 (100%). The Bayesian implementation in ordbetareg has no problem doing this.\nThat being said, I think glmmTMB is a fine package and believe it works fine for many scholars’ problems. brms, which ordbetareg is based on, offers a lot more features, including native support for multiple imputation, time series modeling and even latent variable/factor analysis modeling, but it does require more setup and the models are generally slower.\nFor either package, I highly recommend using marginaleffects, and in particular the avg_slopes function, to convert the coefficient estimates in the package to the bounded scale (i.e. between 0 and 1). This function allows you to then interpret your model estimates as the effect of a covariate in terms of percentage change/proportion change in the bounded response/outcome. marginaleffects is available on CRAN and works great with both ordbetareg and glmmTMB.\nWith both of these packages available, ordered beta regression has very strong support in R. If you don’t have a preference for a particular software package and want to use this model, I would recommend R.\n\n\nPython\nOrdered beta regression is implemented using the scipy package with maximum likelihood estimation. At present, you’ll need to clone (i.e., download) this Github repository to install the package as it is not yet available with pip or conda forge. The package includes plotting functions specific to proportion outcomes and reports coefficient values in the untransformed (logit) scale. marginaleffects support for this package is coming soon.\n\n\nStata\nThere is initial support for Stata as a set of .ado files that can be downloaded and installed in the ado folder in your Stata machine. More details are available on the Github repository. This package supports using the margins command to convert coefficients to the bounded outcome scale, although it does not support all Stata features as of yet. There are plans to make this package available via SSC in the near future.\n\n\nJulia\nOrdered beta regression is available via the SubjectiveScalesModels.jl package, which offers support for a variety of regression models useful in cognitive psychology and other fields with sliders/visual analog scales. The package includes neat visualizations and is maintained by Dominique Makowksi.\n\n\nStan\nAs is evident in the paper, the original model was written in Stan code. While I no longer am maintaining the Stan code, it is available in the paper repository and works great. If you want to incorporate the likelihood or other Stan code into your Stan model, feel free!",
    "crumbs": [
      "Ordered Beta Regression"
    ]
  },
  {
    "objectID": "ordbetareg.html#impact-of-ordered-beta-regression",
    "href": "ordbetareg.html#impact-of-ordered-beta-regression",
    "title": "Ordered Beta Regression",
    "section": "Impact of Ordered Beta Regression",
    "text": "Impact of Ordered Beta Regression\nAs of September 21st, 2024, the ordered beta regression paper above has been cited 24 times in Crossref. The package has been used across the biomedical and social sciences, including studies of endangered species populations, cognitive psychology, hearing loss, bumblebee habitat, and emergency response, just to name a few. For a complete list, see the Google Scholar citation page.",
    "crumbs": [
      "Ordered Beta Regression"
    ]
  },
  {
    "objectID": "research/chapters/heiss-causal-inference-2021/index.html",
    "href": "research/chapters/heiss-causal-inference-2021/index.html",
    "title": "Causal Inference",
    "section": "",
    "text": "One of the most repeated phrases in any introductory statistics class is the warning that “correlation is not causation.” This chapter presents new non-statistical language for creating, measuring, and evaluating causal stories and relationships using observational (i.e. non-experimental) data. It introduces the concept of causal directed acyclic graphs (DAGs) that allow us to formally encode our understanding of causal stories. With well-crafted DAGs, one can use a set of rules called do-calculus to make specific adjustments to statistical models and isolate or identify causal relationships between variables of interest. Edges (or arrows) transmit associations between nodes. Following the fundamental problem of causal inference, answering causal questions without an experiment appears impossible. The chapter explores different methods of making adjustments using synthetic data. Instead of throwing away potentially useful data, one can use other methods to create matches that are less discrete and more informative."
  },
  {
    "objectID": "research/chapters/heiss-causal-inference-2021/index.html#abstract",
    "href": "research/chapters/heiss-causal-inference-2021/index.html#abstract",
    "title": "Causal Inference",
    "section": "",
    "text": "One of the most repeated phrases in any introductory statistics class is the warning that “correlation is not causation.” This chapter presents new non-statistical language for creating, measuring, and evaluating causal stories and relationships using observational (i.e. non-experimental) data. It introduces the concept of causal directed acyclic graphs (DAGs) that allow us to formally encode our understanding of causal stories. With well-crafted DAGs, one can use a set of rules called do-calculus to make specific adjustments to statistical models and isolate or identify causal relationships between variables of interest. Edges (or arrows) transmit associations between nodes. Following the fundamental problem of causal inference, answering causal questions without an experiment appears impossible. The chapter explores different methods of making adjustments using synthetic data. Instead of throwing away potentially useful data, one can use other methods to create matches that are less discrete and more informative."
  },
  {
    "objectID": "research/chapters/heiss-causal-inference-2021/index.html#important-figures",
    "href": "research/chapters/heiss-causal-inference-2021/index.html#important-figures",
    "title": "Causal Inference",
    "section": "Important figures",
    "text": "Important figures\nFigure 6: More complicated DAG showing the relationship between campaign spending and votes won in an election\n\n\n\nFigure 6: More complicated DAG showing the relationship between campaign spending and votes won in an election\n\n\nFigure 17: Adjustment set to identify the relationship between mosquito net use and malaria risk\n\n\n\nFigure 17: Adjustment set to identify the relationship between mosquito net use and malaria risk"
  },
  {
    "objectID": "research/chapters/heiss-causal-inference-2021/index.html#citation",
    "href": "research/chapters/heiss-causal-inference-2021/index.html#citation",
    "title": "Causal Inference",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@incollection{Heiss:2021,\n    address = {Boca Raton, Florida},\n    author = {Andrew Heiss},\n    booktitle = {R for Political Data Science: A Practical Guide},\n    chapter = {10},\n    doi = {10.1201/9781003010623-10},\n    editor = {Francisco Urdinez and Andr{\\'e}s Cruz},\n    pages = {235--274},\n    publisher = {{Chapman and Hall} / CRC},\n    title = {Causal Inference},\n    year = {2021}}"
  },
  {
    "objectID": "research/chapters/chaudhry-heiss-closing-space/index.html",
    "href": "research/chapters/chaudhry-heiss-closing-space/index.html",
    "title": "Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on NGOs",
    "section": "",
    "text": "Figure 1: 2017 CIVICUS Monitor civic space ratings\n\n\n\nFigure 1: 2017 CIVICUS Monitor civic space ratings"
  },
  {
    "objectID": "research/chapters/chaudhry-heiss-closing-space/index.html#important-figure",
    "href": "research/chapters/chaudhry-heiss-closing-space/index.html#important-figure",
    "title": "Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on NGOs",
    "section": "",
    "text": "Figure 1: 2017 CIVICUS Monitor civic space ratings\n\n\n\nFigure 1: 2017 CIVICUS Monitor civic space ratings"
  },
  {
    "objectID": "research/chapters/chaudhry-heiss-closing-space/index.html#citation",
    "href": "research/chapters/chaudhry-heiss-closing-space/index.html#citation",
    "title": "Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on NGOs",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@incollection{ChaudhryHeiss:2022,\n    Address = {Tuscaloosa, AL},\n    Author = {Suparna Chaudhry and Andrew Heiss},\n    Booktitle = {Beyond the Boomerang: New Patterns in Transcalar Advocacy},\n    Editor = {Christopher L. Pallas and Elizabeth Bloodgood},\n    Publisher = {University of Alabama Press},\n    Title = {Closing Space and the Restructuring of Global Activism: Causes and Consequences of the Global Crackdown on {NGOs}},\n    Chapter = {2},\n    Year = {2022}}"
  },
  {
    "objectID": "research/chapters/heiss-ngo-ir-2019/index.html",
    "href": "research/chapters/heiss-ngo-ir-2019/index.html",
    "title": "NGOs and Authoritarianism",
    "section": "",
    "text": "Authoritarian restrictions on domestic and international civil society have increased over the past decade, but authoritarian states continue to allow—and even invite—NGOs to work in their countries. Though the services and advocacy provided by NGOs can challenge the legitimacy and power of authoritarian regimes, the majority of autocratic states allow NGO activities, and NGOs in turn continue to work in these countries in spite of the heavy legal restrictions and attempts to limit their activities. This chapter examines the theories about and the experiences of domestic and international NGOs working in authoritarian countries. The review is premised on the theory of authoritarian institutions: dictators delegate political authority to democratic-appearing institutions in order to remain in power and maintain stability. After providing a brief overview of authoritarian institutionalism and balancing, I discuss how domestic and international NGOs fit into authoritarian stability-seeking calculus. I then look at three forms of state–NGO relationships in the context of authoritarianism and explore how autocrats have addressed and regulated international NGOs in particular. Finally, I conclude with suggestions for future research on NGOs and their relationship with and role in authoritarian regimes."
  },
  {
    "objectID": "research/chapters/heiss-ngo-ir-2019/index.html#abstract",
    "href": "research/chapters/heiss-ngo-ir-2019/index.html#abstract",
    "title": "NGOs and Authoritarianism",
    "section": "",
    "text": "Authoritarian restrictions on domestic and international civil society have increased over the past decade, but authoritarian states continue to allow—and even invite—NGOs to work in their countries. Though the services and advocacy provided by NGOs can challenge the legitimacy and power of authoritarian regimes, the majority of autocratic states allow NGO activities, and NGOs in turn continue to work in these countries in spite of the heavy legal restrictions and attempts to limit their activities. This chapter examines the theories about and the experiences of domestic and international NGOs working in authoritarian countries. The review is premised on the theory of authoritarian institutions: dictators delegate political authority to democratic-appearing institutions in order to remain in power and maintain stability. After providing a brief overview of authoritarian institutionalism and balancing, I discuss how domestic and international NGOs fit into authoritarian stability-seeking calculus. I then look at three forms of state–NGO relationships in the context of authoritarianism and explore how autocrats have addressed and regulated international NGOs in particular. Finally, I conclude with suggestions for future research on NGOs and their relationship with and role in authoritarian regimes."
  },
  {
    "objectID": "research/chapters/heiss-ngo-ir-2019/index.html#figure",
    "href": "research/chapters/heiss-ngo-ir-2019/index.html#figure",
    "title": "NGOs and Authoritarianism",
    "section": "Figure",
    "text": "Figure\nFigure 2: Civil society repression and regulations\n\n\n\nFigure 2: Civil society repression and regulations"
  },
  {
    "objectID": "research/chapters/heiss-ngo-ir-2019/index.html#citation",
    "href": "research/chapters/heiss-ngo-ir-2019/index.html#citation",
    "title": "NGOs and Authoritarianism",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@incollection{Heiss:2019,\n    Address = {London},\n    Author = {Andrew Heiss},\n    Booktitle = {Routledge Handbook of {NGOs} and International Relations},\n    Editor = {Thomas Davies},\n    Publisher = {Routledge},\n    Title = {{NGOs} and Authoritarianism},\n    Chapter = {38},\n    Year = {2019}}"
  },
  {
    "objectID": "research/chapters/johnson-heiss-2018/index.html",
    "href": "research/chapters/johnson-heiss-2018/index.html",
    "title": "Liberal Institutionalism",
    "section": "",
    "text": "Liberal institutionalism presumes that domestic and international institutions play central roles in facilitating cooperation and peace between states. But currently, this influential approach to thinking and practice appears to be in jeopardy. The United Kingdom seeks to be the first state ever to withdraw from the European Union (EU). The United States threatens to renegotiate or leave several international arrangements that it has recently signed or long supported. Meanwhile, China hints that it would be happy to take on greater global leadership if the United States retreats from this traditional role."
  },
  {
    "objectID": "research/chapters/johnson-heiss-2018/index.html#abstract",
    "href": "research/chapters/johnson-heiss-2018/index.html#abstract",
    "title": "Liberal Institutionalism",
    "section": "",
    "text": "Liberal institutionalism presumes that domestic and international institutions play central roles in facilitating cooperation and peace between states. But currently, this influential approach to thinking and practice appears to be in jeopardy. The United Kingdom seeks to be the first state ever to withdraw from the European Union (EU). The United States threatens to renegotiate or leave several international arrangements that it has recently signed or long supported. Meanwhile, China hints that it would be happy to take on greater global leadership if the United States retreats from this traditional role."
  },
  {
    "objectID": "research/chapters/johnson-heiss-2018/index.html#citation",
    "href": "research/chapters/johnson-heiss-2018/index.html#citation",
    "title": "Liberal Institutionalism",
    "section": "Citation",
    "text": "Citation\n\n Add to Zotero \n\n@incollection{JohnsonHeiss:2018,\n    Address = {London},\n    Author = {Tana Johnson and Andrew Heiss},\n    Booktitle = {International Organization and Global Governance},\n    Chapter = {8},\n    Doi = {10.4324/9781315301914},\n    Edition = {2},\n    Editor = {Thomas G. Weiss and Rorden Wilkinson},\n    Pages = {123--34},\n    Publisher = {Routledge},\n    Title = {Liberal Institutionalism},\n    Year = {2018}}"
  }
]